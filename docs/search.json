[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Spatial analysis of public health data",
    "section": "",
    "text": "Welcome\nHello, this website  holds the material used to support the Spatial Analysis of public health data, part of the MSc in Modelling for Global Health at the University of Oxford.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Spatial analysis of public health data",
    "section": "",
    "text": "Overview:\nMuch of the data encountered in public health studies has a spatial dimension. The aim is to cover statistical approaches that can be used to make inferences and predictions from spatial data of the kind encountered in epidemiological analyses. We will introduce different types of spatial data, such as point-level and areal data, and understand that these data types require different analytical approaches. We will understand the need to apply statistical approaches that account for spatial dependence, and cover a range of methods of increasing complexity, such as spatial regression, geostatistical models for point level data and conditional autoregressive models for areal data. The students will need a statistical background and familiarity with linear regression techniques and familiarity with programming in R.\n\n\nLearning outcomes:\n\nManipulate and visualise spatial data using R software.\nCritically evaluate quantitative analyses of spatial data through understanding the implications of spatial dependence.\nApply and critique (spatial) statistical analysis techniques to infer relationships between spatial phenomena.\nExplain and evaluate common issues with geographic data such as representation and uncertainty\nDevelop an ability to apply spatial statistical models in R to spatial point-level and areal data to:\n\nPerform inference of spatial trends\nMake predictions using spatial extrapolation",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Spatial analysis of public health data",
    "section": "",
    "text": "The Bartlett Centre for Advanced Spatial Analysis, https://www.andymac.uk/↩︎\nAQA, https://cls.ucl.ac.uk/team/laura-sheppard/↩︎\nThe Bartlett Centre for Advanced Spatial Analysis, https://adamdennett.co.uk/↩︎\nCentre for Behaviour Change, https://www.chiaragericke.co.uk/about-me↩︎\nThe Bartlett Centre for Advanced Spatial Analysis, https://profiles.ucl.ac.uk/103748-ruth-neville/about↩︎",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software installation",
    "section": "",
    "text": "QGIS\nQGIS is an open-source graphic user interface GIS with many community developed add on packages that (or plugins) that provide additional functionality to the software. In this module we will primarly use QGIS to check our spatial data.\nTo get QGIS on your personal machine go to: https://qgis.org/en/site/forusers/download.html\nI install the OSGeo4W version. The nature of open-source means that several programs will rely on each other for features. OSGeo4W tracks all the shared requirements and does not install any duplicates.\nWhen you click through the dialogue boxes you need to search for QGIS in the OSGeo4W setup and click the refresh button so it changes from skip to install….\nContinuous and discrete data. Source: GIS Stackexchange",
    "crumbs": [
      "Software installation"
    ]
  },
  {
    "objectID": "software.html#r",
    "href": "software.html#r",
    "title": "Software installation",
    "section": "R",
    "text": "R\nR is both a programming language and software environment, originally designed for statistical computing and graphics. R’s great strength is that it is open-source, can be used on any computer operating system and free for anyone to use and contribute to. Because of this, it is rapidly becoming the statistical language of choice for many academics and has a huge user community with people constantly contributing new packages to carry out all manner of statistical, graphical and importantly for us, geographical tasks.\n\nBasics\nThe advised method for using R is to download it to your personal machine, so you can use it in future without any issues.\nTo use “R” we need to bits of software:\n\nR itself: https://cran.rstudio.com/\nThen also RStudio: https://www.rstudio.com/products/rstudio/download/#download\n\nR is the core software and RStudio gives a simpler interface for us to use it. If you used R (which you could) you’d be just typing code, with no other features. RStudio gives us other features to make this much easier, just like how you would never type a report in notepad (Windows) or notes (on Apple), you’d use the word processor to make it easier. Chester Ismay and Albert Y. Kim explain this with a car analogy:\n\n\n\n\n\nR vs RStudio. Source: A ModernDive into R and the Tidyverse, Ismay and Kim, 2021\n\n\n\n\nR and RStudio are software that require updating over time. It’s easy to forget!\n\n\nPackages\nBase R (controlled or driven through RStudio) is very limited. As a result people develop packages to make data loading, wrangling, analysis and visulisation much easier than having to write all the code. All packages will have lots of functions that just look up code from the package to make what you are doing easier (we will go into this in more detail later).\nPackages develop over time and have different versions (like software). This can cause some issues with updates and code not working like it did once before. This book means i have to run all the code anyway, so every year it is checked and most packages are updated.\nThere are many errors and issues that can arise with package installation. You should read the following section now so you are aware of it, but you won’t need it until you hit an issue.\nThe most common package errors and issues are:\n\nThe package relies on another package which you might not have — this is called a dependency. Think about building a house — moving in and living in the house is dependent on the foundations, the walls, the windows. The solution is to install the package you are missing.\nThe package has a new version — simply update it, in RStudio the package tab (which we will see in the module) has an update option.\nYour version of R might not support the package. This is unlikely as I update this every year. But to solve it either use an older version of the package and/or update your version of R.\nThere is an error with the package. If all else fails i remove the package and the re-install it. Again, in the package tab there is a small cross next to each package, clicking it let’s you remove it.\n\nIf the problem persists then assuming the error is package xxx is not available consult the relevant stack overflow question",
    "crumbs": [
      "Software installation"
    ]
  },
  {
    "objectID": "01_geographic_data.html",
    "href": "01_geographic_data.html",
    "title": "1  Geographic information",
    "section": "",
    "text": "1.1 Learning outcomes\nBy the end of this practical you should be able to:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Geographic information</span>"
    ]
  },
  {
    "objectID": "01_geographic_data.html#learning-outcomes",
    "href": "01_geographic_data.html#learning-outcomes",
    "title": "1  Geographic information",
    "section": "",
    "text": "Describe and explain GIS data formats and databases\nSource and pre-process spatial data\nLoad and undertaken some basic manipulation of spatial data\nCreate some maps",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Geographic information</span>"
    ]
  },
  {
    "objectID": "01_geographic_data.html#the-basics-of-geographic-information",
    "href": "01_geographic_data.html#the-basics-of-geographic-information",
    "title": "1  Geographic information",
    "section": "1.2 The Basics of geographic information",
    "text": "1.2 The Basics of geographic information\nGeographic data, geospatial data or geographic information is data that identifies the location of features on Earth. There are two main types of data which are used in GIS applications to represent the real world. Vectors that are composed of points, lines and polygons and rasters that are grids of cells with individual values…\n\n\n\n\n\nTypes of spatial data. Source: Spatial data models\n\n\n\n\nIn the above example the features in the real world (e.g. lake, forest, marsh and grassland) have been represented by points, lines and polygons (vector) or discrete grid cells (raster) of a certain size (e.g. 1 x 1m) specifying land cover type.\n\n1.2.1 Data types in statistics\nBefore we go any further let’s just quick go over the different types of data you might encounter\nContinuous data can be measured on some sort of scale and can have any value on it such as height and weight.\nDiscrete data have finite values (meaning you can finish counting something). It is numeric and countable such as number of shoes or the number of computers within the classroom.\nFoot length would be continuous data but shoe size would be discrete data.\n\n\n\n\n\nContinuous and discrete data. Source: Allison Horst data science and stats illustrations\n\n\n\n\nNominal (also called categorical) data has labels without any quantitative value such as hair colour or type of animal. Think names or categories - there are no numbers here.\nOrdinal, similar to categorical but the data has an order or scale, for example if you have ever seen the chilli rating system on food labels or filled a happiness survey with a range between 1 and 10 — that’s ordinal. Here the order matters, but not the difference between them.\nBinary data is that that can have only two possible outcomes, yes and no or shark and not shark.\n\n\n\n\n\nNominal, ordinal and binary data. Source: Allison Horst data science and stats illustrations\n\n\n\n\n\n\n1.2.2 Important GIS data formats\nThere are a number of commonly used geographic data formats that store vector and raster data that you will come across during this course and it’s important to understand what they are, how they represent data and how you can use them.\n\n1.2.2.1 Shapefiles\nPerhaps the most commonly used GIS data format is the shapefile. Shapefiles were developed by ESRI, one of the first and now certainly the largest commercial GIS company in the world. Despite being developed by a commercial company, they are mostly an open format and can be used (read and written) by a host of GIS Software applications.\nA shapefile is actually a collection of files —- at least three of which are needed for the shapefile to be displayed by GIS software. They are:\n\n.shp - the file which contains the feature geometry\n.shx - an index file which stores the position of the feature IDs in the .shp file\n.dbf - the file that stores all of the attribute information associated with the coordinates – this might be the name of the shape or some other information associated with the feature\n.prj - the file which contains all of the coordinate system information (the location of the shape on Earth’s surface). Data can be displayed without a projection, but the .prj file allows software to display the data correctly where data with different projections might be being used\n\nOn Twitter and want to see the love for shapefiles….have a look at the shapefile account\n\n\n1.2.2.2 GeoJSON\nGeoJSON Geospatial Data Interchange format for JavaScript Object Notation is becoming an increasingly popular spatial data format, particularly for web-based mapping as it is based on JavaScript Object Notation. Unlike a shapefile in a GeoJSON, the attributes, boundaries and projection information are all contained in the same file.\n\n\n1.2.2.3 Raster data\nMost raster data is now provided in GeoTIFF (.tiff) format, which stands for Geostarionary Earth Orbit Tagged Image File. The GeoTIFF data format was created by NASA and is a standard public domain format. All necesary information to establish the location of the data on Earth’s surface is embedded into the image. This includes: map projection, coordinate system, ellipsoid and datum type.\n\n\n1.2.2.4 Geodatabase\nA geodatabase is a collection of geographic data held within a database. Geodatabases were developed by ESRI to overcome some of the limitations of shapefiles. They come in two main types: Personal (up to 1 TB) and File (limited to 250 - 500 MB), with Personal Geodatabases storing everything in a Microsoft Access database (.mdb) file and File Geodatabases offering more flexibility, storing everything as a series of folders in a file system. In the example below we can see that the FCC_Geodatabase (left hand pane) holds multiple points, lines, polygons, tables and raster layers in the contents tab.\n\n\n\n\n\n\n\n\n\n\n\n1.2.2.5 GeoPackage\n\n\n\n\n\nGeoPacakge logo\n\n\n\n\nA GeoPackage is an open, standards-based, platform-independent, portable, self-describing, compact format for transferring geospatial data. It stores spatial data layers (vector and raster) as a single file, and is based upon an SQLite database, a widely used relational database management system, permitting code based, reproducible and transparent workflows. As it stores data in a single file it is very easy to share, copy or move.\n\n\n1.2.2.6 SpatiaLite\n\n\n\n\n\nSpatialLite logo\n\n\n\n\nSpatialLite is an open-source library that extends SQLite core. Support is fairly limited and most software that supports SpatiaLite also supports GeoPackage, as they both build upon SQLite. It doesn’t have any clear advantage over GeoPackage, however it is unable to support raster data.\n\n\n1.2.2.7 PostGIS\n\n\n\n\n\nPostGIS logo\n\n\n\n\nPostGIS is an opensource database extender for PostrgeSQL. Essentially PostgreSQL is a database and PostGIS is an add on which permits spatial functions. The advantages of using PostGIS over a GeoPackage are that it allows users to access the data at the same time, can handle large data more efficiently and reduces processing time. In this example calculating the number of bars per neighbourhood in Leon, Mexico the processing time reduced from 1.443 seconds (SQLite) to 0.08 seconds in PostGIS. However, data stored in PostGIS is much harder to share, move or copy.\n\n\n1.2.2.8 What will I use\nThe variety of data formats can see a bit overwhelming. I still have to check how to load some of these data formats that I don’t use frequently. But don’t worry, most of the time you’ll be using shapefiles, GeoPackages or raster data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Geographic information</span>"
    ]
  },
  {
    "objectID": "01_geographic_data.html#general-data-flow",
    "href": "01_geographic_data.html#general-data-flow",
    "title": "1  Geographic information",
    "section": "1.3 General data flow",
    "text": "1.3 General data flow\nAs Grolemund and Wickham state in R for Data Science…\n\n“Data science is a huge field, and there’s no way you can master it by reading a single book.”\n\nHowever, a nice place to start is looking at the typical workflow of a data science (or GIS) project which you will see throughout these practicals, which is summarised nicely in this diagram produced by Dr. Julia Lowndes adapted from Grolemund and Wickham.\n\n\n\n\n\nUpdated from Grolemund & Wickham’s classis R4DS schematic, envisioned by Dr. Julia Lowndes for her 2019 useR! keynote talk and illustrated by Allison Horst. Source: Allison Horst data science and stats illustrations\n\n\n\n\nTo begin you have to import your data (not necessarily environmental) into R or some other kind of GIS to actually be able to do any kind of analysis on it.\nOnce imported you might need to tidy the data. This really depends on what kind of data it is and we cover this later on in the course. However, putting all of your data into a consistent structure will be very beneficial when you have to do analysis on it — as you can treat it all in the same way. Grolemund and Wickham state that data is tidy when “each column is a variable, and each row is an observation”, we cover this more in next week in the [Tidying data] section.\nWhen you have (or haven’t) tidied data you then will most likely want to transform it. Grolemund and Wickham define this as “narrowing in on observations of interest (like all people in one city, or all data from the last year), creating new variables that are functions of existing variables (like computing speed from distance and time), and calculating a set of summary statistics (like counts or means)”. However, from a GIS point of view I would also include putting all of your data into a similar projection, covered next week in [Changing projections] and any other basic process you might do before the core analysis. Arguably these processes could include things such as: clipping (cookie cutting your study area), buffering (making areas within a distance of a point) and intersecting (where two datasets overlap).\nTidying and transform = data wrangling. Remember from the introduction this could be 50-80% of a data science job!\n\n\n\n\n\ndplyr introduction graphic. Source: Allison Horst data science and stats illustrations\n\n\n\n\nAfter you have transformed the data the next best thing to do is visualise it — even with some basic summary statistics. This simple step will often let you look at your data in a different way and select more appropriate analysis.\nNext up is modelling. Personally within GIS i’d say a better term is processing as the very data itself is usually a computer model of reality. The modelling or processing section is where you conduct the core analysis (more than the basic analysis already mentioned) and try to provide an answer to your research question.\nFinally you have to communicate your study and outputs, it doesn’t matter how good your data wrangling, modelling or processing is, if your intended audience can’t interpret it, well, it’s pretty much useless.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Geographic information</span>"
    ]
  },
  {
    "objectID": "01_geographic_data.html#uk-spatial-geography",
    "href": "01_geographic_data.html#uk-spatial-geography",
    "title": "1  Geographic information",
    "section": "1.4 UK spatial geography",
    "text": "1.4 UK spatial geography\nIn this practical we are going to take some regular data (without any geometry) and join it to a spatial data set so we can map it!\nFirstly we need spatial data. It can be quite a daunting task to attempt to understand all of the boundaries that are in use in England and Wales….briefly:\nStatistical hierarchy\n\nStatistical hierarchy are units that census data is collected, the smallest being an output area with around 100 residents.\nOutput areas can be aggregated to Lower Super Output Areas (LSOAs) with between 1,000 and 3,000 residents. These can be further aggregated to Middle Super Output Areas (MSOAs).\nOutput areas and LSOAs typically fit within administrative electoral wards (below)…\nWards and MSOAs fit within local authority areas\n\n\n\n\n\n\n\n\nNesting areas\n\n\n\n\n\n\n\n\n\nNesting example\n\n\n\nNote that all the boundaries can change (e.g. Some LSOAs between the 2011 census and 2021 census moved). To account for this we can use lookup tables to match the new areas with the old ones.\n\nAdministarive hierarchy\n\nAdministrative areas are based on government areas and this depends on where you are in England….\n\n\n\n\n\n\n\n\nA Beginner’s Guide to UK Geography\n\n\n\nSome parts of England have a two tier structure. Counties take on expensive services - such as education and transport. Whilst local authority districts took on smaller services - such as planning permission, markets and local roads. Under all of this are electoral wards that have local Councillors…\nIn 1974 a two tier system of counties and districts was enacted across England and Wales. In urban areas these were metropolitan counties and metropolitan districts..\nBut in 1986 the metropolitan counties were removed (although still recognised) and the metropolitan districts were left as a single authority.\nFrom 1990 many of the tier structures (not in metropolitan areas) were combined into a single structure called Unitary Authorities, especially in medium-sized urban areas. However, some still retained the two tier structure.\n\nAn easy to read guide on census / administrative geography was produced by the London Borough of Tower Hamlets - skip to page 2 for a visual summary.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Geographic information</span>"
    ]
  },
  {
    "objectID": "01_geographic_data.html#r-spatial-data-intro",
    "href": "01_geographic_data.html#r-spatial-data-intro",
    "title": "1  Geographic information",
    "section": "1.5 R Spatial data intro",
    "text": "1.5 R Spatial data intro\nR has a very well developed ecosystem of packages for working with Spatial Data. Early pioneers like Roger Bivand and Edzer Pebesma along with various colleagues were instrumental in writing packages to interface with some powerful open source libraries for working with spatial data, such as GDAL and GEOS. These were accessed via the rgdal and rgeos packages. The maptools package by Roger Bivand, amongst other things, allowed Shapefiles to be read into R. The sp package (along with spdep) by Edzer Pebesma was very important for defining a series of classes and methods for spatial data natively in R which then allowed others to write software to work with these formats. Many these original packages were retired (and superseded by the ones we will use today) at the end of 2023 as their maintainer Roger Bivand also retired. Other packages like raster advanced the analysis of gridded spatial data, while packages like classInt and RColorbrewer facilitated the binning of data and colouring of choropleth maps.\nWhilst these packages were extremely important for advancing spatial data analysis in R, they were not always the most straightforward to use — making a map in R could take quite a lot of effort and they were static and visually basic. However, more recently new packages have arrived to change this. Now leaflet enables R to interface with the leaflet javascript library for online, dynamic maps. ggplot2 which was developed by Hadley Wickham and colleagues radically changed the way that people thought about and created graphical objects in R, including maps, and introduced a graphical style which has been the envy of other software to the extent that there are now libraries in Python which copy the ggplot2 style!\n\n\n\n\n\nggplot2 introduction graphic. Source: Allison Horst data science and stats illustrations\n\n\n\n\nBuilding on all of these, the new tmap (Thematic Map) package has changed the game completely and now enables us to read, write and manipulate spatial data and produce visually impressive and interactive maps, very easily. In parallel, the sf (Simple Features) package is helping us re-think the way that spatial data can be stored and manipulated. It’s exciting times for geographic information / spatial data science!\n\n1.5.1 Spatial data projections\nSpatial data must be located somewhere on Earth and we need to represent this! We do this with Coordinate Reference Systems, shortened to CRS or often sometimes projections (although a projection is just one part of a coordinate reference system).\nProjection systems are mathematical formulas that specify how our data is represented on a map. These can either be call geographic coordinate reference systems or projected coordinate reference systems. The former treats data as a sphere and the latter as a flat object. You might come across phrases such as a resolution of 5 minutes or a resolution of 30 metres, which can be used to establish what kind of projection system has been used. Let me explain…\nA minute type of resolution (e.g. 5 minute resolution) is a geographic reference system that treats the globe as if it was a sphere divided into 360 equal parts called degrees (which are angular units). Each degree has 60 minutes and each minute has 60 seconds. Arc-seconds of latitude (horizontal lines in the globe figure below) remain almost constant whilst arc-seconds of longitude (vertical lines in the globe figure below) decrease in a trigonometric cosine-based fashion as you move towards the Earth’s poles…\n\n\n\n\n\nLatitude and Longitude. Source: ThoughtCo.\n\n\n\n\nThis causes problems as you increase or decrease latitude the longitudinal lengths alter…For example at the equator (0°, such as Quito) a degree is 111.3 km whereas at 60° (such as Saint Petersburg) a degree is 55.80 km …\nIn contrast a projected coordinate system is defined on a flat, two-dimensional plane (through projecting a spheroid onto a 2D surface) giving it constant lengths, angles and areas…\n\n\n\n\n\nIllustration of vector (point) data in which location of London (the red X) is represented with reference to an origin (the blue circle). The left plot represents a geographic CRS with an origin at 0° longitude and latitude. The right plot represents a projected CRS with an origin located in the sea west of the South West Peninsula. Source: Lovelace et al. (2019) section 2.2\n\n\n\n\n\n\n\nIllustration of vector (point) data in which location of London (the red X) is represented with reference to an origin (the blue circle). The left plot represents a geographic CRS with an origin at 0° longitude and latitude. The right plot represents a projected CRS with an origin located in the sea west of the South West Peninsula. Source: Lovelace et al. (2019) section 2.2\n\n\n\n\nKnowing this, if we want to conduct analysis locally (e.g. at a national level) or use metric (e.g. kilometres) measurements we need to be able to change the projection of our data or “reproject” it. Most countries and even states have their own projected coordinate reference system such as British National Grid in the above example…Note how the origin (0,0) is has moved from the centre of the Earth to the bottom South West corner of the UK, which has now been ironed (or flattened) out.\n\n\n\n\n\n\nImportant\n\n\n\nProjection rules\nUnits are angular (e.g. degrees, latitude and longitude) or the data is global = Geographic coordinate reference system\nUnits are linear (e.g. feet, metres) or data is at a local level (e.g. national, well the last one is not always true, but likely) = Projected coordinate reference system.\n\n\n\nYou might hear some key words about projections that could terrify you! Let’s break them down:\n\nEllipsoid (or spheroid) = size of shape of the Earth (3d)\nDatum = contains the point relationship (where the origin (0,0) of the map is) between a Cartesian coordinates (flat surface) and Earth’s surface. They can be local or geocentric (see below). They set the origin, the scale and orientation of the Coordiante Reference System (CRS).\nLocal datum = changes the Ellipsoid to align with a certain location on the surface (e.g. BNG that uses the OSGB36 datum). A local datum is anything that isn’t the centre of the Earth.\nGeocentric datum = the centre is equal to the Earth’s centre of gravity (e.g. WGS84).\nGeodetic datum = global datum (see above for datum meaning) for representing features (e.g. points and polygons) on earth\nGeodesy (from which we get Geodetic) = measuring Earth’s shape and features (e.g. gravity field).\nCoordinate reference system (CRS) = Formula that defines how the 2D map (e.g. on your screen or a paper map) relates to the 3D Earth. Sometimes called a spatial Reference System (SRS). It also stores the datum information.\n\n\n\n\n\n\n\nTip\n\n\n\nTake home message\nWhen you do analysis on multiple datasets make sure they are all use the same Coordinate Reference System.\nIf it’s local (e.g. city of country analysis) then use a local projected CRS where possible.\n\n\n\n\n1.5.2 Data download\nOk, after all that theory we can start downloading data! In this case we will be joining the “health in general” question from the 2021 census to LSOAs in London (although you could select any area).\nMake a new R project and put this data into a data folder\nThe health data be accessed either from:\n\nThe ONS. Note make sure you have selected the right area (in our case LSOA).\nThe London data store. Note this is excel data and the function we would need is read_excel(excel document, sheet number)\n\nOur spatial data can also be accessed from either:\n\nThe ONS\nThe London data store\n\nIn this example i will use the health data from the ONS and spatial data from the London Datastore.\nFirst, load the packages we need:\n\nlibrary(sf)\nlibrary(tidyverse)\n\nThen our data…\n\n# spatial data\n\nLSOAs &lt;- sf::st_read(\"prac1_data/statistical-gis-boundaries-london/ESRI/LSOA_2011_London_gen_MHW.shp\")\n\n# health data\n  \nhealth &lt;- readr::read_csv(\"prac1_data/TS037-2021-3-filtered-2024-01-24T17_28_55Z.csv\")  \n\nTo check our spatial data let’s make a quick map with the thematic maps package (tmap) this works on the grammar of graphics (famous from ggplots), similar to the grammar of data manipulation (tidyverse) it works on a layered approach. Here we specify the dataset and then how we want to style it..In the most basic form…\n\nlibrary(tmap)\n# plot or view - view will make it interactive\ntmap_mode(\"plot\")\n\nℹ tmap modes \"plot\" - \"view\"\nℹ toggle with `tmap::ttm()`\n\n# load the sf object\ntm_shape(LSOAs) +\n  # style it with polygons.\n  tm_polygons(col = NA, alpha = 0.5)\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_polygons()`: use 'fill' for the fill color of polygons/symbols\n(instead of 'col'), and 'col' for the outlines (instead of 'border.col').[v3-&gt;v4] `tm_polygons()`: use `fill_alpha` instead of `alpha`.\n\n\n\n\n\n\n\n\n\n\n\n1.5.3 Wrangle\nBefore progressing it’s also good practice to standardise our column names…we can do so with the janitor package…\n\n\n\n\n\njanitor::clean_names() example. Source: Allison Horst data science and stats illustrations\n\n\n\n\n\nlibrary(janitor)\n\nLSOAs &lt;- janitor::clean_names(LSOAs)\n\nhealth &lt;- janitor::clean_names(health)\n\nNext we need to join the health data to the spatial data…to do so need to identify a unique column in each dataset to perform the join on, such as a code for the LSOAs (e.g. lsoa11cd and lower layer super output areas code ).\nOnce we have the code we can select a join type…\n\n\n\n\n\nSQL join types. Source: SQL Join Diagram, dofactory\n\n\n\n\nTypically in spatial analysis we use a left join - this retains everything in th left data (which is our spatial data set) and joins data from the right only where there are matches\n\n\n\n\n\ndplyr::left_join() example. Source: Tidy explain by Garrick Aden‑Buie\n\n\n\n\n\nIf there are multiple matches then a new row is created (e.g. If there were two health rows for a single LSOA)\nIf there are no matches then the data is dropped (e.g. the LSOAs not in London), but the polygons (the left dataset) are retained.\nNote, if this were the case i could filter out the London LSOAs based on the lad11cd column starting with E09, something like.. filter(str_detect(lad11cd, \"^E09\")) or join the data and then filter based on NAs\n\n\n\n\n\n\ndplyr::left_join() example. Source: Tidy explain by Garrick Aden‑Buie\n\n\n\n\n\njoined_data &lt;- LSOAs %&gt;%\n  left_join(., \n            health,\n            by = c(\"lsoa11cd\" = \"lower_layer_super_output_areas_code\"))\n\n\n\n1.5.4 Long vs wide data\nYou will get a warning saying that each row in x (the spatial data) was expected to match just 1 y row. However, our health data is long data (also called tidy data). This differs from “regular” wide data as…\n\nEach variable (all values that have the same attribute, e.g. height, temperature, duration, weeks) must form its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\nNote, see Wickham’s paper for worked examples and definition of variables, from page 3.\n\n\n\n\n\nThis figure is taken directly from Grolemund and Wickham (2017) Chapter 12.Following three rules makes a dataset tidy: variables are in columns, observations are in rows, and values are in cells. Source: KSK analytics\n\n\n\n\nTypically in GIS we need our data messy (or wide) where the variables have their own column and each row is an area.\nTo so do first we must make the data into a tibble..\n\n\n\n\n\nData Object Type and Structure. Source: Exploratory Data Analysis in R, Gimond 2022\n\n\n\n\nWe should reflect on data types in R, which will influence the structure we select. Note a tibble is very similar (essentially the same!) to a dataframe, except you are provided with additional information when printing.\n\nlibrary(tidyr)\n\njoined_data_wide &lt;- joined_data %&gt;%\n  as_tibble(.)%&gt;%\n  select(lsoa11cd, general_health_6_categories, observation, usualres, hholdres, popden)%&gt;%\n  # move the general health from 1 column to a column for each category\n  tidyr::pivot_wider(.,\n  # note if an ID is not set then all columns not in names_from are treated as ID\n  # we could set the ID as the LSOA but then this would drop usualres, hholdres and popden which would have to be re-joined (see below).\n    id_cols=lsoa11cd,\n    names_from = general_health_6_categories,\n    values_from = observation)%&gt;%\n    clean_names(.)\n\nThis says there should be one observation per (lsoa11cd, general_health_6_categories)\nWhen we make the tibble we lose the geometry column and so our data becomes non spatial again…really we could have done our wrangling first and then conducted a join! This will create a bit of a mess with columns too (as we already have some), so we will need to select the ones we want…\n\njoined_data_wide_joined &lt;- LSOAs %&gt;%\n  left_join(., \n            joined_data_wide,\n            by = c(\"lsoa11cd\" = \"lsoa11cd\"))%&gt;%\n    select(lsoa11cd, msoa11cd, usualres, hholdres, popden, does_not_apply, very_good_health, good_health, fair_health, bad_health, very_bad_health)\n\n\n\n1.5.5 Map\nOnce we have the wide data we can compute other metrics - this is especially important for mapping as we must never map count data, unless the spatial units are the same size (e.g. hexagons). Instead we should normalise our data using some kind of common denominator….for example percent of usual residents with bad health…where the number of usual residents will vary across the spatial units.\n\njoined_data_wide_joined_map &lt;- joined_data_wide_joined%&gt;%\n  mutate(percent_very_bad = (very_bad_health/usualres)*100)%&gt;%\n  mutate(percent_very_bad = round(percent_very_bad, digits=2))\n\nMake a basic map!\n\n# select the sf object to map\ntm1 &lt;- tm_shape(joined_data_wide_joined_map) + \n  # select what column to map\n  tm_polygons(\"percent_very_bad\", \n              # set a palette \n              palette=\"PuBu\")\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_tm_polygons()`: migrate the argument(s) related to the scale of\nthe visual variable `fill` namely 'palette' (rename to 'values') to fill.scale\n= tm_scale(&lt;HERE&gt;).\n\ntm1\n\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"PuBu\" is named\n\"brewer.pu_bu\"\nMultiple palettes called \"pu_bu\" found: \"brewer.pu_bu\", \"matplotlib.pu_bu\". The first one, \"brewer.pu_bu\", is returned.\n\n\n\n\n\n\n\n\n\nThere are some issues with our map that we can resolve…\n\nThe legend is covering the data and is using the object name (with underscores)\nNo scale bar\nThe LSOAs are fairly small and so it can be challenging to interpret them\n\n\nlibrary(tmap)\n\ntm1 &lt;- tm_shape(joined_data_wide_joined_map) + \n  # select what column to map\n  tm_polygons(\"percent_very_bad\", \n              # set a palette \n              palette=\"PuBu\",\n              # how the data should be divided\n              style=\"jenks\",\n              # legend title\n              title = \"\")+\n  \n  tm_compass(position = c(\"left\", \"top\"), size = 2)+\n  \n  tm_layout(main.title=\"% of population with very bad health\",\n          legend.outside=FALSE, \n          frame = TRUE, \n          legend.position = c(0.8,0),\n          legend.text.size = 1)+\n\n  # tm_layout(legend.outside.size = FALSE, \n  #            legend.position= c(\"right\", \"top\"))+\n  tm_scale_bar(position=c(0,0.03), text.size = 1) +\n  \n  tm_credits(\"Data source: ONS and London Data store\",\n          position=c(0,0), \n          size = 0.8, \n          align=\"left\") \n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_polygons()`: instead of `style = \"jenks\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_polygons()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(main.title = )`\n! `tm_scale_bar()` is deprecated. Please use `tm_scalebar()` instead.\n[v3-&gt;v4] `tm_credits()`: use `position = tm_pos_in(align.h)` instead of\n`align`.\n\ntm1\n\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"PuBu\" is named\n\"brewer.pu_bu\"\nMultiple palettes called \"pu_bu\" found: \"brewer.pu_bu\", \"matplotlib.pu_bu\". The first one, \"brewer.pu_bu\", is returned.\n\n\n\n\n\n\n\n\n\nTo export the map…\n\ntmap_save(tm1, 'very bad health.png')\n\nThis hasn’t solved the LSOA issue - whereby the map is challenging to read due to the spatial units used. We can consider aggregating our units to MSOA as that column is provided within the LSOA data…\nTo do so we’d need to:\n\nAggregate our current data\nLoad the MSOA spatial data, then join and map as above.\n\nTo aggregate the data we use a function called group_by() which is always followed by summarise(). Group by places our data into groups based on a selected column (e.g. MSOA) and then summarises the data for each group (e.g. number of people with very bad health)\n\nMSOA_data &lt;- joined_data_wide_joined_map %&gt;%\n  as_tibble(.)%&gt;%\n  select(-lsoa11cd, -geometry, -percent_very_bad)%&gt;%\n  group_by(msoa11cd)%&gt;%\n  summarise_all(sum)\n\nIn the above code select(-variable) means drop that variable, this has allowed me to use the summarise_all() function as opposed to just summarise(). Now each column is aggregated to MSOA!\nCalculate the percentages\n\nMSOA_data_percent &lt;- MSOA_data%&gt;%\n  mutate(percent_very_bad = (very_bad_health/usualres)*100)%&gt;%\n  mutate(percent_very_bad = round(percent_very_bad, digits=2))\n\nRead in the MSOA spatial data\n\nMSOAs &lt;- sf::st_read(\"prac1_data/statistical-gis-boundaries-london/ESRI/MSOA_2011_London_gen_MHW.shp\")%&gt;%\n  clean_names(.)\n\nReading layer `MSOA_2011_London_gen_MHW' from data source \n  `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\Guest\\Oxford_26\\spatial analysis of public health\\spatial-analysis-of-public-health-data\\prac1_data\\statistical-gis-boundaries-london\\ESRI\\MSOA_2011_London_gen_MHW.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 983 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n\nJoin…\n\nMSOA_joined &lt;- MSOAs %&gt;%\n  left_join(., \n            MSOA_data_percent,\n            by = c(\"msoa11cd\" = \"msoa11cd\"))%&gt;%\n    select(msoa11cd, msoa11cd, usualres.x, hholdres.x, popden.x, does_not_apply, very_good_health, good_health, fair_health, bad_health, very_bad_health, percent_very_bad)\n\nMap the MSOAs\n\ntm2 &lt;- tm_shape(MSOA_joined) + \n  # select what column to map\n  tm_polygons(\"percent_very_bad\", \n              # set a palette \n              palette=\"PuBu\",\n              # how the data should be divided\n              style=\"jenks\",\n              # legend title\n              title = \"\")+\n  \n  tm_compass(position = c(\"left\", \"top\"), size = 2)+\n  \n  tm_layout(main.title=\"% of population with very bad health\",\n          legend.outside=FALSE, \n          frame = TRUE, \n          legend.position = c(0.8,0),\n          legend.text.size = 1)+\n\n  # tm_layout(legend.outside.size = FALSE, \n  #            legend.position= c(\"right\", \"top\"))+\n  tm_scale_bar(position=c(0,0.03), text.size = 1) +\n  \n  tm_credits(\"Data source: ONS and London Data store\",\n          position=c(0,0), \n          size = 0.8, \n          align=\"left\") \n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_polygons()`: instead of `style = \"jenks\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_polygons()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(main.title = )`\n! `tm_scale_bar()` is deprecated. Please use `tm_scalebar()` instead.\n[v3-&gt;v4] `tm_credits()`: use `position = tm_pos_in(align.h)` instead of\n`align`.\n\ntm2\n\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"PuBu\" is named\n\"brewer.pu_bu\"\nMultiple palettes called \"pu_bu\" found: \"brewer.pu_bu\", \"matplotlib.pu_bu\". The first one, \"brewer.pu_bu\", is returned.\n\n\n\n\n\n\n\n\n\nPlot them together…\n\nt=tmap_arrange(tm1, tm2, ncol=2)\n\nt\n\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"PuBu\" is named\n\"brewer.pu_bu\"\nMultiple palettes called \"pu_bu\" found: \"brewer.pu_bu\", \"matplotlib.pu_bu\". The first one, \"brewer.pu_bu\", is returned.\n\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"PuBu\" is named\n\"brewer.pu_bu\"\nMultiple palettes called \"pu_bu\" found: \"brewer.pu_bu\", \"matplotlib.pu_bu\". The first one, \"brewer.pu_bu\", is returned.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Geographic information</span>"
    ]
  },
  {
    "objectID": "02_point_patterns.html",
    "href": "02_point_patterns.html",
    "title": "2  Point patterns and autocorrelation",
    "section": "",
    "text": "2.1 Learning outcomes\nBy the end of this practical you should be able to:\nToday’s lecture is available online…at this URL: https://andrewmaclachlan.github.io/Spatial-analysis-of-public-health-data-24-points-and-autocorrelation/#1",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point patterns and autocorrelation</span>"
    ]
  },
  {
    "objectID": "02_point_patterns.html#learning-outcomes",
    "href": "02_point_patterns.html#learning-outcomes",
    "title": "2  Point patterns and autocorrelation",
    "section": "",
    "text": "Describe and evaluate methods for analysing spatial patterns\nExecute data cleaning and manipulation appropriate for analysis\nDetermine the locations of spatial clusters using point pattern analysis methods\nInvestigate the degree to which values at spatial points are similar (or different) to each other\nInterpret the meaning of spatial autocorrelation in spatial data",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point patterns and autocorrelation</span>"
    ]
  },
  {
    "objectID": "02_point_patterns.html#introduction",
    "href": "02_point_patterns.html#introduction",
    "title": "2  Point patterns and autocorrelation",
    "section": "2.2 Introduction",
    "text": "2.2 Introduction\nToday you will learn how to begin to analyse patterns in spatial data with points and spatially continuous observations.\nThe questions we want to answer are:\n\nFor any given London Ward, are Pharmacies distributed randomly or do they exhibit some kind of dispersed or clustered pattern\nAre the values (in this case the density of pharmacies) similar (or dissimilar) across the wards of London.\n\nFor this practical we are considering pharmacies as a single point in time (i.e. assuming that they will always be available when needed). However, some recent analysis Dr Robin Wilson indicates that between 2022 and 2025, pharmacies open past 9pm on a week day has decreased by 95%, with some areas having very poor access. Additional methods such as stdbscan permits a temporal dimension but is beyond our scope here.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point patterns and autocorrelation</span>"
    ]
  },
  {
    "objectID": "02_point_patterns.html#data",
    "href": "02_point_patterns.html#data",
    "title": "2  Point patterns and autocorrelation",
    "section": "2.3 Data",
    "text": "2.3 Data\nLoad our packages\n\nlibrary(spatstat)\nlibrary(tidyverse)\nlibrary(tmap)\nlibrary(here)\nlibrary(sf)\nlibrary(RColorBrewer)\nlibrary(spdep)\n\nGet the pharmacy data: https://datashare.ed.ac.uk/handle/10283/2501 and load it\n\npharmacy &lt;- st_read(\"prac2_data/PharmacyEW/PharmacyEW.shp\")%&gt;%\n  #remove any duplicates\n  distinct()\n\nReading layer `PharmacyEW' from data source \n  `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\Guest\\Oxford_26\\spatial analysis of public health\\spatial-analysis-of-public-health-data\\prac2_data\\PharmacyEW\\PharmacyEW.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 10589 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 137070 ymin: 19187 xmax: 655138 ymax: 653253\nProjected CRS: OSGB36 / British National Grid\n\n#check CRS\nst_crs(pharmacy)\n\nCoordinate Reference System:\n  User input: OSGB36 / British National Grid \n  wkt:\nPROJCRS[\"OSGB36 / British National Grid\",\n    BASEGEOGCRS[\"OSGB36\",\n        DATUM[\"Ordnance Survey of Great Britain 1936\",\n            ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4277]],\n    CONVERSION[\"British National Grid\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",49,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-2,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996012717,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",400000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",-100000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],\n        BBOX[49.75,-9.01,61.01,2.01]],\n    ID[\"EPSG\",27700]]\n\n#check with a plot\ntm_shape(pharmacy) +\n  tm_dots(col = \"blue\")\n\n\n\n\n\n\n\n\nGet the London ward data and load it https://data.london.gov.uk/dataset/statistical-gis-boundary-files-london\n\nwards&lt;- st_read(\"prac2_data/statistical-gis-boundaries-london/statistical-gis-boundaries-london/ESRI/London_Ward_CityMerged.shp\")%&gt;%\n  st_transform(.,27700)\n\nReading layer `London_Ward_CityMerged' from data source \n  `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\Guest\\Oxford_26\\spatial analysis of public health\\spatial-analysis-of-public-health-data\\prac2_data\\statistical-gis-boundaries-london\\statistical-gis-boundaries-london\\ESRI\\London_Ward_CityMerged.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 625 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9\nProjected CRS: OSGB36 / British National Grid\n\n#check CRS\n#st_crs(wards)\n\nCheck the data\n\n#check with a plot\ntm_shape(wards) +\n  tm_polygons(col = NA)\n\n\n\n\n\n\n\n\n\n2.3.1 Wrangle data\nAs we can see above the pharmacy data is for the whole of the UK and we are just interested in London. So, we need to spatially subset our points within our study area…\nHere, the second operator is blank , , - this controls which columns are kept, although I’d rather keep all of them and manipulate with the tidyverse (e.g. select(the columnns i want).\n\npharmacysub &lt;- pharmacy[wards, , op=st_within]\n\n#check with a plot\ntm_shape(wards) +\n  tm_polygons(col = NA, alpha=0.5)+\ntm_shape(pharmacysub) +\n  tm_dots(col=\"blue\")\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_polygons()`: use 'fill' for the fill color of polygons/symbols\n(instead of 'col'), and 'col' for the outlines (instead of 'border.col').\n[v3-&gt;v4] `tm_polygons()`: use `fill_alpha` instead of `alpha`.\nThis message is displayed once every 8 hours.\n\n\n\n\n\n\n\n\n\nWhen we spatial subset data like this there are different topological relations we can specify. The default is intersects, but we could also use pharmacysub &lt;- pharmacy[wards, , op=st_within], with the operator or op set to st_within, to identify points completely within the borough outline, or a variety of other options such as st_overlaps, st_touches, st_contains, st_disjoint. Any possible topological relationship you can think of a function will exist for it…visually this looks like the image below, where each tick denotes the relations that apply to the polygons. Note, that in several cases multiple topological relations would work.\n\n\n\n\n\nTopological relations between vector geometries. Source: Lovelace et al. 2022\n\n\n\n\nWe can also just use the function which will have the indices of where they intersect.\n\n# add sparse=false to get the complete matrix.\nintersect_indices &lt;-st_intersects(wards, pharmacy)\n\nIf you have used a graphic user interface GIS before, this is the same as select by location (e.g. select by location in QGIS), and as using filter from dplyr is the same as select by attribute.\n\n\n\n\n\n\nTip\n\n\n\nWhat is the difference between intersects and within for points like ours?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point patterns and autocorrelation</span>"
    ]
  },
  {
    "objectID": "02_point_patterns.html#point-patterns",
    "href": "02_point_patterns.html#point-patterns",
    "title": "2  Point patterns and autocorrelation",
    "section": "2.4 Point patterns",
    "text": "2.4 Point patterns\nFor point pattern analysis we need a point pattern object (ppp)…within an observation window. This is specific to the spatstat package as we can’t do this with sf, SpatialPolygonsDataFrames or SpatialPointsDataFrames.\nFor this example i will set the boundary to London so we can see the points\n\nboroughs &lt;- st_read(\"prac2_data/statistical-gis-boundaries-london/statistical-gis-boundaries-london/ESRI/London_Borough_Excluding_MHW.shp\")%&gt;%\n  st_transform(.,27700)\n\nReading layer `London_Borough_Excluding_MHW' from data source \n  `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\Guest\\Oxford_26\\spatial analysis of public health\\spatial-analysis-of-public-health-data\\prac2_data\\statistical-gis-boundaries-london\\statistical-gis-boundaries-london\\ESRI\\London_Borough_Excluding_MHW.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 33 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9\nProjected CRS: OSGB36 / British National Grid\n\n#now set a window as the borough boundary\nwindow &lt;- as.owin(boroughs)\nplot(window)\n\n\n\n\n\n\n\n#create a sp object\npharmacysubsp&lt;- pharmacysub %&gt;%\n  as(., 'Spatial')\n#create a ppp object\npharmacysubsp.ppp &lt;- ppp(x=pharmacysubsp@coords[,1],\n                          y=pharmacysubsp@coords[,2],\n                          window=window)\n\npharmacysubsp.ppp %&gt;%\n  plot(.,pch=16,cex=0.5, \n              main=\"Pharmacies in London\")\n\n\n\n\n\n\n\n\n\n2.4.1 Quadrat analysis\nSo as you saw in the lecture, we are interested in knowing whether the distribution of points in our study area differs from ‘complete spatial randomness’ — CSR. That’s different from a CRS! Be careful!\nThe most basic test of CSR is a quadrat analysis. We can carry out a simple quadrat analysis on our data using the quadrat count() function in spatstat. Note, I wouldn’t recommend doing a quadrat analysis in any real piece of analysis you conduct, but it is useful for starting to understand the Poisson distribution…\n\n#First plot the points\n\nplot(pharmacysubsp.ppp,\n     pch=16,\n     cex=0.5, \n     main=\"Pharmacies\")\n\n#now count the points in that fall in a 20 x 20\n#must be run all together otherwise there is a plot issue\n\npharmacysubsp.ppp %&gt;%\n  quadratcount(.,nx = 20, ny = 20)%&gt;%\n    plot(., add=T, col=\"red\")\n\n\n\n\n\n\n\n\nWe want to know whether or not there is any kind of spatial patterning associated with pharmacies in areas of London. If you recall from the lecture, this means comparing our observed distribution of points with a statistically likely (Complete Spatial Random) distribution, based on the Poisson distribution.\nUsing the same quadratcount() function again (for the same sized grid) we can save the results into a table:\n\n#run the quadrat count\nqcount &lt;- pharmacysubsp.ppp %&gt;%\n  quadratcount(.,nx = 20, ny = 20) %&gt;%\n  as.data.frame() %&gt;%\n  dplyr::count(var1=Freq)%&gt;%\n  dplyr::rename(freqquadratcount=n)\n\nOK, so we now have a frequency table — next we need to calculate our expected values. The formula for calculating expected probabilities based on the Poisson distribution is:\n\\[Pr= (X =k) = \\frac{\\lambda^{k}e^{-\\lambda}}{k!}\\] where:\n\nx is the number of occurrences\nλ is the mean number of occurrences\ne is a constant- 2.718\n\n\nsums &lt;- qcount %&gt;%\n  #calculate the total blue plaques (Var * Freq)\n  mutate(total = var1 * freqquadratcount) %&gt;%\n  # then the sums\n  dplyr::summarise(across(everything(), sum))%&gt;%\n  dplyr::select(-var1) \n\nlambda&lt;- qcount%&gt;%\n  #calculate lambda - sum of freq count / sum of total plaques\n  mutate(total = var1 * freqquadratcount)%&gt;%\n  dplyr::summarise(across(everything(), sum)) %&gt;%\n  mutate(lambda=total/freqquadratcount) %&gt;%\n  dplyr::select(lambda)%&gt;%\n  pull(lambda)\n\nCalculate expected using the Poisson formula from above \\(k\\) is the number of pharmacies counted in a square and is found in the first column of our table…\n\nqcounttable &lt;- qcount %&gt;%\n  #Probability of number of plaques in quadrant using the formula \n  mutate(pr=((lambda^var1)*exp(-lambda))/factorial(var1))%&gt;%\n  #now calculate the expected counts based on our total number of plaques\n  #and save them to the table\n  mutate(expected= (round(pr * sums$freqquadratcount, 0)))\n\nPlot them\n\nqcounttable_long &lt;- qcounttable %&gt;% \n  pivot_longer(c(\"freqquadratcount\", \"expected\"), \n               names_to=\"countvs_expected\", \n               values_to=\"value\")\n\nggplot(qcounttable_long, aes(var1, value)) +\n  geom_line(aes(colour = countvs_expected ))\n\n\n\n\n\n\n\n\nCheck for association between two categorical variables - we are looking to see if our freqneucy is similar to the expected (which is random)\nTo check for sure, we can use the quadrat.test() function, built into spatstat. This uses a Chi Squared test to compare the observed and expected frequencies for each quadrant (rather than for quadrant bins, as we have just computed above).\nA Chi-Squared test determines if there is an association between two categorical variables. The higher the Chi-Squared value, the greater the difference.\nIf the p-value of our Chi-Squared test is &lt; 0.05, then we can reject a null hypothesis that says “there is no pattern - i.e. complete spatial randomness - in our data” (think of a null-hypothesis as the opposite of a hypothesis that says our data exhibit a pattern). What we need to look for is a value for p &gt; 0.05. If our p-value is &gt; 0.05 then this indicates that we have CSR and there is no pattern in our points. If it is &lt; 0.05, this indicates that we do have clustering in our points.\n\nteststats &lt;- quadrat.test(pharmacysubsp.ppp, nx = 20, ny = 20)\n\nWarning: Some expected counts are small; chi^2 approximation may be inaccurate\n\n\nChi square with a p value &lt; 0.05 therefore some clustering…but from the plot, this was expected\n\n\n2.4.2 Ripley K\nOne way of getting around the limitations of quadrat analysis is to compare the observed distribution of points with the Poisson random model for a whole range of different distance radii. This is what Ripley’s K function computes. We can conduct a Ripley’s K test on our data very simply with the spatstat package using the kest() function.\nRipley’s K is defined as…\n\\[K(r) = \\lambda^{-1} \\sum{i}\\sum{j}\\frac{I(d_ij&lt;r)}{n}\\]\n\nIn English: Ripley’s K value for any circle radius \\(r\\) =\n\nThe average density of points for the entire study region (of all locations) \\(\\lambda = (n/ \\Pi r^2))\\)\nMultiplied by the sum of the distances \\(d_ij\\) between all points within that search radius, see Dixon page 2 and Amgad et al. 2015\nDivided by the total number of points, n\nI = 1 or 0 depending if \\(d_ij &lt; r\\)\n\n\nThe plot for K has a number of elements that are worth explaining. First, the Kpois(r) line in Red is the theoretical value of K for each distance window (r) under a Poisson assumption of Complete Spatial Randomness. The Black line is the estimated values of K accounting for the effects of the edge of the study area.\nHere, the correction specifies how points towards the edge are dealt with, in this case, border means that points towards the edge are ignored for the calculation but are included for the central points. Section 2.1, here explains the different options.\nWhere the value of K falls above the line, the data appear to be clustered at that distance. Where the value of K is below the line, the data are dispersed…\n\nK &lt;- pharmacysubsp.ppp %&gt;%\n  Kest(., correction=\"border\") %&gt;%\n  plot()\n\n\n\n\n\n\n\n\nThis was sort of expected too due to our previous analysis - suggesting that there is clustering throughout the points.\n\n\n2.4.3 DBSCAN\nQuadrat and Ripley’s K analysis are useful exploratory techniques for telling us if we have spatial clusters present in our point data, but they are not able to tell us WHERE in our area of interest the clusters are occurring. To discover this we need to use alternative techniques. One popular technique for discovering clusters in space (be this physical space or variable space) is DBSCAN. For the complete overview of the DBSCAN algorithm, read the original paper by Ester et al. (1996) or consult the wikipedia page\nDBSCAN requires you to input two parameters: 1. Epsilon - this is the radius within which the algorithm with search for clusters 2. MinPts - this is the minimum number of points that should be considered a cluster\nWe could use the output of Ripley’s K to inform Epsilon or alternatively we can use kNNdistplot() from the dbscan package to find a suitable eps value based on the ‘knee’ in the plot…\nkNNdistplot() take the average distance to all neighbours then ploits the values in ascending order, where the “knee” is indicates a sudden increase in distance to neighbours.\nFor MinPts we typically start with 4. But, try increasing the value of k in kNNdistplot you will notice as you increase it the knee becomes less obvious.\n\nIf we have MinPts too low we have a massive cluster\nIf we have MinPts too high we have a small single cluster\n\n\n#first extract the points from the spatial points data frame\npharmacysub_coords &lt;- pharmacysub %&gt;%\n  st_coordinates(.)%&gt;%\n  as.data.frame()\n\npharmacysub_coords%&gt;%\n  dbscan::kNNdistplot(.,k=20)\n\n\n\n\n\n\n\n\nI started with an eps of 1600 and a minpts of 20..however…the large eps means that the city centre has a massive cluster…this isn’t exactly what i wanted to pull out. Instead i want to identify local clusters of pharmacies so try reducing the eps to 500 and the minpts to 5\nOPTICS will let us remove the eps parameter but running every possible value, however, minpts is always meant to be domain knowledge\nDepending on your points it might be possible to filter the values you aren’t interested in - this isn’t the case here, but for example stop and search data or flytipping could be filtered (well, depending on the extra data within the columns)\n\n#now run the dbscan analysis\ndb &lt;- pharmacysub_coords %&gt;%\n  fpc::dbscan(.,eps = 500, MinPts = 5)\n\n#now plot the results\nplot(db, pharmacysub_coords, main = \"DBSCAN Output\", frame = F)\nplot(wards$geometry, add=T)\n\n\n\n\n\n\n\n\nOur new db object contains lots of info including the cluster each set of point coordinates belongs to, whether the point is a seed point or a border point etc. We can get a summary by just calling the object. We can now add this cluster membership info back into our dataframe\n\npharmacysub_coords&lt;- pharmacysub_coords %&gt;%\n  mutate(dbcluster=db$cluster)\n\nNow create a ggplot2 object from our data\n\npharmacysub_coordsgt0 &lt;- pharmacysub_coords %&gt;%\n  filter(dbcluster&gt;0)\n\ndbplot &lt;- ggplot(data=wards)+\n  geom_sf()+\n  geom_point(data=pharmacysub_coordsgt0, \n                 aes(X,Y, colour=dbcluster, fill=dbcluster))\n#add the points in\n\ndbplot + theme_bw() + coord_sf()\n\n\n\n\n\n\n\n\nNow, this identifies where we have clustering based on our criteria but it doesn’t show where we have similar densities of pharmacies.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point patterns and autocorrelation</span>"
    ]
  },
  {
    "objectID": "02_point_patterns.html#spatial-autocorrelation",
    "href": "02_point_patterns.html#spatial-autocorrelation",
    "title": "2  Point patterns and autocorrelation",
    "section": "2.5 Spatial Autocorrelation",
    "text": "2.5 Spatial Autocorrelation\nIn this section we are going to explore patterns of spatially referenced continuous observations using various measures of spatial autocorrelation. Spatial autocorrelation is a measure of similarity between nearby data. We need to add all the points to the London wards then compute a density per ward could also use population here too! or some other data that can give us meaningful comparisons for our variable of interest.\n\n2.5.1 Wrangle data\nTo do so we need to use a spatial join!\nThis is similar to the the joins (e.g. left joins) we explored with attribute data but here we just want to join datasets together based on their geometry and keep all their attribute data, this is useful in the code below where i want to join the pharmacies to the LSOA data\nThe output will be a massive dataset where each pharmacy will be a new row and will retain the attributes of the pharmacy data but also append the attribute of the LSOA.\nThe spatial join function,st_join(), defaults to a left join, so in this case the LSOA data is the left dataset and all the right data has been appended to it. If the left data (LSOA) had no matches (so no pharmacies) it would still appear in the final dataset. The default argument for this is st_intersects but we could also use other topological relationship functions such as st_within() instead…\n\nexample&lt;-st_intersects(wards, pharmacysub)\n\nexample\n\nSparse geometry binary predicate list of length 625, where the\npredicate was `intersects'\nfirst 10 elements:\n 1: (empty)\n 2: 61, 65\n 3: 53, 59\n 4: 45, 54, 68\n 5: 44, 48, 49, 62\n 6: (empty)\n 7: 60, 66\n 8: 57\n 9: 55\n 10: 43, 56\n\n\nHere the polygon with the ID of 7, Chessington North and Hook, has two pharmacies within it…we can check this with st_join (or using QGIS by opening the data).\nBut note the ID column added is different to the ID of the data…open pharmacysub from the environment window and you will see the IDs that were returned in st_intersects(). The new IDs above take the data and apply 1 to n, where n is the end of the data. So if we subset our pharmacies on row 60 and 66 it will match our result here.\n\ncheck_example &lt;- wards%&gt;%\n  st_join(pharmacysub)%&gt;%\n  filter(GSS_CODE==\"E05000404\")\n\ncheck_example\n\nSimple feature collection with 2 features and 18 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 517153.7 ymin: 164037.4 xmax: 519553 ymax: 165447.1\nProjected CRS: OSGB36 / British National Grid\n                          NAME  GSS_CODE HECTARES NONLD_AREA LB_GSS_CD\n7   Chessington North and Hook E05000404   192.98          0 E09000021\n7.1 Chessington North and Hook E05000404   192.98          0 E09000021\n                 BOROUGH POLY_ID  ID      PCTName         NamePharm\n7   Kingston upon Thames   50530 130 Kingston PCT Alliance Pharmacy\n7.1 Kingston upon Thames   50530 136 Kingston PCT Alliance Pharmacy\n           Address1    Address2    Address3 Address4 PostCode LSOAName Easting\n7   11 North Parade Chessington      Surrey     &lt;NA&gt;  KT9 1QL     &lt;NA&gt;  518483\n7.1 4 Arcade Parade        Hook Chessington   Surrey  KT9 1AB     &lt;NA&gt;  518015\n    Northing                       geometry\n7     164147 POLYGON ((517175.3 164077.3...\n7.1   164508 POLYGON ((517175.3 164077.3...\n\n\nNow we just take the length of each list per polygon and add this as new column…\n\npoints_sf_joined &lt;- wards%&gt;%\n  mutate(n = lengths(st_intersects(., pharmacysub)))%&gt;%\n  janitor::clean_names()%&gt;%\n  #calculate area\n  mutate(area=st_area(.))%&gt;%\n  #then density of the points per ward\n  mutate(density=n/area)\n\nNow map density\n\npoints_sf_joined&lt;- points_sf_joined %&gt;%                    \n  group_by(gss_code) %&gt;%         \n  summarise(density = first(density),\n          name  = first(gss_code))\n\ntm_shape(points_sf_joined) +\n    tm_polygons(\"density\",\n        style=\"jenks\",\n        palette=\"PuOr\",\n        midpoint=NA)\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_polygons()`: instead of `style = \"jenks\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'midpoint', 'palette' (rename to 'values')\n  to 'tm_scale_intervals(&lt;HERE&gt;)'\nFor small multiples, specify a 'tm_scale_' for each multiple, and put them in a\nlist: 'fill'.scale = list(&lt;scale1&gt;, &lt;scale2&gt;, ...)'\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"PuOr\" is named\n\"brewer.pu_or\"\nMultiple palettes called \"pu_or\" found: \"brewer.pu_or\", \"matplotlib.pu_or\". The first one, \"brewer.pu_or\", is returned.\n\n\n\n\n\n\n\n\n\nSo, from the map, it looks as though we might have some clustering of pharmacies in the centre of London and a few other places, so let’s check this with Moran’s I and some other statistics.\nAs we saw in the session we need to create a spatial weight matrix…to do so we need centroid points first…to then compute the neighbours of each centroid…\n\ncoordsW &lt;- points_sf_joined%&gt;%\n  st_centroid()%&gt;%\n  st_geometry()\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n#check, alpha is transparency \ntm_shape(points_sf_joined) +\n    tm_polygons(alpha=0.1)+\ntm_shape(coordsW) +\n  tm_dots(col = \"blue\")\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_polygons()`: use `fill_alpha` instead of `alpha`.\n\n\n\n\n\n\n\n\n\n\n\n2.5.2 Weight matrix\nNow we need to generate a spatial weights matrix (remember from the lecture). We’ll start with a simple binary matrix of queen’s case neighbours (otherwise known as Contiguity edges corners). This method means that polygons with a shared edge or a corner will be included in computations for the target polygon…A spatial weight matrix represents the spatial element of our data, this means we are trying to conceptualize and model how parts of the data are linked (or not linked) to each other spatially, using rules that we will set.\nIf the features share a boundary they are contiguous, this can also be classed as only common boundaries — a rook (like chess a rook can move forwards or side wards) or any point in common (e.g. corners / other boundaries) — a queen (like chess a queen can move forwards, backwards or on a diagonal).\nAlternatively instead of using contiguous relationships you can use distance based relationships. This is frequently done with k nearest neighbours in which k is set to the closest observations. e.g. K=3 means the three closest observations.\nIn the first instance we must create a neighbours list — which is a list of all the neighbours. To do so we will use polygon to neigbhour function poly2nb() with the argument queen=T saying we want a to use Queens case. Let’s see a summary of the output\n\n#create a neighbours list\nlward_nb &lt;- points_sf_joined %&gt;%\n  poly2nb(., queen=T)\n\nHave a look at the summary of neighbours - average is 5.88\n\nsummary(lward_nb)\n\nNeighbour list object:\nNumber of regions: 625 \nNumber of nonzero links: 3680 \nPercentage nonzero weights: 0.94208 \nAverage number of links: 5.888 \nLink number distribution:\n\n  1   2   3   4   5   6   7   8   9  10  11  12 \n  1   4  15  72 162 182 112  55  14   4   2   2 \n1 least connected region:\n380 with 1 link\n2 most connected regions:\n313 612 with 12 links\n\n\nplot them - we can’t use tmap as it isn’t the right class (e.g. it’s not an sf object)\n\nplot(lward_nb, st_geometry(coordsW), col=\"red\")\n#add a map underneath\nplot(points_sf_joined$geometry, add=T)\n\n\n\n\n\n\n\n\nNext take our weight list and make it into a matrix through the neigbhour to matix function (nb2mat) …style here denotes the weight type:\n\nB is the basic binary coding (1/0)\nW is row standardised (sums over all links to n)\nC is globally standardised (sums over all links to n)\nU is equal to C divided by the number of neighbours (sums over all links to unity)\nS is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\n\n\n#create a spatial weights matrix from these weights\nlward_lw &lt;- lward_nb %&gt;%\n  nb2mat(., style=\"W\",  zero.policy=TRUE)\n\nI have used row…based on the lecture what should the value of the weights sum to?\n\nsum(lward_lw)\n\n[1] 625\n\n\n\n\n2.5.3 Moran’s I\nMoran’s I requires a spatial weight list type object as opposed to matrix, this is simply…\n\nlward_lw &lt;- lward_nb %&gt;%\n  nb2listw(., style=\"W\",  zero.policy=TRUE)\n\nNow let’s run Moran’s I. This test tells us whether the values at neighbouring sites are\n\nsimilar to the target site (giving a Moran’s I close to 1)\nthe value of the target is different to the neighbours (close to -1)\nthe values are random (0)\n\n\ni_lWard_global_density &lt;- points_sf_joined %&gt;%\n  pull(density) %&gt;%\n  as.vector()%&gt;%\n  moran.test(., lward_lw, zero.policy = TRUE)\n\nThe argument zero.policy = TRUE allows neighbours with no values.\n\n\n2.5.4 Geary’s C\nGeary’s C tells us whether similar values or dissimilar values are clustering.\nIt falls between 0 and 2; 1 means no spatial autocorrelation, &lt;1 - positive spatial autocorrelation or similar values clustering, &gt;1 - negative spatial autocorreation or dissimilar values clustering)\n\nc_lward_global_density &lt;- \n  points_sf_joined %&gt;%\n  pull(density) %&gt;%\n  as.vector()%&gt;%\n  geary.test(., lward_lw, zero.policy = TRUE)\n\n\n2.5.4.1 Comparison\nWe should now have an idea about what the main differences between Moran’s I and Geary’s C are, and how to interpret the outputs. But how do these look in an equation. I really dislike using equations as a description is much easier to understand. However, it’s important to see how similar these two autocorrelcation measures are.\nDownload the worked Moran’s I and Geary’s C excel document and consider it in relation to these formulas.\n\n\n\n\n\n\n\nMoran’s I\nGeary’s C\n\n\n\n\n\\[I = \\frac{n}{W} \\cdot \\frac{\\sum_{i} \\sum_{j} w_{ij} (z_i - \\bar{z})(z_j - \\bar{z})}{\\sum_{i} (z_i - \\bar{z})^2}\\]\n\\[C = \\frac{(n - 1)}{2W} \\cdot \\frac{\\sum_{i} \\sum_{j} w_{ij} (z_i - z_j)^2}{\\sum_{i} (z_i - \\bar{z})^2}\\]\n\n\nWhere: ( n ) = number of spatial units ( W = w_{ij} ) = sum of all spatial weights (1 if row standarised)  ( z_i ), ( z_j ) = value at location i and j ( {z} ) = mean of all values (global mean)  ( w_{ij} ) = spatial weight between i and j\nWhere: ( n ) = number of spatial units ( W = w_{ij} ) = sum of all spatial weights (1 if row standarised) ( z_i ), ( z_j ) = values at locations i and j ( {z} ) = mean of all values (global mean)  ( w_{ij} ) = spatial weight between i and j\n\n\n\n\nYou will notice that the key difference is with the numerator\n\n\n\n\n\n\n\n\n\nMeasure\nWhat it compares\nMathematical focus\n\n\n\n\nMoran’s I\nValue of unit vs global mean\nCross-product: \\((z_i - \\bar{z})(z_j - \\bar{z})\\)\n\n\nGeary’s C\nValue of unit vs neighbour\nSquared difference: \\((z_i - z_j)^2\\)\n\n\n\n\n\n2.5.4.2 Key message\nFor Moran’s I\n\nwe are looking at the product of deviaitions from the global mean (subtracting our central value \\((z_i - \\bar{z})\\) and niehgbour \\((z_j - \\bar{z})\\) from the global mean individually, then multiplying them).\nIf both deviations are positive then this suggests clustering\n\nFor Geary’s C\n\nThis changes to the difference between the central value and the neighbour, squared \\((z_i - z_j)^2\\)\nA large difference means more dissimilarity between the values\n\n\n\n\n2.5.5 Getis Ord General G\nGetis Ord General G…? This tells us whether high or low values are clustering. If G &gt; Expected = High values clustering; if G &lt; expected = low values clustering. Again, this is conceptually similar to Moran’s I and Geary’s C but i won’t go into the detail here. The focus is now on the weighted product of values - this means multiplying neighbours and weighting them with our weight matrix (in the numerator). If a high value is near a high value it returns a larger product.\n\ng_lward_global_density &lt;- \n  points_sf_joined %&gt;%\n  pull(density) %&gt;%\n  as.vector()%&gt;%\n  globalG.test(., lward_lw, zero.policy = TRUE)\n\nWarning in globalG.test(., lward_lw, zero.policy = TRUE): Binary weights\nrecommended (especially for distance bands)\n\n\nBased on the results write down what you can conclude here….",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point patterns and autocorrelation</span>"
    ]
  },
  {
    "objectID": "02_point_patterns.html#local-indicies-of-spatial-autocorrelation",
    "href": "02_point_patterns.html#local-indicies-of-spatial-autocorrelation",
    "title": "2  Point patterns and autocorrelation",
    "section": "2.6 Local Indicies of Spatial Autocorrelation",
    "text": "2.6 Local Indicies of Spatial Autocorrelation\n\n2.6.1 Moran’s I\nWe can now also calculate local versions of the Moran’s I statistic (e.g. for each Ward) and a Getis Ord (G_{i}^{}) statistic to spatially see and map where* we have clusters…\nThis iterates over each individual polygon (or point), using just the neighbours for the spatial unit from our weight matrix.\nLocal Moran’s I is:\n\nThe difference between a value and the global mean \\((z_i - \\bar{z})\\) * the difference between a neighbour and the global mean \\(w_{ij}(z_j - \\bar{z})\\), weighted by the weight matrix\nThis is standarised by the global variance \\((m_2)\\) to make sure values are comparable.\n\n[ I_i = {j} w{ij}(z_j - {z}) ]\nWhere:\n\n( I_i ): Local Moran’s I statistic for unit i\n\n( z_i ): Value at location i\n\n( z_j ): Value at neighboring location j\n\n( {z} ): Mean of all (global) values\n\n( w_{ij} ): Spatial weight between locations i and j\n\n( m_2 = _{k=1}^{n} (z_k - {z})^2 ): Global variance\n\n(z_k): is each individual value in the dataset\n\n\nIt returns several columns, of most interest is the Z score. A Z-score is how many standard deviations a value is away (above or below) from the mean. This allows us to state if our value is significantly different than expected value at this location considering the neighours.\nWe are comparing our value of Moran’s I to that of an expected value (computed from a separate equation that uses the spatial weight matrix, and therefore considers the neighbouring values). We are expecting our value of Moran’s I to be in the middle of the distribution of the expected values. These expected values follow a normal distribution, with the middle part representing complete spatial randomness. This is typically between &lt; -1.65 or &gt; +1.65 standard deviations from the mean\nThe null hypothesis is always there is complete spatial randomness. A null hypothesis means:\n\nno statistical significance exists in a set of given observations\n\nIf our value is towards the tails of the distribution then it is unlikely that the value is completely spatially random and we can reject the null hypothesis…as it is not what we expect at this location.\nIn the example where we use a z-score of &gt;2.58 or &lt;-2.58 we interpret this as…\n…&gt; 2.58 or &lt;-2.58 standard deviations away from the mean are significant at the 99% level…this means there is a &lt;1% chance that autocorrelation is not present\nThe Global vs location spatial autocorrelation resource goes through the specific formulas here, but the most important parts are knowing\n\nWhat we are comparing values to in Local Moran’s I\nWhat the results mean\nWhy the results could be important\n\n\ni_lward_local_density &lt;- points_sf_joined %&gt;%\n  pull(density) %&gt;%\n  as.vector()%&gt;%\n  localmoran(., lward_lw, zero.policy = TRUE)%&gt;%\n  as_tibble()\n\nThis outputs a table, so we need to append that back to our sf of the wards to make a map…\n\npoints_sf_joined &lt;- points_sf_joined %&gt;%\n    mutate(density_i =as.numeric(i_lward_local_density$Ii))%&gt;%\n    mutate(density_iz =as.numeric(i_lward_local_density$Z.Ii))%&gt;%\n    mutate(p =as.numeric(i_lward_local_density$`Pr(z != E(Ii))`))\n\nWe’ll set the breaks manually based on the rule that data points:&gt;2.58 or &lt;-2.58 standard deviations away from the mean are significant at the 99% level (&lt;1% chance that autocorrelation not present); &gt;1.96 - &lt;2.58 or &lt;-1.96 to &gt;-2.58 standard deviations are significant at the 95% level (&lt;5% change that autocorrelation not present). &gt;1.65 = 90% etc…like we saw in the lecture…\n\nbreaks1&lt;-c(-1,-0.5,0,0.5,1)\n\nbreaks2&lt;-c(-1000,-2.58,-1.96,-1.65,1.65,1.96,2.58,1000)\n\nNow create a new diverging colour brewer palette and reverse the order using rev() (reverse) so higher values correspond to red - see https://www.r-graph-gallery.com/38-rcolorbrewers-palettes.html\n\nMoranColours&lt;- rev(brewer.pal(8, \"RdGy\"))\n\nRemember Moran’s I - test tells us whether we have clustered values (close to 1) or dispersed values (close to -1) of similar values based on the spatial weight matrix that identifies the neighoburs.\n\n\n\n\n\n\nTip\n\n\n\nIt should not be interpreted as a “hot spot”.\n\n\nThe z-score shows were this is unlikely because of complete spatial randomness and so we have spatial clustering (either clustering of similar or dissimilar values depending on the Moran’s I value) within these locations…\n\ntm_shape(points_sf_joined) +\n    tm_polygons(\"density_i\",\n        style=\"fixed\",\n        breaks=breaks1,\n        palette=MoranColours,\n        midpoint=NA,\n        title=\"Local Moran's I\")\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_polygons()`: instead of `style = \"fixed\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'breaks', 'midpoint', 'palette' (rename to\n  'values') to 'tm_scale_intervals(&lt;HERE&gt;)'\nFor small multiples, specify a 'tm_scale_' for each multiple, and put them in a\nlist: 'fill'.scale = list(&lt;scale1&gt;, &lt;scale2&gt;, ...)'\n[v3-&gt;v4] `tm_polygons()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n\n\nWarning: Values have found that are less than the lowest break. They are\nassigned to the lowest interval\n\n\nWarning: Values have found that are higher than the highest break. They are\nassigned to the highest interval\n\n\n\n\n\n\n\n\ntm_shape(points_sf_joined) +\n    tm_polygons(\"density_iz\",\n        style=\"fixed\",\n        breaks=breaks2,\n        palette=MoranColours,\n        midpoint=NA,\n        title=\"Local Moran's I Z-score\")\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_polygons()`: instead of `style = \"fixed\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'breaks', 'midpoint', 'palette' (rename to\n  'values') to 'tm_scale_intervals(&lt;HERE&gt;)'\nFor small multiples, specify a 'tm_scale_' for each multiple, and put them in a\nlist: 'fill'.scale = list(&lt;scale1&gt;, &lt;scale2&gt;, ...)'[v3-&gt;v4] `tm_polygons()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n\n\n\n\n\n\n\n\n\n\n\n2.6.2 Getis Ord \\(G_{i}^{*}\\)\nWhat about the Getis Ord \\(G_{i}^{*}\\) statistic for hot and cold spots…\nThis is a very similar concept to Local Moran’s I except it just returns a z-score…remember that a z-score shows how many standard deviations a value (our value) is away (above or below) from the mean (of the expected values)\nUltimately a z-score is defined as:\n\\[Z = \\frac{x-\\mu}{\\sigma}\\] Where:\n\n\\(x\\) = the observed value\n\\(\\mu\\) = the mean of the sample\n\\(\\sigma\\) = standard deviation of sample\n\nNote, consult the Global vs location spatial autocorrelation resource for how this is computed in Local Moran’s I if you are interested, although interpretation is the most important part here.\nHowever, in the case of Getis Ord (G_{i}^{*}) this is the local sum (of the neighbourhood. This means for each point \\(i\\) sum all of the surrounding values based on the weight matrix) \\(\\sum_{j} w_{ij} x_j\\) compared to the sum of all features in the entire dataset \\(\\sum_{j} x_j\\)\n[ G_i^* = ]\nIn Moran’s I this is just the value of the spatial unit (e.g. polygon of the ward) compared to the neighbouring units.\nHere, to be significant (or a hot spot) we will have a high value surrounded by high values. The local sum of these values will be different to the expected sum (think of this as all the values in the area) then where this difference is large we can consider it to be not by chance…\nThe same z-score criteria then applies as before..\nThis summary from L3 Harris nicely summaries the Getis Ord \\(G_{i}^{*}\\) output…\n\nThe result of Getis Ord \\(G_{i}^{*}\\) analysis is an array of Z-scores, one for each pixel [or polygon], which is the number of standard deviations that the pixel [or polygon] and its neighbors are from the global mean. High Z-scores indicate more intense clustering of high pixel values, indicating hot spots. Low Z-scores indicate more intense clustering of low values, indicating cold spots. Individual pixels with high or low values by themselves might be interesting but not necessarily significant.\n\n\ngi_lward_local_density &lt;- points_sf_joined %&gt;%\n  pull(density) %&gt;%\n  as.vector()%&gt;%\n  localG(., lward_lw)\n\npoints_sf_joined &lt;- points_sf_joined %&gt;%\n  mutate(density_G = as.numeric(gi_lward_local_density))\n\nNote that because of the differences in Moran’s I and Getis Ord \\(G_{i}^{*}\\) there will be differences between polygons that are classed as significant.\nAnd map the outputs…\n\ngicolours&lt;- rev(brewer.pal(8, \"RdBu\"))\n\n#now plot on an interactive map\ntm_shape(points_sf_joined) +\n    tm_polygons(\"density_G\",\n        style=\"fixed\",\n        breaks=breaks2,\n        palette=gicolours,\n        midpoint=NA,\n        title=\"Gi* Z score\")\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_polygons()`: instead of `style = \"fixed\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'breaks', 'midpoint', 'palette' (rename to\n  'values') to 'tm_scale_intervals(&lt;HERE&gt;)'\nFor small multiples, specify a 'tm_scale_' for each multiple, and put them in a\nlist: 'fill'.scale = list(&lt;scale1&gt;, &lt;scale2&gt;, ...)'\n[v3-&gt;v4] `tm_polygons()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n\n\n\n\n\n\n\n\n\n\n\n2.6.3 Summary\n\n\n\n\n\n\n\n\nStatistic\nWhat is compared\nPurpose\n\n\n\n\nLocal Moran’s I\nDeviation of a point from neibhouring values\nIdentify clusters or spatial outliers (similar/dissimilar)\n\n\nMoran’s I Z-score\nObserved Moran’s I vs expected under randomness\nMeasure statistical significance of spatial autocorrelation at each location; high values surrounded by high values or low values surrounded by low values\n\n\n**Getis-Ord (G_i^*)**\nLocal weighted sum vs global sum of all values\nIdentify hotspots (high values clustering) or coldspots (low values clustering)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point patterns and autocorrelation</span>"
    ]
  },
  {
    "objectID": "02_point_patterns.html#note",
    "href": "02_point_patterns.html#note",
    "title": "2  Point patterns and autocorrelation",
    "section": "2.7 Note",
    "text": "2.7 Note\nIn the analysis you might see a Moran plot where the values of our variable (density of pharmacies) and plotted against (on the y axis) the spatially lagged version (the average value of the same attribute at neighboring locations). However this plot below shows the value of density in relation to the spatial weight matrix…\nThis is useful as we can express the level of spatial association of each observation with its neighboring ones. Points in the upper right (or high-high) and lower left (or low-low) quadrants indicate positive spatial association of values that are higher and lower than the sample mean, respectively. The lower right (or high-low) and upper left (or low-high) quadrants include observations that exhibit negative spatial association; that is, these observed values carry little similarity to their neighboring ones.\nSource: [STAT user guide](https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.4/statug/statug_variogram_details31.htm#:~:text=The%20Moran%20scatter%20plot%20(Anselin,known%20as%20the%20response%20axis.)\n\nmoran_plot_lward_global_density &lt;- points_sf_joined %&gt;%\n  pull(density)%&gt;%\n  as.vector()%&gt;%\n  moran.plot(., lward_lw)\n\n\n\n\n\n\n\n\nWhen you see Moran’s I out in the wild you will come across maps with:\n\nhigh values surrounded by high values (HH)\nlow values nearby other low values (LL)\nlow values among high values (LH)\nhigh values among low values (HL)\n\nHere, we use the values we have, of density and Moran’s I, compared to the mean of density and Moran’s I (termed centering). Where the:\n\nvalue of density is greater than 0 and the value of Moran’s I is greater than 0 then high values (of density) are surrounded by other high values (from Moran’s I)= HH\nvalue of density is lower than 0 and the value of Moran’s I is lower than 0 then low values (of density) are surrounded by other low values (from Moran’s I) = LL\nvalue of density is lower than 0 and the value of Moran’s I is higher than 0 then low values (of density) are surrounded by high values (from Moran’s I) = LH\nvalue of density is higher than 0 and the value of Moran’s I is lower than 0 then high values (of density) are surrounded by high values (from Moran’s I) =HL\n\n\nsignif &lt;- 0.1\n\n\n# centers the variable of interest around its mean\npoints_sf_joined2 &lt;- points_sf_joined %&gt;%\n  mutate(mean_density = density- mean(density))%&gt;%\n  mutate(mean_density = as.vector(mean_density))%&gt;%\n  mutate(mean_densityI= density_i - mean(density_i))%&gt;%\n  mutate(quadrant = case_when(mean_density&gt;0 & mean_densityI &gt;0 ~ 4,\n         mean_density&lt;0 & mean_densityI &lt;0 ~ 1,\n         mean_density&lt;0 & mean_densityI &gt;0 ~ 2,\n         mean_density&gt;0 & mean_densityI &lt;0 ~ 3))%&gt;%\n  mutate(quadrant=case_when(p &gt; signif ~ 0, TRUE ~ quadrant))\n\nbrks &lt;- c(0,1,2,3,4,5)\ncolors &lt;- c(\"white\",\"blue\",\"skyblue\",\"pink\",\"red\")\n\n\ntm_shape(points_sf_joined2) +\n    tm_polygons(\"quadrant\",\n        style=\"fixed\",\n        breaks=brks,\n        labels = c(\"insignificant\",\"low-low\",\"low-high\",\"high-low\",\"high-high\"),\n        palette=colors,\n        title=\"Moran's I HH etc\")\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_polygons()`: instead of `style = \"fixed\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'breaks', 'palette' (rename to 'values'),\n  'labels' to 'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_polygons()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n\n\n\n\n\n\n\n\n\nSource: Carlos Mendez\nThis might seem somewhat confusing as if we look in the South East we have low values of Getis Ord \\(G_{i}^{*}\\) yet we have shown that these are low (density) and high Moran’s I. But as Matthew Peeples concisely summarises remember…\n\nMoran’s I is a measure of the degree to which the value at a target site is similar to values at adjacent sites. Moran’s I is large and positive when the value for a given target (or for all locations in the global case) is similar to adjacent values and negative when the value at a target is dissimilar to adjacent values.\nGetis Ord \\(G_{i}^{*}\\) identifies areas where high or low values cluster in space. It is high where the sum of values within a neighborhood of a given radius or configuration is high relative to the global average and negative where the sum of values within a neighborhood are small relative to the global average and approaches 0 at intermediate values.\n\nSo here we have a high Moran’s I as the values around it are similar (probably all low) but a low Getis Ord as the values within the local area are low relative to the global average.\n\n2.7.1 Consider what this all means\n\nDo you think Pharmacies take into account the ward they are in when they open?\nDo you think people only go to pharmacies within their ward?\nPharmacies don’t exhibit complete spatial randomness and there seems to be clustering in certain locations….\nAnother question we could move onto is now are pharmacies locating around a need (e.g. health outcomes) and does that mean some of the population in London have poor access or choice.\nFor example….we could now look to see if there is clustering of “Deaths from causes considered preventable, under 75 years, standardised mortality ratio” and if that aligns with (or can be explained by) access / clusters of pharmacies.\nIf you wanted to do this (e.g. see if the ratio was clustered), the data is in a somewhat unfriendly format so you’d need to:\n\nDownload the public health profile ward data:\nRead the Indicator definitions for the last row, specifically column G which tells us how it’s created.\nFilter the data based on the indicator value of 93480 (from the Indicator definitions)\nFilter out the London wards - as we did here and join it to our spatial feature.\nRun a Moran’s I or some other spatial autocorrelation\nJoin this back to our spatial feature that has the Moran’s I for pharmacy density\nDecide what to plot - does dispersion of pharmacies occur in the same spatial units as clustering of deaths considered preventable? This could simply be two maps and a table.\n\nShould it be a requirement to have access…well if we consult the “2022 Pharmacy Access Scheme: guidance” then yes it appears so but…“Pharmacies in areas with dense provision of pharmacies remain excluded from the scheme”\n\n\n\n2.7.2 Extensions\n\nThe use of OPTICS\nWe have spatial autocorrelation now can we try and model local differences in density of pharmacies - i.e. what factors might (or might not) explain the difference here\nOr perhaps does the distance to pharmacies assist in explaining deaths from causes considered preventable, under 75 years. Similarly, we could even use this as a dummy variable (yes/no the areas is / is not in a cluster of pharmacies) in a regression model.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point patterns and autocorrelation</span>"
    ]
  },
  {
    "objectID": "03_obesity.html",
    "href": "03_obesity.html",
    "title": "3  Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London",
    "section": "",
    "text": "4 Introduction\nIn this practical we will learn how to explore the spatial distribution of points in the presence of covariates. We will use fast-food outlets and schools in London as an example. Fast-food outlets are known to contribute to childhood obesity, and their proximity to schools can have a significant impact on children’s health. By investigating the spatial distribution of fast-food outlets in relation to schools, we can identify areas where public health interventions are needed to reduce childhood obesity.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London</span>"
    ]
  },
  {
    "objectID": "03_obesity.html#install-packages-if-you-need-to",
    "href": "03_obesity.html#install-packages-if-you-need-to",
    "title": "3  Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London",
    "section": "5.1 Install packages (if you need to)",
    "text": "5.1 Install packages (if you need to)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London</span>"
    ]
  },
  {
    "objectID": "03_obesity.html#load-packages",
    "href": "03_obesity.html#load-packages",
    "title": "3  Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London",
    "section": "5.2 Load packages",
    "text": "5.2 Load packages\n\nlibrary(classInt)\nlibrary(curl)\nlibrary(dbscan)\nlibrary(downloader)\nlibrary(dplyr)\nlibrary(fasterize)\nlibrary(foreign)\nlibrary(fpc)\nlibrary(fs)\nlibrary(geojsonio)\nlibrary(ggplot2)\nlibrary(h3jsr)\nlibrary(h3r)\nlibrary(here)\nlibrary(janitor)\nlibrary(osmdata)\nlibrary(osmextract)\nlibrary(plotly)\nlibrary(r5r)\nlibrary(raster)\nlibrary(readxl)\nlibrary(RColorBrewer)\nlibrary(sf)\nlibrary(sp)\nlibrary(spData)\nlibrary(spatstat)\nlibrary(spatstat.geom)\nlibrary(stars)\nlibrary(terra)\nlibrary(tidyterra)\nlibrary(tidyverse)\nlibrary(tmap)\nlibrary(tmaptools)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London</span>"
    ]
  },
  {
    "objectID": "03_obesity.html#import-necessary-data",
    "href": "03_obesity.html#import-necessary-data",
    "title": "3  Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London",
    "section": "5.3 Import necessary data",
    "text": "5.3 Import necessary data\nFirst some boundary data for London which may come in useful later.\n\n5.3.1 Read in shapefile of London\n\n# Define the URL of the ZIP file\nurl &lt;- \"https://data.london.gov.uk/download/statistical-gis-boundary-files-london/9ba8c833-6370-4b11-abdc-314aa020d5e0/statistical-gis-boundaries-london.zip\"\n\n# Function to download, extract, and load a shapefile\nload_shapefile &lt;- function(zip_url, shapefile_relative_path) {\n  temp &lt;- tempfile(fileext = \".zip\")\n  curl_download(zip_url, temp)\n  \n  temp_dir &lt;- tempfile()\n  dir.create(temp_dir)\n  unzip(temp, exdir = temp_dir)\n  \n  shapefile_path &lt;- file.path(temp_dir, shapefile_relative_path)\n  \n  if (!file.exists(shapefile_path)) {\n    stop(paste(\"Shapefile not found:\", shapefile_relative_path))\n  }\n  \n  return(st_read(shapefile_path))\n}\n\n# Load London Boroughs shapefile\nlondon_boroughs &lt;- load_shapefile(url, \"statistical-gis-boundaries-london/ESRI/London_Borough_Excluding_MHW.shp\") %&gt;% \nst_transform(london_boroughs, crs = 4326)\n\nReading layer `London_Borough_Excluding_MHW' from data source \n  `C:\\Users\\Andy\\AppData\\Local\\Temp\\RtmpcDPNFj\\file3504172f7e28\\statistical-gis-boundaries-london\\ESRI\\London_Borough_Excluding_MHW.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 33 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9\nProjected CRS: OSGB36 / British National Grid\n\nqtm(london_boroughs)\n\n\n\n\n\n\n\n# Load LSOA shapefile\nlondon_LSOA &lt;- load_shapefile(url, \"statistical-gis-boundaries-london/ESRI/LSOA_2011_London_gen_MHW.shp\") %&gt;% \nst_transform(london_boroughs, crs = 4326)\n\nReading layer `LSOA_2011_London_gen_MHW' from data source \n  `C:\\Users\\Andy\\AppData\\Local\\Temp\\RtmpcDPNFj\\file35044ec1535e\\statistical-gis-boundaries-london\\ESRI\\LSOA_2011_London_gen_MHW.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 4835 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 503574.2 ymin: 155850.8 xmax: 561956.7 ymax: 200933.6\nProjected CRS: OSGB36 / British National Grid\n\n#tmap_options(check.and.fix = TRUE)\nqtm(london_LSOA)\n\n\n\n\n\n\n\n\n\n\n5.3.2 Download and read in the Index of Multiple Deprivation (IMD) data\nDownload from: https://communitiesopendata-communities.hub.arcgis.com/datasets/45e05901e0a14cca9ab180975e2e8194_0/explore?location=52.837574%2C-2.489783%2C6\n\n#Load Index of Multiple Deprivation data. This can either be accessed from the CDRC website or downloaded directly from the government website.\n#Downloaded data from the CDRC Index of Multiple Deprivation (IMD) website: https://data.cdrc.ac.uk/dataset/index-multiple-deprivation-imd/resource/data-english-imd-2019\n#English IMD 2019. 2011 LSOA geography. Shapefile format.\n\n#IMD_London &lt;- st_read(here::here(\"data\",\"English IMD 2019\", \"IMD_2019.shp\")) %&gt;%\n#  clean_names()%&gt;%\n#  dplyr::filter(str_detect(la_dcd, \"^E09\"))\n\nIMD_London &lt;- st_read(\"prac3_data/Indices_of_Multiple_Deprivation_2019.geojson\") %&gt;%\n  st_transform(27700) %&gt;% \n  clean_names() %&gt;%\n  dplyr::filter(str_detect(la_dcd, \"^E09\"))\n\nReading layer `Indices_of_Multiple_Deprivation_2019' from data source \n  `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\Guest\\Oxford_26\\spatial analysis of public health\\spatial-analysis-of-public-health-data\\prac3_data\\Indices_of_Multiple_Deprivation_2019.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 32844 features and 64 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -6.418524 ymin: 49.86474 xmax: 1.762942 ymax: 55.81107\nGeodetic CRS:  WGS 84\n\n#Plot to see IMD deciles and check if all is correct\n#tmap_options(check.and.fix = TRUE)\n\ntmap_mode(\"plot\")\n\n#Plot map\nmap &lt;- tm_shape(IMD_London) +\n  tm_polygons(col = \"imd_decile\", \n              style = \"pretty\", \n              n = 10, \n              palette = \"RdYlBu\", \n              title = \"IMD Decile\", \n              border.alpha = 0) +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) \n\n# +\n#   tm_shape(lnd_ff_pts) +\n#   tm_dots(col = \"black\",\n#           size = 0.05,   \n#           border.lwd = 0, \n#           title = \"Fast-food outlets\",  # Label for the dots\n#           legend.show = TRUE)  \n\nmap\n\n\n\n\n\n\n\n\n\n\n5.3.3 Load Fast-food points from Open Street Map (OSM)\nAs mentioned in the lecture, Open Street Map (OSM) is a great source of geospatial data. Here we will use the osmdata package to query OSM for fast-food outlets in London.\nLater on in the practical we will ask you to find alternative points to analyse. Knowing what to search for can be a bit tricky, but the osmdata package has a function called available_features() which can be used to search for different types of points of interest. You can also use the available_tags() function to search for tags related to amenities.\nYou can also visit the OSM wiki to find out more about the different types of features and tags available - https://wiki.openstreetmap.org/wiki/Map_features or indeed, the main OSM page - https://www.openstreetmap.org/#map=13/51.49656/-0.05897\nThe osmdata package uses the Overpass Turbo API, which an also be used directly to download data here - https://overpass-turbo.eu/\n\n# Load the required libraries\n# Load the required libraries\nlibrary(DT)\n\nfeatures &lt;- available_features() # Search for features in London\ntags &lt;- available_tags(\"amenity\") # Search for tags related to amenities\n\n\ntags_df &lt;- data.frame(\n  Tag = tags, # Add the tags directly to the 'Tag' column\n  Description = NA # Placeholder column for descriptions (optional)\n)\n\n# Display the data frame as a DT table\ndatatable(\n  tags_df,# Select only the first 5 rows\n  options = list(pageLength = 10), # Display 5 rows\n  rownames = FALSE\n)\n\n\n\n\n\nUsing the feature tags in OSM, we can download data on fast food outlets directly.\n\n# Define the geographic area (example: Greater London)\nBounding_box &lt;- getbb(\"Greater London\")  # Bounding box for London\n\n# Query OSM for fast food locations\nfast_food &lt;- opq(Bounding_box) %&gt;%\n  add_osm_feature(key = \"amenity\", value = \"fast_food\") %&gt;%\n  osmdata_sf()\n\n# Extract point locations (fast food outlets)\nfast_food_points &lt;- fast_food$osm_points %&gt;% \n  st_transform(4326) %&gt;% \n  mutate(opps = 1)\n\n# View first few rows\n#head(fast_food_points)\n\n#Filter out the points outside of our study area\nfast_food_points &lt;- fast_food_points[london_boroughs,]\n\n# Plot fast food outlets\n#plot(st_geometry(fast_food_points), col = \"red\", pch = 20, main = \"Fast Food Outlets in London\")\n\ntmap_mode(\"plot\")\nmap &lt;- tm_shape(fast_food_points) +\n  tm_dots(col = \"black\",\n          size = 0.05,   \n          border.lwd = 0, \n          title = \"Fast-food outlets\",  # Label for the dots\n          legend.show = TRUE)\n\nmap",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London</span>"
    ]
  },
  {
    "objectID": "03_obesity.html#load-school-points-from-open-street-map-osm",
    "href": "03_obesity.html#load-school-points-from-open-street-map-osm",
    "title": "3  Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London",
    "section": "5.4 Load school points from Open Street Map (OSM)",
    "text": "5.4 Load school points from Open Street Map (OSM)\nSchool data are also available on OSM, however the tags are not that useful for distinguishing between different types of schools. The amenity tag can be used to search for schools, but this will return all types of schools (e.g., primary, secondary, etc.).\nPrimary schools can be filtered in London with the isced:level tag, but this is not always available. We will show you how to get the schools from OSM, but below we we also use an alternative method for accessing point data.\n\n5.4.0.1 Bonus Extra Task - not required\nSee if you can think of a way of filtering out secondary schools from the tags available in OSM - you may need to use the filter() function in R to do this.\n\n# Define the geographic area (example: Greater London)\nBounding_box &lt;- getbb(\"Greater London\")  # Bounding box for London\n\n# Query OSM for school locations\nschools &lt;- opq(Bounding_box) %&gt;%\n  add_osm_feature(key = c(\"amenity\"), value = \"school\") %&gt;%\n  osmdata_sf()\n\n# Extract point locations (schools)\nschools_points &lt;- schools$osm_points %&gt;% \n  st_transform(4326) %&gt;% \n  mutate(opps = 1)\n\n# View first few rows\n#head(schools_points)\n\n#Filter out the schools outside of our study area\nschools_points &lt;- schools_points[london_boroughs,]\n\n# Plot schools\n#plot(st_geometry(schools_points), col = \"blue\", pch = 20, main = \"Schools in London\")\n\ntmap_mode(\"plot\")\nmap &lt;- tm_shape(schools_points) +\n  tm_dots(col = \"black\",\n          size = 0.05,   \n          border.lwd = 0, \n          title = \"All Schools London\",  # Label for the dots\n          legend.show = TRUE)\n\nmap\n\n\n\n\n\n\n\n\n\n\n5.4.1 Alternative Method for Accessing School Points\nAlternatively, the Department for Education’s Edubase database contains information for all schools - https://get-information-schools.service.gov.uk/ - here we read that data in and filter for secondary schools in London.\nThe crucial thing to note here is that the data is in British National Grid format, so we may need to transform it to WGS84 (EPSG 4326) to match the other data we have. However, we know it is in British National Grid as the data contains two columns - “easting” and “northing” - which is the standard way of referencing x and y coordinates in our projected coordinate reference system.\nIn the code below, we use the st_as_sf() function to convert the data to an sf object, and then the st_set_crs() function to set the coordinate reference system to British National Grid (EPSG 27700).\nWe are downloading and processing the data in one block, however, if you are not familiar with the column headers and the data contained in a file, always download and open it in excel first to get a sense of the contents.\n\nlondon_sec_schools &lt;- read_csv(\"https://www.dropbox.com/scl/fi/fhzafgt27v30lmmuo084y/edubasealldata20241003.csv?rlkey=uorw43s44hnw5k9js3z0ksuuq&raw=1\") %&gt;% \n  clean_names() %&gt;% \n  filter(gor_name == \"London\") %&gt;% \n  filter(phase_of_education_name == \"Secondary\") %&gt;% \n  filter(establishment_status_name == \"Open\") %&gt;%\n  st_as_sf(., coords = c(\"easting\", \"northing\")) %&gt;% \n  st_set_crs(27700)\n\nschools_points &lt;- london_sec_schools\n\ntmap_mode(\"view\")\nmap &lt;- tm_shape(schools_points) +\n  tm_dots(col = \"black\",\n          size = 0.05,   \n          border.lwd = 0, \n          title = \"All Schools London\",  # Label for the dots\n          legend.show = TRUE)\n\nmap",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London</span>"
    ]
  },
  {
    "objectID": "03_obesity.html#creating-point-pattern-process-ppp-objects",
    "href": "03_obesity.html#creating-point-pattern-process-ppp-objects",
    "title": "3  Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London",
    "section": "5.5 Creating Point Pattern Process (PPP) Objects",
    "text": "5.5 Creating Point Pattern Process (PPP) Objects\nIn order to carry out spatial point pattern analysis, we need to convert our spatial data into point pattern objects. We will use the spatstat package to create point pattern objects from our fast-food and school locations. We also need to make sure that all of our coordinates are in British National Grid (EPSG 27700) - a Projected Coordinate Reference System (CRS) for the UK as spatstat requires will not work on geographic coorinate systems like wgs84 (CRS: 4326).\nCreate some Point Pattern Process objects:\n\n# Project schools and fast food outlets to British National Grid CRS\nlnd_sch_bng &lt;- st_transform(schools_points, 27700)\nlnd_ff_bng &lt;- st_transform(fast_food_points, 27700)\nlnd_bng &lt;- st_transform(london_boroughs, 27700)\n\n# Summarize the transformed school points\n#summary(lnd_sch_bng)\n\n# Convert schools and fast food locations to PPP objects for spatial analysis\nlnd_sch_ppp &lt;- as.ppp(lnd_sch_bng)\nlnd_ff_ppp &lt;- as.ppp(lnd_ff_bng)\n\n# Remove marks (attributes) from the PPP objects\nmarks(lnd_sch_ppp) &lt;- NULL\nmarks(lnd_ff_ppp) &lt;- NULL\n\n# Convert London borough boundaries to observation window format\nlnd_owin &lt;- as.owin(lnd_bng)\n\n# Assign the observation window to the fast food PPP object\nWindow(lnd_ff_ppp) &lt;- lnd_owin\n\n# Plot the fast food point pattern objects\nplot(lnd_ff_ppp)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London</span>"
    ]
  },
  {
    "objectID": "03_obesity.html#imd-index-of-multiple-deprivation-analysis",
    "href": "03_obesity.html#imd-index-of-multiple-deprivation-analysis",
    "title": "3  Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London",
    "section": "5.6 IMD (Index of Multiple Deprivation) Analysis",
    "text": "5.6 IMD (Index of Multiple Deprivation) Analysis\nThe aim of this analysis is to investigate whether areas with higher levels of deprivation have a greater density of fast-food outlets. This will help us determine if there are inequalities in fast-food outlet distribution based on area level deprivation - and if so, in which areas public health interventions should be prioritised.\n\n5.6.1 Creating a Raster from IMD Data\n\n# Convert the IMD shapefile into a raster format for analysis\nIMD_London_raster &lt;- st_rasterize(IMD_London %&gt;% dplyr::select(imd_decile, geometry))\n# Plot the generated raster\nplot(IMD_London_raster)\n\n\n\n\n\n\n\n# Convert the raster to an IM object for statistical analysis\nIMD_London_im &lt;- as.im(IMD_London_raster)\n# Plot the IM object\nplot(IMD_London_im)\n\n\n\n\n\n\n\n\n\n\n5.6.2 Quadrat Analysis on Fast Food Locations\nWe can carry out a quadrat count analysis to assess the spatial distribution of fast-food outlets in relation to area level deprivation. As you saw in previous practicals on this course, usually a quadrat analysis divides the study area into a grid of equal-sized squares (quadrats) and counts the number of points in each quadrat. Here we use IMD deciles as our ‘quadrat’ areas in exactly the same way. The quadrat count analysis can help us determine if fast-food outlets are clustered, dispersed, or randomly distributed in areas with different levels of deprivation.\n\n# Set up the graphical parameters for a better layout\npar(mar = c(4, 4, 2, 2))  # Adjust margins for visualization\n\ndev.new()  # Open a new plotting window to visualize the quadrat analysis\n\n# Perform quadrat count analysis to assess spatial distribution\nquad_ff_IMD &lt;- quadratcount(lnd_ff_ppp, tess = IMD_London_im)\n\n# Display quadrat count results\nquad_ff_IMD\n\ntile\n   1    2    3    4    5    6    7    8    9   10 \n 287 2517 2842 2338 1714 1400 1481  952  994  281 \n\n#test if significant clustering\nchisq.test(quad_ff_IMD)\n\n\n    Chi-squared test for given probabilities\n\ndata:  quad_ff_IMD\nX-squared = 4797.8, df = 9, p-value &lt; 2.2e-16\n\n\n\n# Plot the quadrat count analysis\nplot(quad_ff_IMD)\n\n\n\n\n\n\n\n\n\n5.6.2.1 Question - What does the quadrat analysis tell you about the spatial distribution of fast-food outlets in relation to area level deprivation?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London</span>"
    ]
  },
  {
    "objectID": "03_obesity.html#density-estimation-with-rho-hat-plot",
    "href": "03_obesity.html#density-estimation-with-rho-hat-plot",
    "title": "3  Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London",
    "section": "5.7 Density Estimation with Rho-Hat Plot",
    "text": "5.7 Density Estimation with Rho-Hat Plot\nAs we saw in the lecture, we can take our analysis a little further by estimating the intensity of fast-food outlets across different levels of deprivation. The rho-hat function in spatstat estimates the intensity of point patterns as a continuous function of a covariate (rather than a discrete class).\n\n# Generate and plot the rho-hat function to assess intensity variation by area level deprivation\nplot(rhohat(lnd_ff_ppp, IMD_London_im))\n\n\n\n\n\n\n\n\n\n5.7.0.1 Question - What does the rho-hat plot tell you about the intensity of fast-food outlets across different levels of deprivation?\n\n\n5.7.0.2 Question - why do you think this plot has a series of peaks and troughs?\nIn the above plot we used IMD deciles to explore the intensity of fast-food outlets across different levels of deprivation. The IMD, however, has a raw score associated with it as well that we can examine\n\n# Convert the IMD shapefile into a raster format for analysis\nimd_ldn_score_raster &lt;- st_rasterize(IMD_London %&gt;% dplyr::select(imd_score, geometry))\n\n# Plot the generated raster\n#plot(imd_ldn_score_raster)\n\n# Convert the raster to an IM object for statistical analysis\nIMD_ldn_score_im &lt;- as.im(imd_ldn_score_raster)\n\n# Plot the IM object\nplot(IMD_ldn_score_im)\n\n\n\n\n\n\n\n\n\nplot(rhohat(lnd_ff_ppp, IMD_ldn_score_im))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London</span>"
    ]
  },
  {
    "objectID": "03_obesity.html#setup-r5-environment",
    "href": "03_obesity.html#setup-r5-environment",
    "title": "3  Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London",
    "section": "8.1 Setup R5 Environment",
    "text": "8.1 Setup R5 Environment\n\n# give yourself a bit more memory\noptions(java.parameters = \"-Xmx20G\")\n# Initialize Java Virtual Machine\nrJava::.jinit()\nrJava::.jcall(\"java.lang.System\", \"S\", \"getProperty\", \"java.version\")\n  # Set memory allocation\n\n# Load OpenStreetMap data for London\noe_match(\"London, England\")\nroads_london = oe_get(\"London, England\", stringsAsFactors = FALSE, quiet = TRUE)\nnames(roads_london)\n#summary(roads_london)\n\n# Filter major road types\nht = c(\"motorway\", \"trunk\", \"primary\", \"secondary\", \"tertiary\", \"residential\", \"unclassified\")\nosm_London_maj_roads = roads_london[roads_london$highway %in% ht, ]\n#plot(osm_London_maj_roads[\"highway\"], key.pos = 1)\n#plot(sf::st_geometry(roads_london))\n\n\nosm_download &lt;- list.files(oe_download_directory())\n\nosm_file &lt;- paste0(oe_download_directory(),\"\\\\\",\"geofabrik_greater-london-latest.osm.pbf\")\n\nfile.copy(from=osm_file, to=here(), \n          overwrite = TRUE, recursive = FALSE, \n          copy.mode = TRUE)\n\n# Setup R5 transport network\nhere()\nr5r_core &lt;- setup_r5(here())",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London</span>"
    ]
  },
  {
    "objectID": "03_obesity.html#reproject-data-to-a-consistent-crs-epsg-4326",
    "href": "03_obesity.html#reproject-data-to-a-consistent-crs-epsg-4326",
    "title": "3  Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London",
    "section": "8.2 Reproject Data to a Consistent CRS (EPSG 4326)",
    "text": "8.2 Reproject Data to a Consistent CRS (EPSG 4326)\n\n# Convert spatial data to EPSG 4326\nschool_points_EPSG &lt;- schools_points %&gt;% \n  st_transform(4326) %&gt;% \n  mutate(opps = 1) %&gt;% \n  mutate(id = row_number())\n\nfastfood_points_EPSG &lt;- fast_food_points %&gt;% \n  st_transform(4326) %&gt;% \n  mutate(opps = 1)\n\nlondon_boroughs_EPSG &lt;- london_boroughs %&gt;% \n  st_transform(4326)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London</span>"
    ]
  },
  {
    "objectID": "03_obesity.html#filter-data-for-the-borough-of-croydon-only",
    "href": "03_obesity.html#filter-data-for-the-borough-of-croydon-only",
    "title": "3  Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London",
    "section": "8.3 Filter Data for the Borough of Croydon Only",
    "text": "8.3 Filter Data for the Borough of Croydon Only\n\n# Extract Croydon boundary\ncroydon &lt;- london_boroughs_EPSG %&gt;% \n  filter(GSS_CODE == \"E09000008\")\n\n# Extract Croydon-specific school and fast food locations\ncroydon_sch_pts &lt;- school_points_EPSG[croydon,]\ncroydon_ff_pts &lt;- fastfood_points_EPSG[croydon,]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London</span>"
    ]
  },
  {
    "objectID": "03_obesity.html#generate-hexagonal-grid-for-croydon",
    "href": "03_obesity.html#generate-hexagonal-grid-for-croydon",
    "title": "3  Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London",
    "section": "8.4 Generate Hexagonal Grid for Croydon",
    "text": "8.4 Generate Hexagonal Grid for Croydon\n\ncroydon_h3 &lt;- polygon_to_cells(croydon, res = 11, simple = FALSE)\ncroydon_h3 &lt;- cell_to_polygon(unlist(croydon_h3$h3_addresses), simple = FALSE)\ncroydon_h3_centroids &lt;- st_centroid(croydon_h3)\n\n# Summarize and visualize the hexagonal grid\n#summary(croydon_h3)\n\n# ggplot() +\n#   geom_sf(data = croydon, fill = NA) +\n#   geom_sf(data = croydon_h3, fill = NA, colour = 'red') +\n#   ggtitle('Resolution 10 hexagons', subtitle = 'Croydon') +\n#   theme_minimal() +\n#   coord_sf()\n\ntm_shape(croydon) +\n  tm_polygons(col = \"red\", alpha = 0) +\ntm_shape(croydon_h3) +\n  tm_polygons(col = NA, alpha = 0) +\ntm_shape(croydon_ff_pts) +\n  tm_dots(col = \"blue\", border.lwd = 0) + \ntm_shape(croydon_sch_pts) +\n  tm_dots(col = \"black\", border.lwd = 0)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London</span>"
    ]
  },
  {
    "objectID": "03_obesity.html#travel-time-matrix-calculation",
    "href": "03_obesity.html#travel-time-matrix-calculation",
    "title": "3  Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London",
    "section": "8.5 Travel Time Matrix Calculation",
    "text": "8.5 Travel Time Matrix Calculation\n\n# Prepare data for travel time calculations\ncroydon_sch = croydon_sch_pts %&gt;% st_drop_geometry()\ncroydon_h3_centroids[c(\"x\", \"y\")] = st_coordinates(croydon_h3_centroids)\n\n# Assign IDs\ncroydon_h3_centroids &lt;- mutate(croydon_h3_centroids, id = row_number())\ncroydon_h3 &lt;- mutate(croydon_h3, id = row_number())\n\n# Define travel parameters\nmode = c(\"WALK\")\nmax_walk_time = 30  # Maximum walking time in minutes\nmax_trip_duration = 120  # Maximum trip duration in minutes\ndeparture_datetime = as.POSIXct(\"01-12-2022 8:30:00\", format = \"%d-%m-%Y %H:%M:%S\")\n\n# Compute travel time matrix\nttm_croydon_h3 = travel_time_matrix(\n  r5r_core = r5r_core,\n  origins = croydon_sch_pts,\n  destinations = croydon_h3_centroids,\n  mode = mode,\n  departure_datetime = departure_datetime,\n  max_walk_time = max_walk_time,\n  max_trip_duration = max_trip_duration\n)\n\n#head(ttm_croydon_h3)\n#nrow(ttm_croydon_h3)\n#summary(ttm_croydon_h3)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London</span>"
    ]
  },
  {
    "objectID": "03_obesity.html#identify-closest-schools-by-travel-time",
    "href": "03_obesity.html#identify-closest-schools-by-travel-time",
    "title": "3  Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London",
    "section": "8.6 Identify Closest Schools by Travel Time",
    "text": "8.6 Identify Closest Schools by Travel Time\n\n# Find minimum travel time per destination\nclosest = aggregate(ttm_croydon_h3$travel_time_p50, by = list(ttm_croydon_h3$to_id), FUN = min, na.rm = TRUE)\nsummary(closest)\n\n# Rename columns\nclosest &lt;- rename(closest, id = Group.1, time = x)\nclosest[\"id\"] = as.integer(closest$id)\nhead(closest)\n\n# Join spatial data with travel time results\ngeo = inner_join(croydon_h3_centroids, closest, by = \"id\")\ngeo_hex = inner_join(croydon_h3, closest, by = \"id\")\nhead(geo)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London</span>"
    ]
  },
  {
    "objectID": "03_obesity.html#visualizing-travel-time-data",
    "href": "03_obesity.html#visualizing-travel-time-data",
    "title": "3  Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London",
    "section": "8.7 Visualizing Travel Time Data",
    "text": "8.7 Visualizing Travel Time Data\n\ntm_shape(geo) +  \n  tm_symbols(col = \"time\", size = 0.05, border.lwd = 0, style = \"pretty\", n = 10, palette = \"RdYlBu\", alpha = 0.3) +  \ntm_shape(croydon_sch_pts) +\n  tm_dots(col = \"black\", border.lwd = 0) +  \ntm_shape(croydon) +\n  tm_polygons(alpha = 0)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London</span>"
    ]
  },
  {
    "objectID": "03_obesity.html#convert-data-to-british-national-grid-epsg-27700-and-analyse-spatial-distribution",
    "href": "03_obesity.html#convert-data-to-british-national-grid-epsg-27700-and-analyse-spatial-distribution",
    "title": "3  Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London",
    "section": "8.8 Convert Data to British National Grid (EPSG 27700) and Analyse Spatial Distribution",
    "text": "8.8 Convert Data to British National Grid (EPSG 27700) and Analyse Spatial Distribution\n\n# Reproject spatial data\ngeo_hex_bng &lt;- st_transform(geo_hex, 27700)\n\n# Convert raster data\nraster_geo &lt;- st_rasterize(geo_hex_bng %&gt;% dplyr::select(time, geometry)) \nst_coordinates(raster_geo)\nplot(raster_geo, axes = T)\ngeo_hex_im &lt;- as.im(raster_geo)\nplot(geo_hex_im)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London</span>"
    ]
  },
  {
    "objectID": "03_obesity.html#quadrat-analysis-of-fast-food-locations",
    "href": "03_obesity.html#quadrat-analysis-of-fast-food-locations",
    "title": "3  Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London",
    "section": "8.9 Quadrat Analysis of Fast Food Locations",
    "text": "8.9 Quadrat Analysis of Fast Food Locations\n\n# Convert to spatial point pattern objects\ncroy_sch_bng &lt;- st_transform(croydon_sch_pts, 27700)\ncroy_ff_bng &lt;- st_transform(croydon_ff_pts, 27700) %&gt;% \n  select(c(\"osm_id\", \"name\", \"geometry\"))\ncroydon_bng &lt;- st_transform(croydon, 27700)\ncroy_ff_ppp &lt;- as.ppp(croy_ff_bng)\ncroydon_owin &lt;- as.owin(croydon_bng)\nWindow(croy_ff_ppp) &lt;- croydon_owin\n\n# Quadrat count analysis\nQ &lt;- quadratcount(croy_ff_ppp, nx= 6, ny=6)\nplot(croy_ff_ppp, pch=20, cols=\"grey70\", main=NULL); plot(Q, add=T)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London</span>"
    ]
  },
  {
    "objectID": "03_obesity.html#spatial-density-analysis",
    "href": "03_obesity.html#spatial-density-analysis",
    "title": "3  Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London",
    "section": "8.10 Spatial Density Analysis",
    "text": "8.10 Spatial Density Analysis\n\nplot(rhohat(croy_ff_ppp, geo_hex_im))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London</span>"
    ]
  },
  {
    "objectID": "03_obesity.html#kolmogorov-smirnov-test-for-spatial-distribution",
    "href": "03_obesity.html#kolmogorov-smirnov-test-for-spatial-distribution",
    "title": "3  Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London",
    "section": "8.11 Kolmogorov-Smirnov Test for Spatial Distribution",
    "text": "8.11 Kolmogorov-Smirnov Test for Spatial Distribution\n\nks_result &lt;- cdf.test(croy_ff_ppp, \"x\")\nprint(ks_result)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Exploring the Spatial Distribution of Points in the Presence of Covariates - Fast-Food Outlets and Schools in London</span>"
    ]
  },
  {
    "objectID": "04_spatial_models.html",
    "href": "04_spatial_models.html",
    "title": "4  Spatial models",
    "section": "",
    "text": "4.1 Introduction\nIn this practical you will be introduced to a suite of different models that will allow you to test a variety of research questions and hypotheses through modelling the associations between two or more spatially referenced variables.\nIn 2023 New Zealand scrapped an anti-smoking law that would mean a generational smoking bad for anyone born after 2008. Instead they limited the number of retailers in 2024 from 6,000 to 600 across the country. With a maximum number per area according to the table set on Ministry of Health website. Here we will explore the factors that might affect smoking in New Zealand and see if they vary spatially. Obviously this new legislation may (most likely will) influence behaviors from 2024.\nThis practical will walk you through the common steps that you should go through when building a regression model using spatial data to test a stated research hypothesis; from carrying out some descriptive visualisation and summary statistics, to interpreting the results and using the outputs of the model to inform your next steps.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial models</span>"
    ]
  },
  {
    "objectID": "04_spatial_models.html#data",
    "href": "04_spatial_models.html#data",
    "title": "4  Spatial models",
    "section": "4.2 Data",
    "text": "4.2 Data\nThis is the same has homework 1….and note, we could use 2023 data as described in the homework 1 task.\n\nGo to the New Zealand spatial data portal and download the file Territorial Authority 2018 (generalised), these are city or district councils. Make sure it’s the Territorial Authority 2018 data not SA1.\nGo to the Stats NZ website and download the Statistical area 1 dataset for 2018 for the whole of New Zealand. Download the csv this time - the one that does not say long \n\nThe figure below explains the geographies of New Zealand, we could replicate this analysis for SA1 or SA2 data.\n\n\n\n\n\n\n\nNZ geographies\n\n\n\n4.2.1 Loading\n\nlibrary(sf)\nlibrary(tidyverse)\n\nta &lt;- st_read(\"prac4_data/statsnzterritorial-authority-2018-generalised-SHP/territorial-authority-2018-generalised.shp\")\n\nReading layer `territorial-authority-2018-generalised' from data source \n  `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\Guest\\Oxford_26\\spatial analysis of public health\\spatial-analysis-of-public-health-data\\prac4_data\\statsnzterritorial-authority-2018-generalised-SHP\\territorial-authority-2018-generalised.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 68 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1067061 ymin: 4701317 xmax: 2523320 ymax: 6242140\nProjected CRS: NZGD2000 / New Zealand Transverse Mercator 2000\n\ncensus &lt;- read_csv(\"prac4_data/SA1_census/Individual_part2_totalNZ-wide_format_updated_16-7-20.csv\")\n\n\n\n4.2.2 Wrangle\nThe .csv we have loaded contains all the data from the week 1 excel homework in a wide format - that means all the SA1s, SA2s etc are are a new row making a huge data file. Normally i would suggest a cool data sci-ency way to pull out our Territorial Authorities, however i can’t see anything that would work. For example, we could try and detect part of a string like “City” or “District” but they appear in other geographies.\nSo, until i can think or a better way we are resigned to subsetting by rows!\n\nlibrary(janitor)\n\ncensus_authorities &lt;- census %&gt;%\n  slice(32413:32480) %&gt;%\n  clean_names()\n\n# or\n\n#census_authorities &lt;- census[32413:32480,]%&gt;%\n#  clean_names()\n\nNow in this case the data can be a little hard to read - we have over 350 columns! It might be sensible here to have a look at the excel document we downloaded in the week 1 homework to identify our our variables.\nLet’s pull out the following variables from the 2018 data:\n\nregular smoker\n\nthat will be modeled by:\n\nmedian income\nhome ownership\nno qualification, see the New Zealand gov careers website for what qualification levels mean\nwe’ll also try and include some spatial data (e.g. density of tobacco shops) at the very end…\n\nBecause we are using spatial units and they may differ in size we should normalise our count data (everything but median income which is continous).\nTo do so i will use the “total” column for each variable, although you could also use population count\n\ncensus_authorities_data &lt;- census_authorities %&gt;%\n  select(area_code,\n         area_description,\n         census_2018_smoking_status_01_regular_smoker_curp_15years_and_over,\n         census_2018_smoking_status_total_stated_curp_15years_and_over,\n         census_2018_total_personal_income_median_curp_15years_and_over,\n         census_2018_individual_home_ownership_02_own_or_partly_own_curp_15years_and_over,\n         census_2018_individual_home_ownership_total_stated_curp_15years_and_over,\n         census_2018_highest_qualification_000_no_qualification_curp_15years_and_over,\n         census_2018_highest_qualification_total_stated_curp_15years_and_over\n         )\n\nLet’s check the type of our data…\n\ndatatypelist &lt;- census_authorities_data %&gt;% \n  summarise_all(class) %&gt;%\n  pivot_longer(everything(), \n               names_to=\"All_variables\", \n               values_to=\"Variable_class\")\n\ndatatypelist\n\n# A tibble: 9 × 2\n  All_variables                                                   Variable_class\n  &lt;chr&gt;                                                           &lt;chr&gt;         \n1 area_code                                                       character     \n2 area_description                                                character     \n3 census_2018_smoking_status_01_regular_smoker_curp_15years_and_… character     \n4 census_2018_smoking_status_total_stated_curp_15years_and_over   character     \n5 census_2018_total_personal_income_median_curp_15years_and_over  character     \n6 census_2018_individual_home_ownership_02_own_or_partly_own_cur… character     \n7 census_2018_individual_home_ownership_total_stated_curp_15year… character     \n8 census_2018_highest_qualification_000_no_qualification_curp_15… character     \n9 census_2018_highest_qualification_total_stated_curp_15years_an… character     \n\n\nCharacter! we will need to change that…\n\ncensus_authorities_data &lt;- census_authorities_data %&gt;%\n  mutate_at(c(\"census_2018_smoking_status_01_regular_smoker_curp_15years_and_over\",\n        \"census_2018_smoking_status_total_stated_curp_15years_and_over\",\n         \"census_2018_total_personal_income_median_curp_15years_and_over\",\n         \"census_2018_individual_home_ownership_02_own_or_partly_own_curp_15years_and_over\",\n         \"census_2018_individual_home_ownership_total_stated_curp_15years_and_over\",\n         \"census_2018_highest_qualification_000_no_qualification_curp_15years_and_over\",\n         \"census_2018_highest_qualification_total_stated_curp_15years_and_over\"),\n        as.numeric)\n\nThe data is selected and now numeric… let’s normalise…and select what we need\n\ncensus_authorities_data_norm &lt;- census_authorities_data%&gt;%\n  mutate(percent_regular_smoker=\n           (census_2018_smoking_status_01_regular_smoker_curp_15years_and_over/census_2018_smoking_status_total_stated_curp_15years_and_over)*100)%&gt;%\n  mutate(percent_home_owner=\n           (census_2018_individual_home_ownership_02_own_or_partly_own_curp_15years_and_over/census_2018_individual_home_ownership_total_stated_curp_15years_and_over)*100)%&gt;%\n  mutate(percent_no_qualification=\n           (census_2018_highest_qualification_000_no_qualification_curp_15years_and_over/ census_2018_highest_qualification_total_stated_curp_15years_and_over)*100)%&gt;%\n  select(area_code,\n         area_description,\n         percent_regular_smoker,\n         percent_home_owner,\n         percent_no_qualification,\n         census_2018_total_personal_income_median_curp_15years_and_over\n         )\n\nQuick plot…\n\nlibrary(ggplot2)\n\nq &lt;- qplot(x = percent_no_qualification, \n           y = percent_regular_smoker, \n           data=census_authorities_data_norm)\n\nq + stat_smooth(method=\"lm\", se=FALSE, size=1) \n\n\n\n\n\n\n\n\nHere, I have plotted the percentage of regular smokers per territorial area against another variable in the dataset that I think might be influential…percentage with no qualification\nRemember that my null hypothesis would be that there is no relationship between percentage of regular smokers and percentage with no qualification. If this null hypothesis was true, then I would not expect to see any pattern in the cloud of points plotted above.\nAs it is, the scatter plot shows that, generally, as the \\(x\\) axis independent variable (percentage with no qualification) goes up, our \\(y\\) axis dependent variable (percent of smokers) goes up. This is not a random cloud of points, but something that indicates there could be a relationship here and so I might be looking to reject my null hypothesis.\nSome conventions - In a regression equation, the dependent variable is always labelled \\(y\\) and shown on the \\(y\\) axis of a graph, the predictor or independent variable(s) is(are) always shown on the \\(x\\) axis.\nI have added a blue line of best-fit - this is the line that can be drawn by minimising the sum of the squared differences between the line and the residuals. The residuals are all of the dots not falling exactly on the blue line. An algorithm known as ‘ordinary least squares’ (OLS) is used to draw this line and it simply tries a selection of different lines until the sum of the squared differences between all of the residuals and the blue line is minimised, leaving the final solution.\nAs a general rule, the better the blue line is at summarising the relationship between \\(y\\) and \\(x\\), the better the model.\nThe equation for the blue line in the graph above can be written:\n\\[y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\]\nwhere:\n\\(\\beta_0\\) is the intercept (the value of \\(y\\) when \\(x = 0\\) - somewhere around 4 on the graph above);\n\\(\\beta_1\\) is sometimes referred to as the ‘slope’ parameter and is simply the change in the value of \\(y\\) for a 1 unit change in the value of \\(x\\) (the slope of the blue line) - reading the graph above, the change in the value of \\(y\\) reading between 20 and 15 on the \\(x\\) axis looks to be around a change from 15 to 17.5 on the \\(y\\) - about 0.5 per 1 unit value of \\(x\\).\n\\(\\epsilon_i\\) is a random error term (positive or negative) that should sum to 0 - essentially, if you add all of the vertical differences between the blue line and all of the residuals, it should sum to 0.\nAny value of \\(y\\) along the blue line can be modeled using the corresponding value of \\(x\\) and these parameter values. Examining the graph above we would expect the percent smoking in an area where 26% of the population to not have any qualifications, to equal around 17%, but we can confirm this by plugging the \\(\\beta\\) parameter values and the value of \\(x\\) into equation (1):\n\n4 + (0.5*26) + 0\n\n[1] 17",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial models</span>"
    ]
  },
  {
    "objectID": "04_spatial_models.html#regression",
    "href": "04_spatial_models.html#regression",
    "title": "4  Spatial models",
    "section": "4.3 Regression",
    "text": "4.3 Regression\nIn the graph above, I used a method called ‘lm’ in the stat_smooth() function in ggplot2 to draw the regression line. ‘lm’ stands for ‘linear model’ and is a standard function in R for running linear regression models. Use the help system to find out more about lm - ?lm\nBelow is the code that could be used to draw the blue line in our scatter plot. Note, the tilde ~ symbol means “is modeled by”.\n\n#now model\nmodel1 &lt;- census_authorities_data_norm %&gt;%\n  lm(percent_regular_smoker ~\n       percent_no_qualification,\n     data=.)\n\nLet’s have a closer look at our model…\n\n#show the summary of those outputs\nsummary(model1)\n\n\nCall:\nlm(formula = percent_regular_smoker ~ percent_no_qualification, \n    data = .)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7961 -2.3344 -0.5826  1.1691 16.4704 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               1.20258    1.98667   0.605    0.547    \npercent_no_qualification  0.64634    0.08289   7.798  6.1e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.588 on 66 degrees of freedom\nMultiple R-squared:  0.4795,    Adjusted R-squared:  0.4716 \nF-statistic:  60.8 on 1 and 66 DF,  p-value: 6.097e-11\n\n\n\n4.3.1 Interpretation\nIn running a regression model, we are effectively trying to test (disprove) our null hypothesis. If our null hypothesis was true, then we would expect our coefficients to = 0.\nIn the output summary of the model above, there are a number of features you should pay attention to:\nCoefficient Estimates - these are the \\(\\beta_0\\) (intercept) and \\(\\beta_1\\) (slope) parameter estimates from Equation 1. You will notice that at \\(\\beta_0 = 1.2\\) and \\(\\beta_1 = 0.64\\) they are pretty close (not awful!) to the estimates of 4 and 0.5 that we read from the graph earlier, but more precise.\nCoefficient Standard Errors - these represent the average amount the coefficient varies from the average value of the dependent variable (its standard deviation). So, for a 1% increase in percent with no qualifications, while the model says we might expect percent of smokers to rise by 0.6%, this might vary, on average, by about 0.08%. As a rule of thumb, we are looking for a lower value in the standard error relative to the size of the coefficient.\nNote that is the coefficient represents a one unit change, here it is %, as the variable is % unauthorized absence in school So one unit is a 1% change…\nCoefficient t-value - this is the value of the coefficient divided by the standard error and so can be thought of as a kind of standardised coefficient value. The larger (either positive or negative) the value the greater the relative effect that particular independent variable is having on the dependent variable (this is perhaps more useful when we have several independent variables in the model) .\nCoefficient p-value - Pr(&gt;|t|) - the p-value is a measure of significance. There is lots of debate about p-values which I won’t go into here, but essentially it refers to the probability of getting a coefficient as large as the one observed in a set of random data. p-values can be thought of as percentages, so if we have a p-value of 0.5, then there is a 5% chance that our coefficient could have occurred in some random data, or put another way, a 95% chance that out coefficient could have only occurred in our data.\nAs a rule of thumb, the smaller the p-value, the more significant that variable is in the story and the smaller the chance that the relationship being observed is just random. Generally, statisticians use 5% or 0.05 as the acceptable cut-off for statistical significance - anything greater than that we should be a little sceptical about.\nIn r the codes ***, **, **, . are used to indicate significance. We generally want at least a single * next to our coefficient for it to be worth considering.\nR-Squared - This can be thought of as an indication of how good your model is - a measure of ‘goodness-of-fit’ (of which there are a number of others). \\(r^2\\) is quite an intuitive measure of fit as it ranges between 0 and 1 and can be thought of as the % of variation in the dependent variable (in our case percentage of smokers) explained by variation in the independent variable(s). In our example, an \\(r^2\\) value of 0.47 indicates that around 47% of the variation in percentage of smokers can be explained by variation in percentage with no qualifications. In other words, this is quite a good model. The \\(r^2\\) value will increase as more independent explanatory variables are added into the model, so where this might be an issue, the adjusted r-squared value can be used to account for this affect.\n\n\n4.3.2 Assumptions underpinning linear regression\n\n4.3.2.1 Assumption 1 - there is a linear relationship between the dependent and independent variables\nThe best way to test for this assumption is to plot a scatter plot similar to the one created earlier. It may not always be practical to create a series of scatter plots, so one quick way to check that a linear relationship is probable is to look at the frequency distributions of the variables. If they are normally distributed, then there is a good chance that if the two variables are in some way correlated, this will be a linear relationship.\nFor example, look at the frequency distributions of our two variables earlier:\n\n#let's check the distribution of these variables first\n\nggplot(census_authorities_data_norm,\n       aes(x=percent_regular_smoker)) + \n  geom_histogram(aes(y = after_stat(density)),\n                 binwidth = 1) + \n  geom_density(colour=\"red\", \n               size=1, \n               adjust=1)\n\n\n\n\n\n\n\n\nHere, adding after_stat(density) means that the histogram is a density plot, this plots the chance that any value in the data is equal to that value.\n\nggplot(census_authorities_data_norm,\n       aes(x=percent_home_owner)) +\n  geom_histogram(aes(y = after_stat(density)),\n                 binwidth = 1) + \n  geom_density(colour=\"red\",\n               size=1, \n               adjust=1)\n\n\n\n\n\n\n\n\nIt is not a requirement of regression to have normally distributed variables, however if they aren’t the residuals may well be normally distributed, but could vary (meaning we have hetroscedasticity) which we discuss later on.\nOne way that we might be able to achieve a linear relationship between our two variables is to transform the non-normally distributed variable so that it is more normally distributed. For more information on this see transforming variables.\nHowever you will be changing the relationship of your data - it won’t be linear anymore! This could improve your model but is at the expense of interpretation. Aside from log transformation which has the rules in the link above.\nTypically if you do a power transformation you can keep the direction of the relationship (positive or negative) and the t-value will give an idea of the importance of the variable in the model - that’s about it!\n\n\n4.3.2.2 Assumption 2 - the residuals in your model should be normally distributed\nThis assumption is easy to check. When we ran our model1 earlier, one of the outputs stored in our model1 object is the residual value for each case (authority) in your dataset. We can access these values using augment() from broom which will add model output to the original data…\nWe can plot these as a histogram and see if there is a normal distribution:\n\nlibrary(broom)\n\n#save the residuals into your dataframe\nmodel_data &lt;- model1 %&gt;%\n  augment(., census_authorities_data_norm)\n\n#plot residuals\nmodel_data%&gt;%\ndplyr::select(.resid)%&gt;%\n  pull()%&gt;%\n  qplot()+ \n  geom_histogram() \n\n\n\n\n\n\n\n\nExamining the histogram above, i am happy this is rather normal although there is evidently an outlier in our data somewhere.\n\n\n4.3.2.3 Assumption 3 - no multicolinearity in the independent variables\nNow, the regression model we have be experimenting with so far is a simple bivariate (two variable) model. One of the nice things about regression modelling is while we can only easily visualise linear relationships in a two (or maximum 3) dimension scatter plot, mathematically, we can have as many dimensions / variables as we like.\nAs such, we could extend model 1 into a multiple regression model by adding some more explanatory variables that we think could affect the percentage of people smoking…\n\nmodel2 &lt;- lm(percent_regular_smoker ~\n               percent_no_qualification + \n               percent_home_owner +\n               census_2018_total_personal_income_median_curp_15years_and_over,\n             data = census_authorities_data_norm)\n\nsummary(model2)\n\n\nCall:\nlm(formula = percent_regular_smoker ~ percent_no_qualification + \n    percent_home_owner + census_2018_total_personal_income_median_curp_15years_and_over, \n    data = census_authorities_data_norm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.9633 -1.6181 -0.4151  1.3204 12.1627 \n\nCoefficients:\n                                                                 Estimate\n(Intercept)                                                     1.762e+01\npercent_no_qualification                                        7.757e-01\npercent_home_owner                                             -3.992e-01\ncensus_2018_total_personal_income_median_curp_15years_and_over -3.922e-05\n                                                               Std. Error\n(Intercept)                                                     5.089e+00\npercent_no_qualification                                        9.198e-02\npercent_home_owner                                              7.015e-02\ncensus_2018_total_personal_income_median_curp_15years_and_over  9.961e-05\n                                                               t value Pr(&gt;|t|)\n(Intercept)                                                      3.463 0.000959\npercent_no_qualification                                         8.433 5.57e-12\npercent_home_owner                                              -5.690 3.40e-07\ncensus_2018_total_personal_income_median_curp_15years_and_over  -0.394 0.695119\n                                                                  \n(Intercept)                                                    ***\npercent_no_qualification                                       ***\npercent_home_owner                                             ***\ncensus_2018_total_personal_income_median_curp_15years_and_over    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.96 on 64 degrees of freedom\nMultiple R-squared:  0.6564,    Adjusted R-squared:  0.6403 \nF-statistic: 40.75 on 3 and 64 DF,  p-value: 7.514e-15\n\n\nExamining the output above, it is clear that including these variables into our model improves the fit from an \\(r^2\\) of around 47% to an \\(r^2\\) of 65%. However, income is not significant so we should consider removing it..\n\nmodel2 &lt;- lm(percent_regular_smoker ~\n               percent_no_qualification + \n               percent_home_owner ,\n             data = census_authorities_data_norm)\n\nsummary(model2)\n\n\nCall:\nlm(formula = percent_regular_smoker ~ percent_no_qualification + \n    percent_home_owner, data = census_authorities_data_norm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.6020 -1.6175 -0.3927  1.2883 11.7795 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              16.02096    3.04321   5.264 1.70e-06 ***\npercent_no_qualification  0.79755    0.07283  10.950  &lt; 2e-16 ***\npercent_home_owner       -0.40092    0.06955  -5.764 2.45e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.941 on 65 degrees of freedom\nMultiple R-squared:  0.6556,    Adjusted R-squared:  0.645 \nF-statistic: 61.86 on 2 and 65 DF,  p-value: 9.037e-16\n\n\nHere we see the \\(r^2\\) is almost the same. But do our two explanatory variables satisfy the no-multicoliniarity assumption? If not and the variables are highly correlated, then we are effectively double counting the influence of these variables and overstating their explanatory power\n\n\n\n\n\nWord anatomy of multicollinearity. Source: What the Heck is Multicollinearity?, Andrew Ozbun\n\n\n\n\nTo check this, we can compute the product moment correlation coefficient between the variables, using the corrr() package, that’s part of tidymodels. In an ideal world, we would be looking for something less than a 0.8 correlation\n\nlibrary(corrr)\n\ncorrelation &lt;- census_authorities_data_norm %&gt;%\n  dplyr::select(percent_no_qualification, \n               percent_home_owner)%&gt;%\n    correlate()\n\n#visualise the correlation matrix\nrplot(correlation)\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\nℹ The deprecated feature was likely used in the corrr package.\n  Please report the issue at &lt;https://github.com/tidymodels/corrr/issues&gt;.\n\n\n\n\n\n\n\n\ncorrelation\n\n# A tibble: 2 × 3\n  term                     percent_no_qualification percent_home_owner\n  &lt;chr&gt;                                       &lt;dbl&gt;              &lt;dbl&gt;\n1 percent_no_qualification                   NA                  0.360\n2 percent_home_owner                          0.360             NA    \n\n\nAnother way that we can check for Multicolinearity is to examine the VIF for the model. If we have VIF values for any variable exceeding 10, then we may need to worry and perhaps remove that variable from the analysis:\n\nlibrary(car)\nvif(model2)\n\npercent_no_qualification       percent_home_owner \n                1.149067                 1.149067 \n\n\n\n\n4.3.2.4 Assumption 4 - homoscedasticity\nHomoscedasticity means that the errors/residuals in the model exhibit constant / homogeneous variance, if they don’t, then we would say that there is hetroscedasticity present. Why does this matter? Andy Field does a much better job of explaining this in discovering statistics — but essentially, if your errors do not have constant variance, then your parameter estimates could be wrong, as could the estimates of their significance.\nThe best way to check for homo/hetroscedasticity is to plot the residuals in the model against the predicted values. We are looking for a cloud of points with no apparent patterning to them.\n\n#print some model diagnositcs. \npar(mfrow=c(2,2))    #plot to 2 by 2 array\nplot(model2)\n\n\n\n\n\n\n\n\nIn the series of plots above, the first plot (residuals vs fitted), we would hope to find a random cloud of points with a straight horizontal red line. Looking at the plot, the curved red line would suggest some hetroscedasticity, but the cloud looks quite random. Similarly we are looking for a random cloud of points with no apparent patterning or shape in the third plot of standardised residuals vs fitted values. Here, the cloud of points also looks fairly random, with perhaps some shaping indicated by the red line.\nSection 5.7.6 in Tidyverse Skills for Data Science explains each of these plots in more detail\nIn the plots here we are looking for:\n\nResiduals vs Fitted: a flat and horizontal line. This is looking at the linear relationship assumption between our variables\nNormal Q-Q: all points falling on the line. This checks if the residuals (observed minus predicted) are normally distributed\nScale vs Location: flat and horizontal line, with randomly spaced points. This is the homoscedasticity (errors/residuals in the model exhibit constant / homogeneous variance). Are the residuals (also called errors) spread equally along all of the data.\nResiduals vs Leverage - Identifies outliers (or influential observations), the three largest outliers are identified with values in the plot.\n\nThe University of Viginia Library provides examples of good and bad models in relation to these plots.\nThere is an easier way to produce this plot using check_model() from the performance package, that even includes what you are looking for…note that the Posterior predictive check is the comparison between the fitted model and the observed data.\nThe default argument is check=all but we can specify what to check for…see the arguments section in the documentation…e.g. check = c(\"vif\", \"qq\")\n\nlibrary(performance)\n\ncheck_model(model2, check=\"all\")\n\n\n\n\n\n\n\n\n\n\n4.3.2.5 Assumption 5 - independence of errors\nThis assumption simply states that the residual values (errors) in your model must not be correlated in any way. If they are, then they exhibit autocorrelation which suggests that something might be going on in the background that we have not sufficiently accounted for in our model.\n\n4.3.2.5.1 Standard autocorrelation\nIf you are running a regression model on data that do not have explicit space or time dimensions, then the standard test for autocorrelation would be the Durbin-Watson test.\nThis tests whether residuals are correlated and produces a summary statistic that ranges between 0 and 4, with 2 signifying no autocorrelation. A value greater than 2 suggesting negative autocorrelation and and value of less than 2 indicating positive autocorrelation.\nIn his excellent text book, Andy Field suggests that you should be concerned with Durbin-Watson test statistics &lt;1 or &gt;3. So let’s see:\n\n#run durbin-watson test\ndw &lt;- durbinWatsonTest(model2)\ntidy(dw)\n\n# A tibble: 1 × 5\n  statistic p.value autocorrelation method             alternative\n      &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;      \n1      1.41  0.0140           0.227 Durbin-Watson Test two.sided  \n\n\nAs you can see, the DW statistics for our model is 1.41, so some indication of autocorrelation, but perhaps nothing to worry about.\n\n\n4.3.2.5.2 Spatial autocorrelation\nHOWEVER\nWe are using spatially referenced data and as such we should check for spatial-autocorrelation.\nThe first test we should carry out is to map the residuals to see if there are any apparent obvious patterns:\n\n#and for future use, write the residuals out\nmodel2_residuals &lt;- model2 %&gt;%\n  augment(., census_authorities_data_norm)\n\n\nta_model_data&lt;- ta%&gt;%\n  clean_names(.)%&gt;%\n  left_join(., model2_residuals,\n            by=c(\"ta2018_v1\"=\"area_code\"))\n\n\nlibrary(tmap)\n\n#now plot the residuals\ntmap_mode(\"plot\")\n\nℹ tmap modes \"plot\" - \"view\"\nℹ toggle with `tmap::ttm()`\n\ntm_shape(ta_model_data) +\n  tm_polygons(\".resid\",\n              palette = \"RdYlBu\")\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n[v3-&gt;v4] `tm_tm_polygons()`: migrate the argument(s) related to the scale of\nthe visual variable `fill` namely 'palette' (rename to 'values') to fill.scale\n= tm_scale(&lt;HERE&gt;).[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"RdYlBu\" is named\n\"brewer.rd_yl_bu\"Multiple palettes called \"rd_yl_bu\" found: \"brewer.rd_yl_bu\", \"matplotlib.rd_yl_bu\". The first one, \"brewer.rd_yl_bu\", is returned.\n[scale] tm_polygons:() the data variable assigned to 'fill' contains positive and negative values, so midpoint is set to 0. Set 'midpoint = NA' in 'fill.scale = tm_scale_intervals(&lt;HERE&gt;)' to use all visual values (e.g. colors)\n\n\n\n\n\n\n\n\n\nNow, we can immediately see some issues with this…\n\nI have left the data “outside territorial authority” in the data and it has a large negative residual\nChatham Islands has been included and is also a large positive residual.\n\nFor the purposes of this practical let’s remove them and re-run the model…\nNote i have added in median income and it is now significant!\n\ncensus_authorities_data_norm_filter &lt;- census_authorities_data_norm %&gt;%\n  # != means not equal to\n  filter(area_code != \"999\" & area_code != \"067\")\n\nmodel3 &lt;- lm(percent_regular_smoker ~\n               percent_no_qualification + \n               percent_home_owner +\n               census_2018_total_personal_income_median_curp_15years_and_over,\n             data = census_authorities_data_norm_filter)\n\nsummary(model3)\n\n\nCall:\nlm(formula = percent_regular_smoker ~ percent_no_qualification + \n    percent_home_owner + census_2018_total_personal_income_median_curp_15years_and_over, \n    data = census_authorities_data_norm_filter)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6984 -1.5153 -0.4655  1.2802  5.3085 \n\nCoefficients:\n                                                                 Estimate\n(Intercept)                                                     2.796e+01\npercent_no_qualification                                        5.152e-01\npercent_home_owner                                             -3.051e-01\ncensus_2018_total_personal_income_median_curp_15years_and_over -3.295e-04\n                                                               Std. Error\n(Intercept)                                                     3.939e+00\npercent_no_qualification                                        7.357e-02\npercent_home_owner                                              5.305e-02\ncensus_2018_total_personal_income_median_curp_15years_and_over  8.034e-05\n                                                               t value Pr(&gt;|t|)\n(Intercept)                                                      7.098 1.45e-09\npercent_no_qualification                                         7.002 2.13e-09\npercent_home_owner                                              -5.752 2.92e-07\ncensus_2018_total_personal_income_median_curp_15years_and_over  -4.101 0.000122\n                                                                  \n(Intercept)                                                    ***\npercent_no_qualification                                       ***\npercent_home_owner                                             ***\ncensus_2018_total_personal_income_median_curp_15years_and_over ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.118 on 62 degrees of freedom\nMultiple R-squared:  0.7607,    Adjusted R-squared:  0.7491 \nF-statistic: 65.68 on 3 and 62 DF,  p-value: &lt; 2.2e-16\n\n\nWe see that the \\(r^2\\) has jumped to 76%…now Durbin Watson test and map again.\n\ndw &lt;- durbinWatsonTest(model3)\ntidy(dw)\n\n# A tibble: 1 × 5\n  statistic p.value autocorrelation method             alternative\n      &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;      \n1      1.27       0           0.345 Durbin-Watson Test two.sided  \n\n#and for future use, write the residuals out\nmodel3_residuals &lt;- model3 %&gt;%\n  augment(., census_authorities_data_norm_filter)\n\n\nta_model3_data&lt;- ta%&gt;%\n  clean_names(.)%&gt;%\n  left_join(., model3_residuals,\n            by=c(\"ta2018_v1\"=\"area_code\"))\n\ntm_shape(ta_model3_data) +\n  tm_polygons(\".resid\",\n              palette = \"RdYlBu\")\n\n\n\n\n\n\n\n\nThis suggests that there could well be some spatial autocorrelation biasing our model, but can we test for spatial autocorrelation more systematically?\nYes - and some of you will remember this from the practical a few days ago. We can calculate a number of different statistics to check for spatial autocorrelation - the most common of these being Moran’s I.\n\n#calculate the centroids \n\nta_model3_data &lt;- ta_model3_data %&gt;%\n  filter(ta2018_v1 != \"999\" & ta2018_v1 != \"067\")\n\ncoordsW &lt;- ta_model3_data%&gt;%\n  st_centroid()%&gt;%\n  st_geometry()\n\nplot(coordsW)\n\n\n\n\n\n\n\n\n\nlibrary(spdep)\n\n#Now we need to generate a spatial weights matrix \n#We'll start with a simple binary matrix of queen's case neighbours\n\nta_nb &lt;- ta_model3_data %&gt;%\n  poly2nb(., queen=T)\n\n#or nearest neighbours\nta_knn &lt;-coordsW %&gt;%\n  knearneigh(., k=4)\n\nta_knn_nb &lt;- ta_knn %&gt;%\n  knn2nb()\n\n#plot them\nplot(ta_nb, st_geometry(coordsW), col=\"red\")\n\n\n\n\n\n\n\n\n\n#create a spatial weights matrix object from these weights\n\nta_queens_weight &lt;- ta_nb %&gt;%\n  nb2listw(., style=\"W\")\n\nta_knn_4_weight &lt;- ta_knn_nb %&gt;%\n  nb2listw(., style=\"W\")\n\nThe style argument means the style of the output — B is binary encoding listing them as neighbours or not, W row standardised that we saw yesterday.\nNow run a Moran’s I test on the residuals, first using queens neighbours\n\nqueen &lt;- ta_model3_data %&gt;%\n  st_drop_geometry()%&gt;%\n  dplyr::select(.resid)%&gt;%\n  pull()%&gt;%\n  moran.test(., ta_queens_weight)%&gt;%\n  tidy()\n\nThen nearest k-nearest neighbours\n\nnearest_neighbour &lt;- ta_model3_data %&gt;%\n  st_drop_geometry()%&gt;%\n  dplyr::select(.resid)%&gt;%\n  pull()%&gt;%\n  moran.test(., ta_knn_4_weight)%&gt;%\n  tidy()\n\nqueen\n\n# A tibble: 1 × 7\n  estimate1 estimate2 estimate3 statistic   p.value method           alternative\n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;      \n1     0.342   -0.0154   0.00818      3.95 0.0000387 Moran I test un… greater    \n\nnearest_neighbour\n\n# A tibble: 1 × 7\n  estimate1 estimate2 estimate3 statistic    p.value method          alternative\n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;      \n1     0.334   -0.0154   0.00622      4.43 0.00000474 Moran I test u… greater    \n\n\nObserving the Moran’s I statistic for both Queen’s case neighbours and k-nearest neighbours of 4, we can see that the Moran’s I statistic is somewhere between 0.34 and 0.33. Remembering that Moran’s I ranges from between -1 and +1 (0 indicating no spatial autocorrelation) we can conclude that there is some weak to moderate spatial autocorrelation in our residuals.\nThis means that despite passing most of the assumptions of linear regression, we could have a situation here where the presence of some spatial autocorrelation could be leading to biased estimates of our parameters and significance values.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial models</span>"
    ]
  },
  {
    "objectID": "04_spatial_models.html#spatial-regression-models",
    "href": "04_spatial_models.html#spatial-regression-models",
    "title": "4  Spatial models",
    "section": "4.4 Spatial regression models",
    "text": "4.4 Spatial regression models\n\n4.4.1 The spatial lag (lagged dependent variable) model\nRunning a Moran’s I test on the residuals from the model suggested that there might be some spatial autocorrelation occurring suggesting that places where the model over-predicted the percent of people smoking (those shown in blue in the map above with negative residuals) and under-predicted (those shown in red/orange) occasionally were near to each other.\nWard and Gleditsch (2008) describe this situation (where the value of our \\(y\\) dependent variable - percent people who smoke - may be influenced by neighbouring values) and suggest the way to deal with it is to incorporate a spatially-lagged version of this variable amongst the independent variables on the right-hand side of the equation. In this case, Equation 1 would be updated to look like this:\n\\[y_i = \\beta_0 + \\beta_1x_i + \\rho w_i.y_i + \\epsilon_i\\]\nIn this equation, \\(w\\) is the spatial weights matrix you generated and \\(w_i\\) is vector of all neighbouring areas (in our case, Territorial Authority) for any Territorial Authority \\(y_i\\). If using binary this will be a sum (e.g. the value of \\(y\\) plotted against the sum of \\(y\\) of the neighbouring Territorial Authority). This may be explained as a weighted sum as the non-neighbours will be removed, the weight being the neighbours If this is row standardised it will be a weighted average of the neighbouring \\(y\\) values. Typically we use a row standardised matrix.\nIn this model, a positive value for the \\(\\rho w_i.y_i\\) parameter would indicate that the average value for the percent smoking is expected to be higher if, on average, neighbouring authorities also have a higher percentage of people who smoke.\n\n\\(\\rho\\) denotes (represents) the spatial lag\n\nFor more details on running a spatially lagged regression model and interpreting the outputs, see the chapter on spatially lagged models by Ward and Gleditsch (2008) available online\n\n4.4.1.1 Queen’s case lag\nNow run a spatially-lagged regression model with a queen’s case weights matrix\n\nlibrary(spatialreg)\n\nslag_dv_model2_queen &lt;- lagsarlm(percent_regular_smoker ~\n               percent_no_qualification + \n               percent_home_owner +\n               census_2018_total_personal_income_median_curp_15years_and_over,\n               data = ta_model3_data, \n               nb2listw(ta_nb, style=\"C\"), \n               method = \"eigen\")\n\n#what do the outputs show?\ntidy(slag_dv_model2_queen)\n\n# A tibble: 5 × 5\n  term                                     estimate std.error statistic  p.value\n  &lt;chr&gt;                                       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 rho                                       2.33e-2 0.0311        0.748 4.55e- 1\n2 (Intercept)                               2.76e+1 3.82          7.23  4.66e-13\n3 percent_no_qualification                  5.08e-1 0.0720        7.06  1.67e-12\n4 percent_home_owner                       -3.00e-1 0.0516       -5.82  5.99e- 9\n5 census_2018_total_personal_income_media… -3.33e-4 0.0000777    -4.28  1.85e- 5\n\n#glance() gives model stats but this need something produced from a linear model\n#here we have used lagsarlm()\nglance(slag_dv_model2_queen)\n\n# A tibble: 1 × 6\n  r.squared   AIC   BIC deviance logLik  nobs\n      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1     0.763  294.  307.     276.  -141.    66\n\nhold&lt;-summary(slag_dv_model2_queen)\n\nsum(hold$residuals)\n\n[1] -3.039236e-15\n\n\nRunning the spatially-lagged model with a Queen’s case spatial weights matrix reveals that in this example, there is an insignificant and small effect associated with the spatially lagged dependent variable. However, a different conception of neighbours we might get a different outcome….\nHere:\n\nRho is our spatial lag (0.0232) that measures the variable in the surrounding spatial areas as defined by the spatial weight matrix. We use this as an extra explanatory variable to account for clustering (identified by Moran’s I). If significant it means that the percent of smokers in a unit vary based on the percent of smokers in the neighboring units. If it is positive it means as the percent of smokers increase in the surrounding units so does our central value\nLog likelihood shows how well the data fits the model (like the AIC, which we cover later), the higher the value the better the models fits the data.\nLikelihood ratio (LR) test shows if the addition of the lag is an improvement (from linear regression) and if that’s significant. This code would give the same output…\n\n\nlibrary(lmtest)\nlrtest(slag_dv_model2_queen, model3)\n\n\nLagrange Multiplier (LM) is a test for the absence of spatial autocorrelation in the lag model residuals. If significant then you can reject the Null (no spatial autocorrelation) and accept the alternative (is spatial autocorrelcation)\nWald test (often not used in interpretation of lag models), it tests if the new parameters (the lag) should be included it in the model…if significant then the new variable improves the model fit and needs to be included. This is similar to the LR test and i’ve not seen a situation where one is significant and the other not. Probably why it’s not used!\n\nIn this case we have spatial autocorrelation in the residuals of the model, but the model is not an improvement on OLS — this can also be confirmed with the AIC score (we cover that later) but the lower it is the better. Here it is 293, in OLS (model 3) it was 292. The Log likelihood for model 3 (OLS) was -141, here it is -140.\n\n4.4.1.1.1 Lag impacts\nWarning according to Solymosi and Medina (2022) you must not not compare the coefficients of this to regular OLS…Why ?\nWell in OLS recall we can use the coefficients to say…a 1 unit change in the independent variable means a drop or rise in the dependent (for a 1% increase in percent with no qualifications the percent of smokers rises by 0.64 percent). BUT here the model is not consistent as the observations will change based on the weight matrix neighbours selected which might vary (almost certainly in a distance based matrix). This means we have a direct effect (standard OLS) and then an indirect effect in the model (impact of the spatial lag).\nWe can compute these direct and indirect effects using code from Solymosi and Medina (2022) and the spatialreg package. Here the impacts() function calculates the impact of the spatial lag. We can fit this to our entire spatial weights….\n\nlibrary(spdep)\n\n# weight list is just the code from the lagsarlm\nweight_list&lt;-nb2listw(ta_knn_nb, style=\"C\")\n\nimp &lt;- impacts(slag_dv_model2_queen, listw=weight_list)\n\nimp\n\nImpact measures (lag, exact):\n                                                                            Direct\npercent_no_qualification dy/dx                                        0.5083288540\npercent_home_owner dy/dx                                             -0.3001931060\ncensus_2018_total_personal_income_median_curp_15years_and_over dy/dx -0.0003328803\n                                                                          Indirect\npercent_no_qualification dy/dx                                        1.206307e-02\npercent_home_owner dy/dx                                             -7.123832e-03\ncensus_2018_total_personal_income_median_curp_15years_and_over dy/dx -7.899525e-06\n                                                                             Total\npercent_no_qualification dy/dx                                        0.5203919197\npercent_home_owner dy/dx                                             -0.3073169376\ncensus_2018_total_personal_income_median_curp_15years_and_over dy/dx -0.0003407798\n\n\nNow it is appropriate to compare these coefficients to the OLS outputs…however if you have a very large matrix this might not work, instead a sparse matrix that uses approximation methods (see Solymosi and Medina (2022) and within that resource, Lesage and Pace 2009). This is beyond the scope of the content here, but essentially this makes the method faster on larger data…but only row standardised is permitted here…\n\n\n\n\n4.4.2 The spatial error model\nAnother way of coneptualising spatial dependence in regression models is not through values of the dependent variable in some areas affecting those in neighbouring areas (as they do in the spatial lag model), but in treating the spatial autocorrelation in the residuals as something that we need to deal with, perhaps reflecting some spatial autocorrelation amongst unobserved independent variables or some other mis-specification of the model.\nWard and Gleditsch (2008) characterise this model as seeing spatial autocorrelation as a nuisance rather than being particularly informative, however it can still be handled within the model, albeit slightly differently.\nThe spatial error model can be written:\n\\[y_i = \\beta_0 + \\beta_1x_i + \\lambda w_i\\xi_i + \\epsilon_i\\]\nwhere\n\n\\(\\xi_i\\) is the spatial component of the error terms…in other words the residuals of the values in surrounding spatial units based on the weight matrix. The same rules apply as the lag (e.g. binary = sum, row standardisied = weighted average).\n\\(\\lambda\\) is a measure of correlation between neighbouring residuals.. “it indicates the extent to which the spatial component of the errors \\(\\xi_i\\) are correlated with one another for nearby observations” as per the weight matrix, Ward and Gleditsch (2008). If there is no correlation between the then this defaults to normal OLS regression\n\nFor more detail on the spatial error model, see Ward and Gleditsch (2008)\nWe can run a spatial error model on the same data below:\n\nsem_1 &lt;- errorsarlm(percent_regular_smoker ~\n               percent_no_qualification + \n               percent_home_owner +\n               census_2018_total_personal_income_median_curp_15years_and_over,\n               data = ta_model3_data, \n               nb2listw(ta_nb, style=\"C\"), \n               method = \"eigen\")\n\n\ntidy(sem_1)\n\n# A tibble: 5 × 5\n  term                                     estimate std.error statistic  p.value\n  &lt;chr&gt;                                       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)                               2.53e+1 3.78           6.69 2.28e-11\n2 percent_no_qualification                  5.71e-1 0.0686         8.33 0       \n3 percent_home_owner                       -2.94e-1 0.0526        -5.58 2.41e- 8\n4 census_2018_total_personal_income_media… -3.04e-4 0.0000796     -3.82 1.33e- 4\n5 lambda                                    4.32e-1 0.118          3.65 2.62e- 4\n\n\nComparing the results of the spatial error model with the spatially lagged model and the original OLS model, the suggestion here is that the spatially correlated errors in residuals lead to over/under estimations of the coefficients.\nNote, here we can compare to OLS as there is no spatial lag.\nThe \\(\\lambda\\) parameter in the spatial error model is larger than the standard error and is significant, but the \\(\\rho\\) parameter is not larger than the standard error and is not significant.\nSo we can conclude that spatial dependence might be borne in mind when interpreting the results of this regression model. This suggests to us that the smoking is not influenced by neighbouring polygons (or smokers) but the errors between the neighours are related and we may have some mis-specification of the model - e.g. a missing variable.\n\n\n4.4.3 Key advice\nThe lag model accounts for situations where the value of the dependent variable in one area might be associated with or influenced by the values of that variable in neighbouring zones (however we choose to define neighbouring in our spatial weights matrix). With our example, smoking in one neighbourhood might be related to smoking in another. You may be able to think of other examples where similar associations may occur. You might run a lag model if you identify spatial autocorrelation in the dependent variable (closer spatial units have similar values) with Moran’s I.\nThe error model deals with spatial autocorrelation (closer spatial units have similar values) of the residuals (vertical distance between your point and line of model – errors – over-predictions or under-predictions) again, potentially revealed though a Moran’s I analysis. The error model is not assuming that neighbouring independent variables are influencing the dependent variable but rather the assumption is of an issue with the specification of the model or the data used (e.g. clustered errors are due to some un-observed clustered variables not included in the model). For example, smoking prevelance may be similar in bordering neighbourhoods because residents in these neighbouring places come from similar socio-economic backgrounds and this was not included as an independent variable in the original model. There is no spatial process (no cross authority interaction) just a cluster of an un-accounted for but influential variable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial models</span>"
    ]
  },
  {
    "objectID": "04_spatial_models.html#gwr",
    "href": "04_spatial_models.html#gwr",
    "title": "4  Spatial models",
    "section": "4.5 GWR",
    "text": "4.5 GWR\nRather than spatial autocorrelation causing problems with our model, it might be that a “global” regression model does not capture the full story. In some parts of our study area, the relationships between the dependent and independent variable may not exhibit the same slope coefficient.\nIf this occurs, then we have ’non-stationarity’ - this is when the global model does not represent the relationships between variables that might vary locally.\n\n4.5.1 Bandwidth\nThis part of the practical will only skirt the edges of GWR, for much more detail you should visit the GWR website which is produced and maintained by Prof Chris Brunsdon and Dr Martin Charlton who originally developed the technique.\nI should also acknowledge the guide on GWR produced by the University of Bristol, which was a great help when producing this exercise.\n\nlibrary(spgwr)\n\ncoordsW2 &lt;- st_coordinates(coordsW)\n\nta_model3_data_gwr &lt;- cbind(ta_model3_data,coordsW2)\n\ngwrbandwidth &lt;- gwr.sel(percent_regular_smoker ~\n               percent_no_qualification + \n               percent_home_owner +\n               census_2018_total_personal_income_median_curp_15years_and_over,\n               data = ta_model3_data_gwr,  \n                        coords=cbind(ta_model3_data_gwr$X, ta_model3_data_gwr$Y),\n                  adapt=T)\n\nAdaptive q: 0.381966 CV score: 258.7724 \nAdaptive q: 0.618034 CV score: 288.1859 \nAdaptive q: 0.236068 CV score: 223.3048 \nAdaptive q: 0.145898 CV score: 210.9782 \nAdaptive q: 0.03932385 CV score: 212.4756 \nAdaptive q: 0.1017793 CV score: 202.4218 \nAdaptive q: 0.09472045 CV score: 203.8337 \nAdaptive q: 0.111242 CV score: 202.5485 \nAdaptive q: 0.1053937 CV score: 201.7625 \nAdaptive q: 0.1063105 CV score: 201.6931 \nAdaptive q: 0.1074627 CV score: 201.8976 \nAdaptive q: 0.1061616 CV score: 201.6666 \nAdaptive q: 0.1059668 CV score: 201.6645 \nAdaptive q: 0.1057479 CV score: 201.7017 \nAdaptive q: 0.1060531 CV score: 201.6499 \nAdaptive q: 0.1060938 CV score: 201.6545 \nAdaptive q: 0.1060124 CV score: 201.6567 \nAdaptive q: 0.1060531 CV score: 201.6499 \n\ngwrbandwidth\n\n[1] 0.1060531\n\n\nSetting adapt=T here means to automatically find the proportion of observations for the weighting using k nearest neighbours (an adaptive bandwidth), False would mean a global bandwidth and that would be in meters (as our data is projected).\nOccasionally data can come with longitude and latitude as columns (e.g. WGS84) and we can use this straight in the function to save making centroids, calculating the coordinates and then joining - the argument for this is longlat=TRUE and then the columns selected in the coords argument e.g. coords=cbind(long, lat). The distance result will then be in KM.\nThe optimal bandwidth is about 0.106 meaning 10.6% of all the total spatial units should be used for the local regression based on k-nearest neighbours. Which is about 7 of the 66 territorial authorities.\nThis approach uses cross validation to search for the optimal bandwidth, it compares different bandwidths to minimise model residuals — this is why we specify the regression model within gwr.sel(). It does this with a Gaussian weighting scheme (which is the default) - meaning that near points have greater influence in the regression and the influence decreases with distance - there are variations of this, but Gaussian is a fine to use in most applications. To change this we would set the argument gweight = gwr.Gauss in the gwr.sel() function — gwr.bisquare() is the other option. We don’t go into cross validation in this module.\nHowever we could set either the number of neighbours considered or the distance within which to considered points ourselves, manually, in the gwr() function below.\nTo set the number of other neighbours considered simply change the adapt argument to the value you want - it must be the number of neighbours divided by the total (e.g. to consider 20 neighbours it would be 20/66 and you’d use the value of 0.30)\nTo set the bandwidth, remove the adapt argument and replace it with bandwidth and set it, in this case, in meters.\nTo conclude, we can:\n\nset the bandwidth in gwr.sel() automatically using:\n\nthe number of neighbors\na distance threshold\n\nOr, we can set it manually in gwr() using:\n\na number of neighbors\na distance threshold\n\n\n\n\n\n\n\n\nTip\n\n\n\nBUT a problem with setting a fixed bandwidth is we assume that all variables have the same relationship across the same space (using the same number of neighbours or distance). We can let these bandwidths vary as some relationships will operate on different spatial scales…this is called Multiscale GWR and Lex Comber recently said that all GWR should be Multisacle (oops!). We have already covered a lot here so i won’t go into it. If you are interested Lex has a good tutorial on Multiscale GWR\n\n\n\n\n4.5.2 Model\n\n#run the gwr model\ngwr_model = gwr(percent_regular_smoker ~\n               percent_no_qualification + \n               percent_home_owner +\n               census_2018_total_personal_income_median_curp_15years_and_over,\n               data = ta_model3_data_gwr,  \n                        coords=cbind(ta_model3_data_gwr$X, ta_model3_data_gwr$Y), \n                adapt=gwrbandwidth,\n                #matrix output\n                hatmatrix=TRUE,\n                #standard error\n                se.fit=TRUE)\n\n#print the results of the model\ngwr_model\n\nCall:\ngwr(formula = percent_regular_smoker ~ percent_no_qualification + \n    percent_home_owner + census_2018_total_personal_income_median_curp_15years_and_over, \n    data = ta_model3_data_gwr, coords = cbind(ta_model3_data_gwr$X, \n        ta_model3_data_gwr$Y), adapt = gwrbandwidth, hatmatrix = TRUE, \n    se.fit = TRUE)\nKernel function: gwr.Gauss \nAdaptive quantile: 0.1060531 (about 6 of 66 data points)\nSummary of GWR coefficient estimates at data points:\n                                                                      Min.\nX.Intercept.                                                    3.3703e+00\npercent_no_qualification                                        4.1815e-01\npercent_home_owner                                             -4.4357e-01\ncensus_2018_total_personal_income_median_curp_15years_and_over -9.2994e-04\n                                                                   1st Qu.\nX.Intercept.                                                    1.2399e+01\npercent_no_qualification                                        4.8399e-01\npercent_home_owner                                             -3.1838e-01\ncensus_2018_total_personal_income_median_curp_15years_and_over -5.2518e-04\n                                                                    Median\nX.Intercept.                                                    1.7101e+01\npercent_no_qualification                                        5.2129e-01\npercent_home_owner                                             -2.4756e-01\ncensus_2018_total_personal_income_median_curp_15years_and_over -9.6083e-05\n                                                                   3rd Qu.\nX.Intercept.                                                    3.4618e+01\npercent_no_qualification                                        5.8085e-01\npercent_home_owner                                             -1.8668e-01\ncensus_2018_total_personal_income_median_curp_15years_and_over -2.1233e-05\n                                                                      Max.\nX.Intercept.                                                    4.8631e+01\npercent_no_qualification                                        6.9995e-01\npercent_home_owner                                             -1.6115e-01\ncensus_2018_total_personal_income_median_curp_15years_and_over  1.9089e-04\n                                                                Global\nX.Intercept.                                                   27.9622\npercent_no_qualification                                        0.5152\npercent_home_owner                                             -0.3051\ncensus_2018_total_personal_income_median_curp_15years_and_over -0.0003\nNumber of data points: 66 \nEffective number of parameters (residual: 2traceS - traceS'S): 25.09782 \nEffective degrees of freedom (residual: 2traceS - traceS'S): 40.90218 \nSigma (residual: 2traceS - traceS'S): 1.503921 \nEffective number of parameters (model: traceS): 19.93781 \nEffective degrees of freedom (model: traceS): 46.06219 \nSigma (model: traceS): 1.417184 \nSigma (ML): 1.183931 \nAICc (GWR p. 61, eq 2.33; p. 96, eq. 4.21): 272.3116 \nAIC (GWR p. 96, eq. 4.22): 229.5246 \nResidual sum of squares: 92.51173 \nQuasi-global R2: 0.9204215 \n\n\nThe output from the GWR model reveals how the coefficients vary across the 66 territorial authorities New Zealand. You will see how the global coefficients are exactly the same as the coefficients in the earlier lm model. In this particular model, if we take percent with no qualification, we can see that the coefficients range from a minimum value of 0.41 (1 unit change in percent with no qualification resulting in a rise in percent smoking of 0.41) to 0.69 (1 unit change in percent with no qualification resulting in a rise in percent smoking of 0.69).\nYou will notice that the R-Squared value (Quasi global R-squared) has improved - this is not uncommon for GWR models, but it doesn’t necessarily mean they are definitely better than global models. The small number of cases (spatial areas) under the kernel means that GW models have been criticised for lacking statistical robustness.\nThe best way to compare models is with the AIC (Akaike Information Criterion) or for smaller sample sizes the sample-size adjusted AICc, especially when you number of points is less than 40! Which it will be in GWR. The models must also be using the same data and be over the same study area!\nAIC is calculated using the:\n\nnumber of independent variables\nmaximum likelihood estimate of the model (how well the model reproduces the data).\n\nThe lower the value the better the better the model fit is, see scribbr if you want to know more here..although this is enough to get you through most situations.\nCoefficient ranges can also be seen for the other variables and they suggest some interesting spatial patterning.\n\n\n4.5.3 Map coefficients\nTo explore this we can plot the GWR coefficients for different variables. Firstly we can attach the coefficients to our original dataframe - this can be achieved simply as the coefficients for each territoiral authority appear in the same order in our spatial points dataframe as they do in the original dataframe.\n\nresults &lt;- as.data.frame(gwr_model$SDF)\nnames(results)\n\n [1] \"sum.w\"                                                                \n [2] \"X.Intercept.\"                                                         \n [3] \"percent_no_qualification\"                                             \n [4] \"percent_home_owner\"                                                   \n [5] \"census_2018_total_personal_income_median_curp_15years_and_over\"       \n [6] \"X.Intercept._se\"                                                      \n [7] \"percent_no_qualification_se\"                                          \n [8] \"percent_home_owner_se\"                                                \n [9] \"census_2018_total_personal_income_median_curp_15years_and_over_se\"    \n[10] \"gwr.e\"                                                                \n[11] \"pred\"                                                                 \n[12] \"pred.se\"                                                              \n[13] \"localR2\"                                                              \n[14] \"X.Intercept._se_EDF\"                                                  \n[15] \"percent_no_qualification_se_EDF\"                                      \n[16] \"percent_home_owner_se_EDF\"                                            \n[17] \"census_2018_total_personal_income_median_curp_15years_and_over_se_EDF\"\n[18] \"pred.se.1\"                                                            \n[19] \"coord.x\"                                                              \n[20] \"coord.y\"                                                              \n\n\n\n#attach coefficients to original\n\n\nta_model3_data_gwr_results &lt;- ta_model3_data_gwr %&gt;%\n  mutate(coefqualification = results$percent_no_qualification,\n         coefhomeowner = results$percent_home_owner,\n         coefincome = results$census_2018_total_personal_income_median_curp_15years_and_over_se)\n\n\ntm_shape(ta_model3_data_gwr_results) +\n  tm_polygons(col = \"coefqualification\", \n              palette = \"RdBu\", \n              alpha = 0.5)\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_tm_polygons()`: migrate the argument(s) related to the scale of\nthe visual variable `fill` namely 'palette' (rename to 'values') to fill.scale\n= tm_scale(&lt;HERE&gt;).\n[v3-&gt;v4] `tm_polygons()`: use 'fill' for the fill color of polygons/symbols\n(instead of 'col'), and 'col' for the outlines (instead of 'border.col').\n[v3-&gt;v4] `tm_polygons()`: use `fill_alpha` instead of `alpha`.\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"RdBu\" is named\n\"brewer.rd_bu\"\nMultiple palettes called \"rd_bu\" found: \"brewer.rd_bu\", \"matplotlib.rd_bu\". The first one, \"brewer.rd_bu\", is returned.\n\n\n\n\n\n\n\n\n\nTaking the first coefficient we see that this varies across the country. A possible trend to explore further might be the inclusion or distance to tobacco retailers - both Wellington and Auckland have some of the highest coefficients, meaning a 1 unit (percent) change in no qualification returns a higher rise in percent of people smoking.\n\n\n4.5.4 Significance\nOf course, these results may not be statistically significant across the whole of New Zealand. Roughly speaking, if a coefficient estimate is more than 2 standard errors away from zero, then it is “statistically significant”.\nRemember from earlier the standard error is “the average amount the coefficient varies from the average value of the dependent variable (its standard deviation). So, for 1% increase in percent with no qualifications, while the model says we might expect GSCE scores to drop by 0.6%, this might vary, on average, by about 0.08%. As a rule of thumb, we are looking for a lower value in the standard error relative to the size of the coefficient.”\nTo calculate standard errors, for each variable you can use a formula similar to this:\n\n#run the significance test\n# abs gets the absolute vale - negative values are converted into positive value\nsigTest = abs(gwr_model$SDF$percent_no_qualification)- 2* gwr_model$SDF$percent_no_qualification_se\n\n\n#store significance results\nta_model3_data_gwr_results &lt;- ta_model3_data_gwr_results %&gt;%\n  mutate(gwrpercent_no_qualification_sig = sigTest)\n\n\ntm_shape(ta_model3_data_gwr_results) +\n  tm_polygons(col = \"gwrpercent_no_qualification_sig\", \n              palette = \"RdYlBu\")\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_tm_polygons()`: migrate the argument(s) related to the scale of\nthe visual variable `fill` namely 'palette' (rename to 'values') to fill.scale\n= tm_scale(&lt;HERE&gt;).\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"RdYlBu\" is named\n\"brewer.rd_yl_bu\"\nMultiple palettes called \"rd_yl_bu\" found: \"brewer.rd_yl_bu\", \"matplotlib.rd_yl_bu\". The first one, \"brewer.rd_yl_bu\", is returned.\n\n\n\n\n\n\n\n\n\nIf this is greater than zero (i.e. the estimate is more than two standard errors away from zero), it is very unlikely that the true value is zero, i.e. it is statistically significant (at nearly the 95% confidence level)\nThis is a combination of two ideas:\n\n95% of data in a normal distribution is within two standard deviations of the mean\nStatistical significance in a regression is normally measured at the 95% level. If the p-value is less than 5% — 0.05 — then there’s a 95% probability that a coefficient as large as you are observing didn’t occur by chance\n\nCombining these two means if…\n\nthe coefficient is large in relation to its standard error and\nthe p-value tells you if that largeness is statistically acceptable - at the 95% level (less than 5% — 0.05)\n\nYou can be confident that in your sample, nearly all of the time, that is a real and reliable coefficient value.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial models</span>"
    ]
  },
  {
    "objectID": "04_spatial_models.html#including-spatial-features",
    "href": "04_spatial_models.html#including-spatial-features",
    "title": "4  Spatial models",
    "section": "4.6 Including spatial features",
    "text": "4.6 Including spatial features\nI mentioned at the start we could also try and include some spatial features within our model - for example the density of shops that sell tobacco? In a sense this is similar to what is termed a spatially omitted covariate… typically this means that the nearer something is the more influence it might have on our model (e.g. parks and house prices?).\nHowever, we could only really calculate distance to tobacco shops from the centroid of the territorial authority as a new independent variable. But instead we could use density of shops per territorial authority!\nHere, i will use open street map to calculate a density. I have downloaded the data from Geofabrik\n\nosm_pois &lt;- st_read(\"prac4_data/new-zealand-latest-free.shp/gis_osm_pois_a_free_1.shp\")\n\nReading layer `gis_osm_pois_a_free_1' from data source \n  `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\Guest\\Oxford_26\\spatial analysis of public health\\spatial-analysis-of-public-health-data\\prac4_data\\new-zealand-latest-free.shp\\gis_osm_pois_a_free_1.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 59185 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -176.6404 ymin: -52.59086 xmax: 178.5443 ymax: -34.42668\nGeodetic CRS:  WGS 84\n\nosm_pois_filter &lt;- osm_pois %&gt;%\n  filter(fclass == \"supermarket\" | fclass ==\"convenience\")%&gt;%\n  st_transform(., 2135)\n\nNow let’s intersect with the territorial authority boundaries and calculate a density.\nNote that the districts object is in the correct CRS, however i beleive this is due to naming differences so we will transform it just incase.\n\nta &lt;- ta %&gt;%\n   st_transform(., 2135)\n\nstores_joined &lt;- ta %&gt;%\n  mutate(n = lengths(st_intersects(., osm_pois_filter)))%&gt;%\n  janitor::clean_names()%&gt;%\n  #calculate area\n  mutate(area=st_area(.))%&gt;%\n  #then density of the points per ward\n  mutate(density=n/area)\n\nNow, we could use this in our model!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Spatial models</span>"
    ]
  },
  {
    "objectID": "05_food_bank_accessibility.html",
    "href": "05_food_bank_accessibility.html",
    "title": "5  Food bank accessibility",
    "section": "",
    "text": "5.1 Convert food bank addresses to point locations\nFirstly, load the R packages needed for the practical. You may need to install some of these for the first time.\n#Load relevant packages\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(RColorBrewer)\nlibrary(ggplot2) #for graphs\nlibrary(tmap) #for maps\nlibrary(tmaptools) #for maps\nlibrary(leaflet) #for maps\nlibrary(reader)\nlibrary(readxl)\nlibrary(data.table)\nlibrary(knitr)\nlibrary(sp)\nlibrary(sf)\nlibrary(spdep)\nlibrary(spatstat)\nlibrary(PostcodesioR) #for postcode lat and long\nlibrary(mosaic) #for favstats function\nlibrary(geodist) #for distances\nlibrary(rje) #for row mins\nlibrary(osmdata) #for osm\nlibrary(osmextract) # for osm\nlibrary(osrm) #for osm\nlibrary(r5r) #for public transport\nlibrary(tidygeocoder)\nlibrary(tidytransit) #for public transport",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Food bank accessibility</span>"
    ]
  },
  {
    "objectID": "05_food_bank_accessibility.html#convert-food-bank-addresses-to-point-locations",
    "href": "05_food_bank_accessibility.html#convert-food-bank-addresses-to-point-locations",
    "title": "5  Food bank accessibility",
    "section": "",
    "text": "5.1.1 Load the spatial data\nWe need to load the spatial data for different spatial scales. This polygons data will be used for creating maps and calculating accessibility to food banks centres. These were downloaded from the Open Geography Portal (from the Office for National Statistics).\n\nLower super output area (LSOA) scale\nLocal authority scale\nEntire country scale\n\nThere are 35,672 LSOAs (for England and Wales)\n\n#2021 shape file for LSOAs\nLSOA_2021 &lt;- st_read(\"prac5_data/LSOA_2021/LSOA_(Dec_2021)_Boundaries_Generalised_Clipped_EW_(BGC).shp\") %&gt;%\n  st_transform(., 27700) %&gt;%\n  clean_names() \n\nReading layer `LSOA_(Dec_2021)_Boundaries_Generalised_Clipped_EW_(BGC)' from data source `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\Guest\\Oxford_26\\spatial analysis of public health\\spatial-analysis-of-public-health-data\\prac5_data\\LSOA_2021\\LSOA_(Dec_2021)_Boundaries_Generalised_Clipped_EW_(BGC).shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 35672 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 82668.52 ymin: 5352.6 xmax: 655653.8 ymax: 657539.4\nProjected CRS: OSGB36 / British National Grid\n\n\nThere are 374 local authorities (for the UK)\n\n#2022 shape file for LAs\nLA_2022 &lt;- st_read(\"prac5_data/LA_2022/LAD_MAY_2022_UK_BFE_V3.shp\") %&gt;%\n  st_transform(., 27700) %&gt;%\n  clean_names() \n\nReading layer `LAD_MAY_2022_UK_BFE_V3' from data source \n  `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\Guest\\Oxford_26\\spatial analysis of public health\\spatial-analysis-of-public-health-data\\prac5_data\\LA_2022\\LAD_MAY_2022_UK_BFE_V3.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 374 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -116.1928 ymin: 5333.81 xmax: 655989 ymax: 1220310\nProjected CRS: OSGB36 / British National Grid\n\n\n\n#2023 shape file for UK\nUK_border &lt;- st_read(\"prac5_data/UK_2023/CTRY_DEC_2023_UK_BUC.shp\") %&gt;%\n  st_transform(., 27700) %&gt;%\n  clean_names() \n\nReading layer `CTRY_DEC_2023_UK_BUC' from data source \n  `C:\\Users\\Andy\\OneDrive - University College London\\Teaching\\Guest\\Oxford_26\\spatial analysis of public health\\spatial-analysis-of-public-health-data\\prac5_data\\UK_2023\\CTRY_DEC_2023_UK_BUC.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 4 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -116.1928 ymin: 7054.1 xmax: 655653.8 ymax: 1220310\nProjected CRS: OSGB36 / British National Grid\n\n\n\n\n5.1.2 Data on Trussell food bank centres\nNext, we load the Trussell food bank data with addresses (this data was provided by Trussell in February 2023 so some centres may have closed in the last two years or new centres may have opened).\n\n#locations of food banks from TT\nTT_foodbanks &lt;- read_excel(\"prac5_data/Food banks Feb23.xlsx\") %&gt;%\n  clean_names()\n\nIn order to derive point locations from postcodes, we need to clean the food bank postcodes so they are in a uniform and readable form. At this stage, we also remove food bank centres that are not currently open or are warehouses or offices (where people cannot collect food from).\n\n#removing white space from postcodes\nTT_foodbanks$postcode &lt;- gsub(\"[[:space:]]\", \"\", TT_foodbanks$postcode)\n\n#filter out only distribution centres\nTT_foodbanks &lt;- TT_foodbanks %&gt;%\n  filter(type == \"Distribution Centre\") %&gt;% #only want distribution centres and not warehouses or offices\n  filter(operating_model != \"Temporarily Closed\") #remove centres that are currently closed\n\nNext, we derive the point locations from postcodes. Unfortunately, the approach below only allows 100 locations at a time to be converte and the lat/lon will be a centroid of the postcode not the actual location of the distribution centre.\nThe function lapply used in these code chunks applies a function over a list or vector. We are applying it to a list of postcodes.\n\nTT_foodbanks1 &lt;- TT_foodbanks[1:100, ]\n\n#make postcodes into a list\nlist &lt;- as.list(unique(TT_foodbanks1$postcode))\n\n#foodbank locations\n#lat and long from postcodes using bulk postcode look up function - limited to 100\npc_list &lt;- list(postcodes = list)\nbulk_lookup_result &lt;- bulk_postcode_lookup(pc_list)\nstr(bulk_lookup_result[1])\n\nList of 1\n $ :List of 2\n  ..$ query : chr \"KT190JG\"\n  ..$ result:List of 42\n  .. ..$ postcode                       : chr \"KT19 0JG\"\n  .. ..$ quality                        : int 1\n  .. ..$ eastings                       : int 520624\n  .. ..$ northings                      : int 164193\n  .. ..$ country                        : chr \"England\"\n  .. ..$ nhs_ha                         : chr \"South East Coast\"\n  .. ..$ longitude                      : num -0.269\n  .. ..$ latitude                       : num 51.4\n  .. ..$ european_electoral_region      : chr \"South East\"\n  .. ..$ primary_care_trust             : chr \"Surrey\"\n  .. ..$ region                         : chr \"South East\"\n  .. ..$ lsoa                           : chr \"Epsom and Ewell 002A\"\n  .. ..$ msoa                           : chr \"Epsom and Ewell 002\"\n  .. ..$ incode                         : chr \"0JG\"\n  .. ..$ outcode                        : chr \"KT19\"\n  .. ..$ parliamentary_constituency     : chr \"Epsom and Ewell\"\n  .. ..$ parliamentary_constituency_2024: chr \"Epsom and Ewell\"\n  .. ..$ admin_district                 : chr \"Epsom and Ewell\"\n  .. ..$ parish                         : chr \"Epsom and Ewell, unparished area\"\n  .. ..$ admin_county                   : chr \"Surrey\"\n  .. ..$ date_of_introduction           : chr \"198001\"\n  .. ..$ admin_ward                     : chr \"Ewell Court\"\n  .. ..$ ced                            : chr \"Ewell Court, Auriol and Cuddington\"\n  .. ..$ ccg                            : chr \"NHS Surrey Heartlands\"\n  .. ..$ nuts                           : chr \"Epsom and Ewell\"\n  .. ..$ pfa                            : chr \"Surrey\"\n  .. ..$ nhs_region                     : chr \"South East\"\n  .. ..$ ttwa                           : chr \"London\"\n  .. ..$ national_park                  : chr \"England (non-National Park)\"\n  .. ..$ bua                            : chr \"Ewell\"\n  .. ..$ icb                            : chr \"NHS Surrey Heartlands Integrated Care Board\"\n  .. ..$ cancer_alliance                : chr \"Surrey and Sussex\"\n  .. ..$ lsoa11                         : chr \"Epsom and Ewell 002A\"\n  .. ..$ msoa11                         : chr \"Epsom and Ewell 002\"\n  .. ..$ lsoa21                         : chr \"Epsom and Ewell 002A\"\n  .. ..$ msoa21                         : chr \"Epsom and Ewell 002\"\n  .. ..$ oa21                           : chr \"E00155030\"\n  .. ..$ ruc11                          : chr \"(England/Wales) Urban major conurbation\"\n  .. ..$ ruc21                          : chr \"Urban: Nearer to a major town or city\"\n  .. ..$ lep1                           : chr \"Coast to Capital\"\n  .. ..$ lep2                           : NULL\n  .. ..$ codes                          :List of 29\n  .. .. ..$ admin_district                 : chr \"E07000208\"\n  .. .. ..$ admin_county                   : chr \"E10000030\"\n  .. .. ..$ admin_ward                     : chr \"E05015097\"\n  .. .. ..$ parish                         : chr \"E43000137\"\n  .. .. ..$ parliamentary_constituency     : chr \"E14001227\"\n  .. .. ..$ parliamentary_constituency_2024: chr \"E14001227\"\n  .. .. ..$ ccg                            : chr \"E38000264\"\n  .. .. ..$ ccg_id                         : chr \"92A\"\n  .. .. ..$ ced                            : chr \"E58001483\"\n  .. .. ..$ nuts                           : chr \"TLJ26\"\n  .. .. ..$ lsoa                           : chr \"E01030394\"\n  .. .. ..$ msoa                           : chr \"E02006336\"\n  .. .. ..$ lau2                           : chr \"E07000208\"\n  .. .. ..$ pfa                            : chr \"E23000031\"\n  .. .. ..$ nhs_region                     : chr \"E40000005\"\n  .. .. ..$ ttwa                           : chr \"E30000234\"\n  .. .. ..$ national_park                  : chr \"E65000001\"\n  .. .. ..$ bua                            : chr \"E63012408\"\n  .. .. ..$ icb                            : chr \"E54000063\"\n  .. .. ..$ cancer_alliance                : chr \"E56000012\"\n  .. .. ..$ lsoa11                         : chr \"E01030394\"\n  .. .. ..$ msoa11                         : chr \"E02006336\"\n  .. .. ..$ lsoa21                         : chr \"E01030394\"\n  .. .. ..$ msoa21                         : chr \"E02006336\"\n  .. .. ..$ oa21                           : chr \"E00155030\"\n  .. .. ..$ ruc11                          : chr \"A1\"\n  .. .. ..$ ruc21                          : chr \"UN1\"\n  .. .. ..$ lep1                           : chr \"E37000044\"\n  .. .. ..$ lep2                           : NULL\n\n#postcode look up - extract 2nd element from the list\nbulk_list &lt;- lapply(bulk_lookup_result, \"[[\", 2)\n\nTT_foodbank_location_points1 &lt;-\n  # iterate over each element and extract the following into a dataframe\n  map_dfr(bulk_list,\n          `[`,\n          c(\"postcode\", \"longitude\", \"latitude\"))\n\n#as spatial points\nTT_foodbank_location_points1 &lt;- \n  st_as_sf(TT_foodbank_location_points1,\n           coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\nTT_foodbanks2 &lt;- TT_foodbanks[101:200, ]\n\n#make postcodes into a list\nlist &lt;- as.list(unique(TT_foodbanks2$postcode))\n\n#foodbank locations\n#lat and long from postcodes\npc_list &lt;- list(postcodes = list)\nbulk_lookup_result &lt;- bulk_postcode_lookup(pc_list)\nstr(bulk_lookup_result[1])\n\nList of 1\n $ :List of 2\n  ..$ query : chr \"LA231DY\"\n  ..$ result:List of 42\n  .. ..$ postcode                       : chr \"LA23 1DY\"\n  .. ..$ quality                        : int 1\n  .. ..$ eastings                       : int 341211\n  .. ..$ northings                      : int 498535\n  .. ..$ country                        : chr \"England\"\n  .. ..$ nhs_ha                         : chr \"North West\"\n  .. ..$ longitude                      : num -2.91\n  .. ..$ latitude                       : num 54.4\n  .. ..$ european_electoral_region      : chr \"North West\"\n  .. ..$ primary_care_trust             : chr \"Cumbria Teaching\"\n  .. ..$ region                         : chr \"North West\"\n  .. ..$ lsoa                           : chr \"South Lakeland 001D\"\n  .. ..$ msoa                           : chr \"South Lakeland 001\"\n  .. ..$ incode                         : chr \"1DY\"\n  .. ..$ outcode                        : chr \"LA23\"\n  .. ..$ parliamentary_constituency     : chr \"Westmorland and Lonsdale\"\n  .. ..$ parliamentary_constituency_2024: chr \"Westmorland and Lonsdale\"\n  .. ..$ admin_district                 : chr \"Westmorland and Furness\"\n  .. ..$ parish                         : chr \"Windermere and Bowness\"\n  .. ..$ admin_county                   : NULL\n  .. ..$ date_of_introduction           : chr \"198001\"\n  .. ..$ admin_ward                     : chr \"Windermere and Ambleside\"\n  .. ..$ ced                            : NULL\n  .. ..$ ccg                            : chr \"NHS Lancashire and South Cumbria\"\n  .. ..$ nuts                           : chr \"Westmorland and Furness\"\n  .. ..$ pfa                            : chr \"Cumbria\"\n  .. ..$ nhs_region                     : chr \"North West\"\n  .. ..$ ttwa                           : chr \"Kendal\"\n  .. ..$ national_park                  : chr \"Lake District\"\n  .. ..$ bua                            : chr \"Windermere\"\n  .. ..$ icb                            : chr \"NHS Lancashire and South Cumbria Integrated Care Board\"\n  .. ..$ cancer_alliance                : chr \"Lancashire and South Cumbria\"\n  .. ..$ lsoa11                         : chr \"South Lakeland 001D\"\n  .. ..$ msoa11                         : chr \"South Lakeland 001\"\n  .. ..$ lsoa21                         : chr \"South Lakeland 001D\"\n  .. ..$ msoa21                         : chr \"South Lakeland 001\"\n  .. ..$ oa21                           : chr \"E00098085\"\n  .. ..$ ruc11                          : chr \"(England/Wales) Rural town and fringe in a sparse setting\"\n  .. ..$ ruc21                          : chr \"Larger rural: Further from a major town or city\"\n  .. ..$ lep1                           : chr \"Cumbria\"\n  .. ..$ lep2                           : NULL\n  .. ..$ codes                          :List of 29\n  .. .. ..$ admin_district                 : chr \"E06000064\"\n  .. .. ..$ admin_county                   : chr \"E99999999\"\n  .. .. ..$ admin_ward                     : chr \"E05014249\"\n  .. .. ..$ parish                         : chr \"E04002661\"\n  .. .. ..$ parliamentary_constituency     : chr \"E14001580\"\n  .. .. ..$ parliamentary_constituency_2024: chr \"E14001580\"\n  .. .. ..$ ccg                            : chr \"E38000228\"\n  .. .. ..$ ccg_id                         : chr \"01K\"\n  .. .. ..$ ced                            : chr \"E99999999\"\n  .. .. ..$ nuts                           : chr \"TLD14\"\n  .. .. ..$ lsoa                           : chr \"E01019395\"\n  .. .. ..$ msoa                           : chr \"E02004015\"\n  .. .. ..$ lau2                           : chr \"E06000064\"\n  .. .. ..$ pfa                            : chr \"E23000002\"\n  .. .. ..$ nhs_region                     : chr \"E40000014\"\n  .. .. ..$ ttwa                           : chr \"E30000223\"\n  .. .. ..$ national_park                  : chr \"E26000011\"\n  .. .. ..$ bua                            : chr \"E63007482\"\n  .. .. ..$ icb                            : chr \"E54000048\"\n  .. .. ..$ cancer_alliance                : chr \"E56000018\"\n  .. .. ..$ lsoa11                         : chr \"E01019395\"\n  .. .. ..$ msoa11                         : chr \"E02004015\"\n  .. .. ..$ lsoa21                         : chr \"E01019395\"\n  .. .. ..$ msoa21                         : chr \"E02004015\"\n  .. .. ..$ oa21                           : chr \"E00098085\"\n  .. .. ..$ ruc11                          : chr \"D2\"\n  .. .. ..$ ruc21                          : chr \"RLF1\"\n  .. .. ..$ lep1                           : chr \"E37000007\"\n  .. .. ..$ lep2                           : NULL\n\n#postcode look up\nbulk_list &lt;- lapply(bulk_lookup_result, \"[[\", 2)\n\nTT_foodbank_location_points2 &lt;-\n  map_dfr(bulk_list,\n          `[`,\n          c(\"postcode\", \"longitude\", \"latitude\"))\n\n#as spatial points\nTT_foodbank_location_points2 &lt;- \n  st_as_sf(TT_foodbank_location_points2,\n           coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\nTT_foodbanks3 &lt;- TT_foodbanks[201:300, ]\n\n#make postcodes into a list\nlist &lt;- as.list(unique(TT_foodbanks3$postcode))\n\n#foodbank locations\n#lat and long from postcodes\npc_list &lt;- list(postcodes = list)\nbulk_lookup_result &lt;- bulk_postcode_lookup(pc_list)\nstr(bulk_lookup_result[1])\n\nList of 1\n $ :List of 2\n  ..$ query : chr \"CB259HR\"\n  ..$ result:List of 42\n  .. ..$ postcode                       : chr \"CB25 9HR\"\n  .. ..$ quality                        : int 1\n  .. ..$ eastings                       : int 549616\n  .. ..$ northings                      : int 265265\n  .. ..$ country                        : chr \"England\"\n  .. ..$ nhs_ha                         : chr \"East of England\"\n  .. ..$ longitude                      : num 0.191\n  .. ..$ latitude                       : num 52.3\n  .. ..$ european_electoral_region      : chr \"Eastern\"\n  .. ..$ primary_care_trust             : chr \"Cambridgeshire\"\n  .. ..$ region                         : chr \"East of England\"\n  .. ..$ lsoa                           : chr \"South Cambridgeshire 004C\"\n  .. ..$ msoa                           : chr \"South Cambridgeshire 004\"\n  .. ..$ incode                         : chr \"9HR\"\n  .. ..$ outcode                        : chr \"CB25\"\n  .. ..$ parliamentary_constituency     : chr \"Ely and East Cambridgeshire\"\n  .. ..$ parliamentary_constituency_2024: chr \"Ely and East Cambridgeshire\"\n  .. ..$ admin_district                 : chr \"South Cambridgeshire\"\n  .. ..$ parish                         : chr \"Waterbeach\"\n  .. ..$ admin_county                   : chr \"Cambridgeshire\"\n  .. ..$ date_of_introduction           : chr \"200609\"\n  .. ..$ admin_ward                     : chr \"Milton & Waterbeach\"\n  .. ..$ ced                            : chr \"Waterbeach\"\n  .. ..$ ccg                            : chr \"NHS Cambridgeshire and Peterborough\"\n  .. ..$ nuts                           : chr \"South Cambridgeshire\"\n  .. ..$ pfa                            : chr \"Cambridgeshire\"\n  .. ..$ nhs_region                     : chr \"East of England\"\n  .. ..$ ttwa                           : chr \"Cambridge\"\n  .. ..$ national_park                  : chr \"England (non-National Park)\"\n  .. ..$ bua                            : chr \"Waterbeach\"\n  .. ..$ icb                            : chr \"NHS Cambridgeshire and Peterborough Integrated Care Board\"\n  .. ..$ cancer_alliance                : chr \"East of England\"\n  .. ..$ lsoa11                         : chr \"South Cambridgeshire 004C\"\n  .. ..$ msoa11                         : chr \"South Cambridgeshire 004\"\n  .. ..$ lsoa21                         : chr \"South Cambridgeshire 004C\"\n  .. ..$ msoa21                         : chr \"South Cambridgeshire 004\"\n  .. ..$ oa21                           : chr \"E00092301\"\n  .. ..$ ruc11                          : chr \"(England/Wales) Rural town and fringe\"\n  .. ..$ ruc21                          : chr \"Larger rural: Nearer to a major town or city\"\n  .. ..$ lep1                           : chr \"Greater Cambridge and Greater Peterborough\"\n  .. ..$ lep2                           : NULL\n  .. ..$ codes                          :List of 29\n  .. .. ..$ admin_district                 : chr \"E07000012\"\n  .. .. ..$ admin_county                   : chr \"E10000003\"\n  .. .. ..$ admin_ward                     : chr \"E05011301\"\n  .. .. ..$ parish                         : chr \"E04001848\"\n  .. .. ..$ parliamentary_constituency     : chr \"E14001224\"\n  .. .. ..$ parliamentary_constituency_2024: chr \"E14001224\"\n  .. .. ..$ ccg                            : chr \"E38000260\"\n  .. .. ..$ ccg_id                         : chr \"06H\"\n  .. .. ..$ ced                            : chr \"E58000102\"\n  .. .. ..$ nuts                           : chr \"TLH42\"\n  .. .. ..$ lsoa                           : chr \"E01018300\"\n  .. .. ..$ msoa                           : chr \"E02003778\"\n  .. .. ..$ lau2                           : chr \"E07000012\"\n  .. .. ..$ pfa                            : chr \"E23000023\"\n  .. .. ..$ nhs_region                     : chr \"E40000013\"\n  .. .. ..$ ttwa                           : chr \"E30000186\"\n  .. .. ..$ national_park                  : chr \"E65000001\"\n  .. .. ..$ bua                            : chr \"E63010385\"\n  .. .. ..$ icb                            : chr \"E54000056\"\n  .. .. ..$ cancer_alliance                : chr \"E56000035\"\n  .. .. ..$ lsoa11                         : chr \"E01018300\"\n  .. .. ..$ msoa11                         : chr \"E02003778\"\n  .. .. ..$ lsoa21                         : chr \"E01018300\"\n  .. .. ..$ msoa21                         : chr \"E02003778\"\n  .. .. ..$ oa21                           : chr \"E00092301\"\n  .. .. ..$ ruc11                          : chr \"D1\"\n  .. .. ..$ ruc21                          : chr \"RLN1\"\n  .. .. ..$ lep1                           : chr \"E37000059\"\n  .. .. ..$ lep2                           : NULL\n\n#postcode look up\nbulk_list &lt;- lapply(bulk_lookup_result, \"[[\", 2)\n\nTT_foodbank_location_points3 &lt;-\n  map_dfr(bulk_list,\n          `[`,\n          c(\"postcode\", \"longitude\", \"latitude\"))\n\n#as spatial points\nTT_foodbank_location_points3 &lt;- \n  st_as_sf(TT_foodbank_location_points3,\n           coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\nTT_foodbanks4 &lt;- TT_foodbanks[301:400, ]\n\n#make postcodes into a list\nlist &lt;- as.list(unique(TT_foodbanks4$postcode))\n\n#foodbank locations\n#lat and long from postcodes\npc_list &lt;- list(postcodes = list)\nbulk_lookup_result &lt;- bulk_postcode_lookup(pc_list)\nstr(bulk_lookup_result[1])\n\nList of 1\n $ :List of 2\n  ..$ query : chr \"IV191AL\"\n  ..$ result:List of 42\n  .. ..$ postcode                       : chr \"IV19 1AL\"\n  .. ..$ quality                        : int 1\n  .. ..$ eastings                       : int 277951\n  .. ..$ northings                      : int 882070\n  .. ..$ country                        : chr \"Scotland\"\n  .. ..$ nhs_ha                         : chr \"Highland\"\n  .. ..$ longitude                      : num -4.06\n  .. ..$ latitude                       : num 57.8\n  .. ..$ european_electoral_region      : chr \"Scotland\"\n  .. ..$ primary_care_trust             : chr \"Highland Health and Social Care Partnership\"\n  .. ..$ region                         : NULL\n  .. ..$ lsoa                           : chr \"Tain - 04\"\n  .. ..$ msoa                           : chr \"Tain\"\n  .. ..$ incode                         : chr \"1AL\"\n  .. ..$ outcode                        : chr \"IV19\"\n  .. ..$ parliamentary_constituency     : chr \"Caithness, Sutherland and Easter Ross\"\n  .. ..$ parliamentary_constituency_2024: chr \"Caithness, Sutherland and Easter Ross\"\n  .. ..$ admin_district                 : chr \"Highland\"\n  .. ..$ parish                         : NULL\n  .. ..$ admin_county                   : NULL\n  .. ..$ date_of_introduction           : chr \"198001\"\n  .. ..$ admin_ward                     : chr \"Tain and Easter Ross\"\n  .. ..$ ced                            : NULL\n  .. ..$ ccg                            : chr \"Highland Health and Social Care Partnership\"\n  .. ..$ nuts                           : chr \"Highland\"\n  .. ..$ pfa                            : chr \"Scotland\"\n  .. ..$ nhs_region                     : NULL\n  .. ..$ ttwa                           : chr \"Alness and Invergordon\"\n  .. ..$ national_park                  : NULL\n  .. ..$ bua                            : NULL\n  .. ..$ icb                            : NULL\n  .. ..$ cancer_alliance                : NULL\n  .. ..$ lsoa11                         : chr \"Tain - 04\"\n  .. ..$ msoa11                         : chr \"Tain\"\n  .. ..$ lsoa21                         : chr \"Tain - 04\"\n  .. ..$ msoa21                         : chr \"Tain\"\n  .. ..$ oa21                           : chr \"S00162432\"\n  .. ..$ ruc11                          : chr \"(Scotland) Very Remote Small Town\"\n  .. ..$ ruc21                          : chr \"Remote Small Towns\"\n  .. ..$ lep1                           : NULL\n  .. ..$ lep2                           : NULL\n  .. ..$ codes                          :List of 29\n  .. .. ..$ admin_district                 : chr \"S12000017\"\n  .. .. ..$ admin_county                   : chr \"S99999999\"\n  .. .. ..$ admin_ward                     : chr \"S13002996\"\n  .. .. ..$ parish                         : chr \"S99999999\"\n  .. .. ..$ parliamentary_constituency     : chr \"S14000069\"\n  .. .. ..$ parliamentary_constituency_2024: chr \"S14000069\"\n  .. .. ..$ ccg                            : chr \"S03000044\"\n  .. .. ..$ ccg_id                         : chr \"044\"\n  .. .. ..$ ced                            : chr \"S99999999\"\n  .. .. ..$ nuts                           : chr \"TLM20\"\n  .. .. ..$ lsoa                           : chr \"S01017931\"\n  .. .. ..$ msoa                           : chr \"S02003326\"\n  .. .. ..$ lau2                           : chr \"S30000055\"\n  .. .. ..$ pfa                            : chr \"S23000009\"\n  .. .. ..$ nhs_region                     : chr \"S99999999\"\n  .. .. ..$ ttwa                           : chr \"S22000048\"\n  .. .. ..$ national_park                  : chr \"S99999999\"\n  .. .. ..$ bua                            : chr \"S99999999\"\n  .. .. ..$ icb                            : chr \"S99999999\"\n  .. .. ..$ cancer_alliance                : chr \"S99999999\"\n  .. .. ..$ lsoa11                         : chr \"S01010755\"\n  .. .. ..$ msoa11                         : chr \"S02002023\"\n  .. .. ..$ lsoa21                         : chr \"S01017931\"\n  .. .. ..$ msoa21                         : chr \"S02003326\"\n  .. .. ..$ oa21                           : chr \"S00162432\"\n  .. .. ..$ ruc11                          : chr \"5\"\n  .. .. ..$ ruc21                          : chr \"4\"\n  .. .. ..$ lep1                           : chr \"S99999999\"\n  .. .. ..$ lep2                           : NULL\n\n#postcode look up\nbulk_list &lt;- lapply(bulk_lookup_result, \"[[\", 2)\n\nTT_foodbank_location_points4 &lt;-\n  map_dfr(bulk_list,\n          `[`,\n          c(\"postcode\", \"longitude\", \"latitude\"))\n\n#as spatial points\nTT_foodbank_location_points4 &lt;- \n  st_as_sf(TT_foodbank_location_points4,\n           coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\nTT_foodbanks5 &lt;- TT_foodbanks[401:500, ]\n\n#make postcodes into a list\nlist &lt;- as.list(unique(TT_foodbanks5$postcode))\n\n#foodbank locations\n#lat and long from postcodes\npc_list &lt;- list(postcodes = list)\nbulk_lookup_result &lt;- bulk_postcode_lookup(pc_list)\nstr(bulk_lookup_result[1])\n\nList of 1\n $ :List of 2\n  ..$ query : chr \"LD39EA\"\n  ..$ result:List of 42\n  .. ..$ postcode                       : chr \"LD3 9EA\"\n  .. ..$ quality                        : int 1\n  .. ..$ eastings                       : int 304317\n  .. ..$ northings                      : int 229234\n  .. ..$ country                        : chr \"Wales\"\n  .. ..$ nhs_ha                         : chr \"Powys Teaching Health Board\"\n  .. ..$ longitude                      : num -3.39\n  .. ..$ latitude                       : num 52\n  .. ..$ european_electoral_region      : chr \"Wales\"\n  .. ..$ primary_care_trust             : chr \"Powys Teaching Health Board\"\n  .. ..$ region                         : NULL\n  .. ..$ lsoa                           : chr \"Powys 017F\"\n  .. ..$ msoa                           : chr \"Powys 017\"\n  .. ..$ incode                         : chr \"9EA\"\n  .. ..$ outcode                        : chr \"LD3\"\n  .. ..$ parliamentary_constituency     : chr \"Brecon, Radnor and Cwm Tawe\"\n  .. ..$ parliamentary_constituency_2024: chr \"Brecon, Radnor and Cwm Tawe\"\n  .. ..$ admin_district                 : chr \"Powys\"\n  .. ..$ parish                         : chr \"Brecon\"\n  .. ..$ admin_county                   : NULL\n  .. ..$ date_of_introduction           : chr \"198001\"\n  .. ..$ admin_ward                     : chr \"Brecon West\"\n  .. ..$ ced                            : NULL\n  .. ..$ ccg                            : chr \"Powys Teaching\"\n  .. ..$ nuts                           : chr \"Powys\"\n  .. ..$ pfa                            : chr \"Dyfed-Powys\"\n  .. ..$ nhs_region                     : NULL\n  .. ..$ ttwa                           : chr \"Brecon\"\n  .. ..$ national_park                  : chr \"Brecon Beacons\"\n  .. ..$ bua                            : chr \"Brecon\"\n  .. ..$ icb                            : NULL\n  .. ..$ cancer_alliance                : NULL\n  .. ..$ lsoa11                         : chr \"Powys 017F\"\n  .. ..$ msoa11                         : chr \"Powys 017\"\n  .. ..$ lsoa21                         : chr \"Powys 017F\"\n  .. ..$ msoa21                         : chr \"Powys 017\"\n  .. ..$ oa21                           : chr \"W00002616\"\n  .. ..$ ruc11                          : chr \"(England/Wales) Rural town and fringe in a sparse setting\"\n  .. ..$ ruc21                          : chr \"Larger rural: Further from a major town or city\"\n  .. ..$ lep1                           : NULL\n  .. ..$ lep2                           : NULL\n  .. ..$ codes                          :List of 29\n  .. .. ..$ admin_district                 : chr \"W06000023\"\n  .. .. ..$ admin_county                   : chr \"W99999999\"\n  .. .. ..$ admin_ward                     : chr \"W05001121\"\n  .. .. ..$ parish                         : chr \"W04000255\"\n  .. .. ..$ parliamentary_constituency     : chr \"W07000085\"\n  .. .. ..$ parliamentary_constituency_2024: chr \"W07000085\"\n  .. .. ..$ ccg                            : chr \"W11000024\"\n  .. .. ..$ ccg_id                         : chr \"7A7\"\n  .. .. ..$ ced                            : chr \"W99999999\"\n  .. .. ..$ nuts                           : chr \"TLL41\"\n  .. .. ..$ lsoa                           : chr \"W01001901\"\n  .. .. ..$ msoa                           : chr \"W02000113\"\n  .. .. ..$ lau2                           : chr \"W06000023\"\n  .. .. ..$ pfa                            : chr \"W15000004\"\n  .. .. ..$ nhs_region                     : chr \"W99999999\"\n  .. .. ..$ ttwa                           : chr \"W22000023\"\n  .. .. ..$ national_park                  : chr \"W18000001\"\n  .. .. ..$ bua                            : chr \"W45000918\"\n  .. .. ..$ icb                            : chr \"W99999999\"\n  .. .. ..$ cancer_alliance                : chr \"W99999999\"\n  .. .. ..$ lsoa11                         : chr \"W01001901\"\n  .. .. ..$ msoa11                         : chr \"W02000113\"\n  .. .. ..$ lsoa21                         : chr \"W01001901\"\n  .. .. ..$ msoa21                         : chr \"W02000113\"\n  .. .. ..$ oa21                           : chr \"W00002616\"\n  .. .. ..$ ruc11                          : chr \"D2\"\n  .. .. ..$ ruc21                          : chr \"RLF1\"\n  .. .. ..$ lep1                           : chr \"W99999999\"\n  .. .. ..$ lep2                           : NULL\n\n#postcode look up\nbulk_list &lt;- lapply(bulk_lookup_result, \"[[\", 2)\n\nTT_foodbank_location_points5 &lt;-\n  map_dfr(bulk_list,\n          `[`,\n          c(\"postcode\", \"longitude\", \"latitude\"))\n\n#as spatial points\nTT_foodbank_location_points5 &lt;- \n  st_as_sf(TT_foodbank_location_points5,\n           coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\nTT_foodbanks6 &lt;- TT_foodbanks[501:600, ]\n\n#make postcodes into a list\nlist &lt;- as.list(unique(TT_foodbanks1$postcode))\n\n#foodbank locations\n#lat and long from postcodes\npc_list &lt;- list(postcodes = list)\nbulk_lookup_result &lt;- bulk_postcode_lookup(pc_list)\nstr(bulk_lookup_result[1])\n\nList of 1\n $ :List of 2\n  ..$ query : chr \"KT190JG\"\n  ..$ result:List of 42\n  .. ..$ postcode                       : chr \"KT19 0JG\"\n  .. ..$ quality                        : int 1\n  .. ..$ eastings                       : int 520624\n  .. ..$ northings                      : int 164193\n  .. ..$ country                        : chr \"England\"\n  .. ..$ nhs_ha                         : chr \"South East Coast\"\n  .. ..$ longitude                      : num -0.269\n  .. ..$ latitude                       : num 51.4\n  .. ..$ european_electoral_region      : chr \"South East\"\n  .. ..$ primary_care_trust             : chr \"Surrey\"\n  .. ..$ region                         : chr \"South East\"\n  .. ..$ lsoa                           : chr \"Epsom and Ewell 002A\"\n  .. ..$ msoa                           : chr \"Epsom and Ewell 002\"\n  .. ..$ incode                         : chr \"0JG\"\n  .. ..$ outcode                        : chr \"KT19\"\n  .. ..$ parliamentary_constituency     : chr \"Epsom and Ewell\"\n  .. ..$ parliamentary_constituency_2024: chr \"Epsom and Ewell\"\n  .. ..$ admin_district                 : chr \"Epsom and Ewell\"\n  .. ..$ parish                         : chr \"Epsom and Ewell, unparished area\"\n  .. ..$ admin_county                   : chr \"Surrey\"\n  .. ..$ date_of_introduction           : chr \"198001\"\n  .. ..$ admin_ward                     : chr \"Ewell Court\"\n  .. ..$ ced                            : chr \"Ewell Court, Auriol and Cuddington\"\n  .. ..$ ccg                            : chr \"NHS Surrey Heartlands\"\n  .. ..$ nuts                           : chr \"Epsom and Ewell\"\n  .. ..$ pfa                            : chr \"Surrey\"\n  .. ..$ nhs_region                     : chr \"South East\"\n  .. ..$ ttwa                           : chr \"London\"\n  .. ..$ national_park                  : chr \"England (non-National Park)\"\n  .. ..$ bua                            : chr \"Ewell\"\n  .. ..$ icb                            : chr \"NHS Surrey Heartlands Integrated Care Board\"\n  .. ..$ cancer_alliance                : chr \"Surrey and Sussex\"\n  .. ..$ lsoa11                         : chr \"Epsom and Ewell 002A\"\n  .. ..$ msoa11                         : chr \"Epsom and Ewell 002\"\n  .. ..$ lsoa21                         : chr \"Epsom and Ewell 002A\"\n  .. ..$ msoa21                         : chr \"Epsom and Ewell 002\"\n  .. ..$ oa21                           : chr \"E00155030\"\n  .. ..$ ruc11                          : chr \"(England/Wales) Urban major conurbation\"\n  .. ..$ ruc21                          : chr \"Urban: Nearer to a major town or city\"\n  .. ..$ lep1                           : chr \"Coast to Capital\"\n  .. ..$ lep2                           : NULL\n  .. ..$ codes                          :List of 29\n  .. .. ..$ admin_district                 : chr \"E07000208\"\n  .. .. ..$ admin_county                   : chr \"E10000030\"\n  .. .. ..$ admin_ward                     : chr \"E05015097\"\n  .. .. ..$ parish                         : chr \"E43000137\"\n  .. .. ..$ parliamentary_constituency     : chr \"E14001227\"\n  .. .. ..$ parliamentary_constituency_2024: chr \"E14001227\"\n  .. .. ..$ ccg                            : chr \"E38000264\"\n  .. .. ..$ ccg_id                         : chr \"92A\"\n  .. .. ..$ ced                            : chr \"E58001483\"\n  .. .. ..$ nuts                           : chr \"TLJ26\"\n  .. .. ..$ lsoa                           : chr \"E01030394\"\n  .. .. ..$ msoa                           : chr \"E02006336\"\n  .. .. ..$ lau2                           : chr \"E07000208\"\n  .. .. ..$ pfa                            : chr \"E23000031\"\n  .. .. ..$ nhs_region                     : chr \"E40000005\"\n  .. .. ..$ ttwa                           : chr \"E30000234\"\n  .. .. ..$ national_park                  : chr \"E65000001\"\n  .. .. ..$ bua                            : chr \"E63012408\"\n  .. .. ..$ icb                            : chr \"E54000063\"\n  .. .. ..$ cancer_alliance                : chr \"E56000012\"\n  .. .. ..$ lsoa11                         : chr \"E01030394\"\n  .. .. ..$ msoa11                         : chr \"E02006336\"\n  .. .. ..$ lsoa21                         : chr \"E01030394\"\n  .. .. ..$ msoa21                         : chr \"E02006336\"\n  .. .. ..$ oa21                           : chr \"E00155030\"\n  .. .. ..$ ruc11                          : chr \"A1\"\n  .. .. ..$ ruc21                          : chr \"UN1\"\n  .. .. ..$ lep1                           : chr \"E37000044\"\n  .. .. ..$ lep2                           : NULL\n\n#postcode look up\nbulk_list &lt;- lapply(bulk_lookup_result, \"[[\", 2)\n\nTT_foodbank_location_points6 &lt;-\n  map_dfr(bulk_list,\n          `[`,\n          c(\"postcode\", \"longitude\", \"latitude\"))\n\n#as spatial points\nTT_foodbank_location_points6 &lt;- \n  st_as_sf(TT_foodbank_location_points6,\n           coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\nTT_foodbanks7 &lt;- TT_foodbanks[601:700, ]\n\n#make postcodes into a list\nlist &lt;- as.list(unique(TT_foodbanks7$postcode))\n\n#foodbank locations\n#lat and long from postcodes\npc_list &lt;- list(postcodes = list)\nbulk_lookup_result &lt;- bulk_postcode_lookup(pc_list)\nstr(bulk_lookup_result[1])\n\nList of 1\n $ :List of 2\n  ..$ query : chr \"CW109AR\"\n  ..$ result:List of 42\n  .. ..$ postcode                       : chr \"CW10 9AR\"\n  .. ..$ quality                        : int 1\n  .. ..$ eastings                       : int 370328\n  .. ..$ northings                      : int 366143\n  .. ..$ country                        : chr \"England\"\n  .. ..$ nhs_ha                         : chr \"North West\"\n  .. ..$ longitude                      : num -2.45\n  .. ..$ latitude                       : num 53.2\n  .. ..$ european_electoral_region      : chr \"North West\"\n  .. ..$ primary_care_trust             : chr \"Central and Eastern Cheshire\"\n  .. ..$ region                         : chr \"North West\"\n  .. ..$ lsoa                           : chr \"Cheshire East 024C\"\n  .. ..$ msoa                           : chr \"Cheshire East 024\"\n  .. ..$ incode                         : chr \"9AR\"\n  .. ..$ outcode                        : chr \"CW10\"\n  .. ..$ parliamentary_constituency     : chr \"Mid Cheshire\"\n  .. ..$ parliamentary_constituency_2024: chr \"Mid Cheshire\"\n  .. ..$ admin_district                 : chr \"Cheshire East\"\n  .. ..$ parish                         : chr \"Middlewich\"\n  .. ..$ admin_county                   : NULL\n  .. ..$ date_of_introduction           : chr \"198001\"\n  .. ..$ admin_ward                     : chr \"Middlewich\"\n  .. ..$ ced                            : NULL\n  .. ..$ ccg                            : chr \"NHS Cheshire and Merseyside\"\n  .. ..$ nuts                           : chr \"Cheshire East\"\n  .. ..$ pfa                            : chr \"Cheshire\"\n  .. ..$ nhs_region                     : chr \"North West\"\n  .. ..$ ttwa                           : chr \"Crewe\"\n  .. ..$ national_park                  : chr \"England (non-National Park)\"\n  .. ..$ bua                            : chr \"Middlewich\"\n  .. ..$ icb                            : chr \"NHS Cheshire and Merseyside Integrated Care Board\"\n  .. ..$ cancer_alliance                : chr \"Cheshire and Merseyside\"\n  .. ..$ lsoa11                         : chr \"Cheshire East 024C\"\n  .. ..$ msoa11                         : chr \"Cheshire East 024\"\n  .. ..$ lsoa21                         : chr \"Cheshire East 024C\"\n  .. ..$ msoa21                         : chr \"Cheshire East 024\"\n  .. ..$ oa21                           : chr \"E00092976\"\n  .. ..$ ruc11                          : chr \"(England/Wales) Urban city and town\"\n  .. ..$ ruc21                          : chr \"Urban: Nearer to a major town or city\"\n  .. ..$ lep1                           : chr \"Cheshire and Warrington\"\n  .. ..$ lep2                           : NULL\n  .. ..$ codes                          :List of 29\n  .. .. ..$ admin_district                 : chr \"E06000049\"\n  .. .. ..$ admin_county                   : chr \"E99999999\"\n  .. .. ..$ admin_ward                     : chr \"E05008640\"\n  .. .. ..$ parish                         : chr \"E04013194\"\n  .. .. ..$ parliamentary_constituency     : chr \"E14001361\"\n  .. .. ..$ parliamentary_constituency_2024: chr \"E14001361\"\n  .. .. ..$ ccg                            : chr \"E38000233\"\n  .. .. ..$ ccg_id                         : chr \"27D\"\n  .. .. ..$ ced                            : chr \"E99999999\"\n  .. .. ..$ nuts                           : chr \"TLD62\"\n  .. .. ..$ lsoa                           : chr \"E01018426\"\n  .. .. ..$ msoa                           : chr \"E02003813\"\n  .. .. ..$ lau2                           : chr \"E06000049\"\n  .. .. ..$ pfa                            : chr \"E23000006\"\n  .. .. ..$ nhs_region                     : chr \"E40000014\"\n  .. .. ..$ ttwa                           : chr \"E30000197\"\n  .. .. ..$ national_park                  : chr \"E65000001\"\n  .. .. ..$ bua                            : chr \"E63008785\"\n  .. .. ..$ icb                            : chr \"E54000008\"\n  .. .. ..$ cancer_alliance                : chr \"E56000005\"\n  .. .. ..$ lsoa11                         : chr \"E01018426\"\n  .. .. ..$ msoa11                         : chr \"E02003813\"\n  .. .. ..$ lsoa21                         : chr \"E01018426\"\n  .. .. ..$ msoa21                         : chr \"E02003813\"\n  .. .. ..$ oa21                           : chr \"E00092976\"\n  .. .. ..$ ruc11                          : chr \"C1\"\n  .. .. ..$ ruc21                          : chr \"UN1\"\n  .. .. ..$ lep1                           : chr \"E37000003\"\n  .. .. ..$ lep2                           : NULL\n\n#postcode look up\nbulk_list &lt;- lapply(bulk_lookup_result, \"[[\", 2)\n\nTT_foodbank_location_points7 &lt;-\n  map_dfr(bulk_list,\n          `[`,\n          c(\"postcode\", \"longitude\", \"latitude\"))\n\n#as spatial points\nTT_foodbank_location_points7 &lt;- \n  st_as_sf(TT_foodbank_location_points7,\n           coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\nTT_foodbanks8 &lt;- TT_foodbanks[701:800, ]\n\n#make postcodes into a list\nlist &lt;- as.list(unique(TT_foodbanks8$postcode))\n\n#foodbank locations\n#lat and long from postcodes\npc_list &lt;- list(postcodes = list)\nbulk_lookup_result &lt;- bulk_postcode_lookup(pc_list)\nstr(bulk_lookup_result[1])\n\nList of 1\n $ :List of 2\n  ..$ query : chr \"WR101DT\"\n  ..$ result:List of 42\n  .. ..$ postcode                       : chr \"WR10 1DT\"\n  .. ..$ quality                        : int 1\n  .. ..$ eastings                       : int 394862\n  .. ..$ northings                      : int 245902\n  .. ..$ country                        : chr \"England\"\n  .. ..$ nhs_ha                         : chr \"West Midlands\"\n  .. ..$ longitude                      : num -2.08\n  .. ..$ latitude                       : num 52.1\n  .. ..$ european_electoral_region      : chr \"West Midlands\"\n  .. ..$ primary_care_trust             : chr \"Worcestershire\"\n  .. ..$ region                         : chr \"West Midlands\"\n  .. ..$ lsoa                           : chr \"Wychavon 012B\"\n  .. ..$ msoa                           : chr \"Wychavon 012\"\n  .. ..$ incode                         : chr \"1DT\"\n  .. ..$ outcode                        : chr \"WR10\"\n  .. ..$ parliamentary_constituency     : chr \"West Worcestershire\"\n  .. ..$ parliamentary_constituency_2024: chr \"West Worcestershire\"\n  .. ..$ admin_district                 : chr \"Wychavon\"\n  .. ..$ parish                         : chr \"Pershore\"\n  .. ..$ admin_county                   : chr \"Worcestershire\"\n  .. ..$ date_of_introduction           : chr \"198001\"\n  .. ..$ admin_ward                     : chr \"Pershore\"\n  .. ..$ ced                            : chr \"Pershore\"\n  .. ..$ ccg                            : chr \"NHS Herefordshire and Worcestershire\"\n  .. ..$ nuts                           : chr \"Wychavon\"\n  .. ..$ pfa                            : chr \"West Mercia\"\n  .. ..$ nhs_region                     : chr \"Midlands\"\n  .. ..$ ttwa                           : chr \"Evesham\"\n  .. ..$ national_park                  : chr \"England (non-National Park)\"\n  .. ..$ bua                            : chr \"Pershore\"\n  .. ..$ icb                            : chr \"NHS Herefordshire and Worcestershire Integrated Care Board\"\n  .. ..$ cancer_alliance                : chr \"West Midlands\"\n  .. ..$ lsoa11                         : chr \"Wychavon 012B\"\n  .. ..$ msoa11                         : chr \"Wychavon 012\"\n  .. ..$ lsoa21                         : chr \"Wychavon 012B\"\n  .. ..$ msoa21                         : chr \"Wychavon 012\"\n  .. ..$ oa21                           : chr \"E00165283\"\n  .. ..$ ruc11                          : chr \"(England/Wales) Rural town and fringe\"\n  .. ..$ ruc21                          : chr \"Larger rural: Nearer to a major town or city\"\n  .. ..$ lep1                           : chr \"Worcestershire\"\n  .. ..$ lep2                           : NULL\n  .. ..$ codes                          :List of 29\n  .. .. ..$ admin_district                 : chr \"E07000238\"\n  .. .. ..$ admin_county                   : chr \"E10000034\"\n  .. .. ..$ admin_ward                     : chr \"E05015463\"\n  .. .. ..$ parish                         : chr \"E04010425\"\n  .. .. ..$ parliamentary_constituency     : chr \"E14001579\"\n  .. .. ..$ parliamentary_constituency_2024: chr \"E14001579\"\n  .. .. ..$ ccg                            : chr \"E38000236\"\n  .. .. ..$ ccg_id                         : chr \"18C\"\n  .. .. ..$ ced                            : chr \"E58001951\"\n  .. .. ..$ nuts                           : chr \"TLG12\"\n  .. .. ..$ lsoa                           : chr \"E01032406\"\n  .. .. ..$ msoa                           : chr \"E02006759\"\n  .. .. ..$ lau2                           : chr \"E07000238\"\n  .. .. ..$ pfa                            : chr \"E23000016\"\n  .. .. ..$ nhs_region                     : chr \"E40000011\"\n  .. .. ..$ ttwa                           : chr \"E30000205\"\n  .. .. ..$ national_park                  : chr \"E65000001\"\n  .. .. ..$ bua                            : chr \"E63010804\"\n  .. .. ..$ icb                            : chr \"E54000019\"\n  .. .. ..$ cancer_alliance                : chr \"E56000007\"\n  .. .. ..$ lsoa11                         : chr \"E01032406\"\n  .. .. ..$ msoa11                         : chr \"E02006759\"\n  .. .. ..$ lsoa21                         : chr \"E01032406\"\n  .. .. ..$ msoa21                         : chr \"E02006759\"\n  .. .. ..$ oa21                           : chr \"E00165283\"\n  .. .. ..$ ruc11                          : chr \"D1\"\n  .. .. ..$ ruc21                          : chr \"RLN1\"\n  .. .. ..$ lep1                           : chr \"E37000038\"\n  .. .. ..$ lep2                           : NULL\n\n#postcode look up\nbulk_list &lt;- lapply(bulk_lookup_result, \"[[\", 2)\n\nTT_foodbank_location_points8 &lt;-\n  map_dfr(bulk_list,\n          `[`,\n          c(\"postcode\", \"longitude\", \"latitude\"))\n\n#as spatial points\nTT_foodbank_location_points8 &lt;- \n  st_as_sf(TT_foodbank_location_points8,\n           coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\nTT_foodbanks9 &lt;- TT_foodbanks[801:900, ]\n\n#make postcodes into a list\nlist &lt;- as.list(unique(TT_foodbanks9$postcode))\n\n#foodbank locations\n#lat and long from postcodes\npc_list &lt;- list(postcodes = list)\nbulk_lookup_result &lt;- bulk_postcode_lookup(pc_list)\nstr(bulk_lookup_result[1])\n\nList of 1\n $ :List of 2\n  ..$ query : chr \"G211UX\"\n  ..$ result:List of 42\n  .. ..$ postcode                       : chr \"G21 1UX\"\n  .. ..$ quality                        : int 1\n  .. ..$ eastings                       : int 260591\n  .. ..$ northings                      : int 669115\n  .. ..$ country                        : chr \"Scotland\"\n  .. ..$ nhs_ha                         : chr \"Greater Glasgow and Clyde\"\n  .. ..$ longitude                      : num -4.23\n  .. ..$ latitude                       : num 55.9\n  .. ..$ european_electoral_region      : chr \"Scotland\"\n  .. ..$ primary_care_trust             : chr \"Glasgow City Community Health Partnership\"\n  .. ..$ region                         : NULL\n  .. ..$ lsoa                           : chr \"Springburn East and Cowlairs - 02\"\n  .. ..$ msoa                           : chr \"Springburn East and Cowlairs\"\n  .. ..$ incode                         : chr \"1UX\"\n  .. ..$ outcode                        : chr \"G21\"\n  .. ..$ parliamentary_constituency     : chr \"Glasgow North East\"\n  .. ..$ parliamentary_constituency_2024: chr \"Glasgow North East\"\n  .. ..$ admin_district                 : chr \"Glasgow City\"\n  .. ..$ parish                         : NULL\n  .. ..$ admin_county                   : NULL\n  .. ..$ date_of_introduction           : chr \"198001\"\n  .. ..$ admin_ward                     : chr \"Springburn/Robroyston\"\n  .. ..$ ced                            : NULL\n  .. ..$ ccg                            : chr \"Glasgow City Community Health & Care Partnership\"\n  .. ..$ nuts                           : chr \"Glasgow City\"\n  .. ..$ pfa                            : chr \"Scotland\"\n  .. ..$ nhs_region                     : NULL\n  .. ..$ ttwa                           : chr \"Glasgow\"\n  .. ..$ national_park                  : NULL\n  .. ..$ bua                            : NULL\n  .. ..$ icb                            : NULL\n  .. ..$ cancer_alliance                : NULL\n  .. ..$ lsoa11                         : chr \"Springburn East and Cowlairs - 02\"\n  .. ..$ msoa11                         : chr \"Springburn East and Cowlairs\"\n  .. ..$ lsoa21                         : chr \"Springburn East and Cowlairs - 02\"\n  .. ..$ msoa21                         : chr \"Springburn East and Cowlairs\"\n  .. ..$ oa21                           : chr \"S00157264\"\n  .. ..$ ruc11                          : chr \"(Scotland) Large Urban Area\"\n  .. ..$ ruc21                          : chr \"Large Urban Areas\"\n  .. ..$ lep1                           : NULL\n  .. ..$ lep2                           : NULL\n  .. ..$ codes                          :List of 29\n  .. .. ..$ admin_district                 : chr \"S12000049\"\n  .. .. ..$ admin_county                   : chr \"S99999999\"\n  .. .. ..$ admin_ward                     : chr \"S13002983\"\n  .. .. ..$ parish                         : chr \"S99999999\"\n  .. .. ..$ parliamentary_constituency     : chr \"S14000086\"\n  .. .. ..$ parliamentary_constituency_2024: chr \"S14000086\"\n  .. .. ..$ ccg                            : chr \"S03000043\"\n  .. .. ..$ ccg_id                         : chr \"043\"\n  .. .. ..$ ced                            : chr \"S99999999\"\n  .. .. ..$ nuts                           : chr \"TLM32\"\n  .. .. ..$ lsoa                           : chr \"S01017366\"\n  .. .. ..$ msoa                           : chr \"S02003223\"\n  .. .. ..$ lau2                           : chr \"S30000052\"\n  .. .. ..$ pfa                            : chr \"S23000009\"\n  .. .. ..$ nhs_region                     : chr \"S99999999\"\n  .. .. ..$ ttwa                           : chr \"S22000065\"\n  .. .. ..$ national_park                  : chr \"S99999999\"\n  .. .. ..$ bua                            : chr \"S99999999\"\n  .. .. ..$ icb                            : chr \"S99999999\"\n  .. .. ..$ cancer_alliance                : chr \"S99999999\"\n  .. .. ..$ lsoa11                         : chr \"S01010215\"\n  .. .. ..$ msoa11                         : chr \"S02001923\"\n  .. .. ..$ lsoa21                         : chr \"S01017366\"\n  .. .. ..$ msoa21                         : chr \"S02003223\"\n  .. .. ..$ oa21                           : chr \"S00157264\"\n  .. .. ..$ ruc11                          : chr \"1\"\n  .. .. ..$ ruc21                          : chr \"1\"\n  .. .. ..$ lep1                           : chr \"S99999999\"\n  .. .. ..$ lep2                           : NULL\n\n#postcode look up\nbulk_list &lt;- lapply(bulk_lookup_result, \"[[\", 2)\n\nTT_foodbank_location_points9 &lt;-\n  map_dfr(bulk_list,\n          `[`,\n          c(\"postcode\", \"longitude\", \"latitude\"))\n\n#as spatial points\nTT_foodbank_location_points9 &lt;- \n  st_as_sf(TT_foodbank_location_points9,\n           coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\nTT_foodbanks10 &lt;- TT_foodbanks[901:1000, ]\n\n#make postcodes into a list\nlist &lt;- as.list(unique(TT_foodbanks10$postcode))\n\n#foodbank locations\n#lat and long from postcodes\npc_list &lt;- list(postcodes = list)\nbulk_lookup_result &lt;- bulk_postcode_lookup(pc_list)\nstr(bulk_lookup_result[1])\n\nList of 1\n $ :List of 2\n  ..$ query : chr \"BN38EQ\"\n  ..$ result:List of 42\n  .. ..$ postcode                       : chr \"BN3 8EQ\"\n  .. ..$ quality                        : int 1\n  .. ..$ eastings                       : int 526765\n  .. ..$ northings                      : int 107097\n  .. ..$ country                        : chr \"England\"\n  .. ..$ nhs_ha                         : chr \"South East Coast\"\n  .. ..$ longitude                      : num -0.201\n  .. ..$ latitude                       : num 50.8\n  .. ..$ european_electoral_region      : chr \"South East\"\n  .. ..$ primary_care_trust             : chr \"Brighton and Hove City\"\n  .. ..$ region                         : chr \"South East\"\n  .. ..$ lsoa                           : chr \"Brighton and Hove 006B\"\n  .. ..$ msoa                           : chr \"Brighton and Hove 006\"\n  .. ..$ incode                         : chr \"8EQ\"\n  .. ..$ outcode                        : chr \"BN3\"\n  .. ..$ parliamentary_constituency     : chr \"Hove and Portslade\"\n  .. ..$ parliamentary_constituency_2024: chr \"Hove and Portslade\"\n  .. ..$ admin_district                 : chr \"Brighton and Hove\"\n  .. ..$ parish                         : chr \"Brighton and Hove, unparished area\"\n  .. ..$ admin_county                   : NULL\n  .. ..$ date_of_introduction           : chr \"198001\"\n  .. ..$ admin_ward                     : chr \"Hangleton & Knoll\"\n  .. ..$ ced                            : NULL\n  .. ..$ ccg                            : chr \"NHS Sussex\"\n  .. ..$ nuts                           : chr \"Brighton and Hove\"\n  .. ..$ pfa                            : chr \"Sussex\"\n  .. ..$ nhs_region                     : chr \"South East\"\n  .. ..$ ttwa                           : chr \"Brighton\"\n  .. ..$ national_park                  : chr \"England (non-National Park)\"\n  .. ..$ bua                            : chr \"Brighton and Hove\"\n  .. ..$ icb                            : chr \"NHS Sussex Integrated Care Board\"\n  .. ..$ cancer_alliance                : chr \"Surrey and Sussex\"\n  .. ..$ lsoa11                         : chr \"Brighton and Hove 006B\"\n  .. ..$ msoa11                         : chr \"Brighton and Hove 006\"\n  .. ..$ lsoa21                         : chr \"Brighton and Hove 006B\"\n  .. ..$ msoa21                         : chr \"Brighton and Hove 006\"\n  .. ..$ oa21                           : chr \"E00085198\"\n  .. ..$ ruc11                          : chr \"(England/Wales) Urban city and town\"\n  .. ..$ ruc21                          : chr \"Urban: Nearer to a major town or city\"\n  .. ..$ lep1                           : chr \"Coast to Capital\"\n  .. ..$ lep2                           : NULL\n  .. ..$ codes                          :List of 29\n  .. .. ..$ admin_district                 : chr \"E06000043\"\n  .. .. ..$ admin_county                   : chr \"E99999999\"\n  .. .. ..$ admin_ward                     : chr \"E05015402\"\n  .. .. ..$ parish                         : chr \"E43000034\"\n  .. .. ..$ parliamentary_constituency     : chr \"E14001296\"\n  .. .. ..$ parliamentary_constituency_2024: chr \"E14001296\"\n  .. .. ..$ ccg                            : chr \"E38000021\"\n  .. .. ..$ ccg_id                         : chr \"09D\"\n  .. .. ..$ ced                            : chr \"E99999999\"\n  .. .. ..$ nuts                           : chr \"TLJ21\"\n  .. .. ..$ lsoa                           : chr \"E01016883\"\n  .. .. ..$ msoa                           : chr \"E02003496\"\n  .. .. ..$ lau2                           : chr \"E06000043\"\n  .. .. ..$ pfa                            : chr \"E23000033\"\n  .. .. ..$ nhs_region                     : chr \"E40000005\"\n  .. .. ..$ ttwa                           : chr \"E30000179\"\n  .. .. ..$ national_park                  : chr \"E65000001\"\n  .. .. ..$ bua                            : chr \"E63013666\"\n  .. .. ..$ icb                            : chr \"E54000064\"\n  .. .. ..$ cancer_alliance                : chr \"E56000012\"\n  .. .. ..$ lsoa11                         : chr \"E01016883\"\n  .. .. ..$ msoa11                         : chr \"E02003496\"\n  .. .. ..$ lsoa21                         : chr \"E01016883\"\n  .. .. ..$ msoa21                         : chr \"E02003496\"\n  .. .. ..$ oa21                           : chr \"E00085198\"\n  .. .. ..$ ruc11                          : chr \"C1\"\n  .. .. ..$ ruc21                          : chr \"UN1\"\n  .. .. ..$ lep1                           : chr \"E37000044\"\n  .. .. ..$ lep2                           : NULL\n\n#postcode look up\nbulk_list &lt;- lapply(bulk_lookup_result, \"[[\", 2)\n\nTT_foodbank_location_points10 &lt;-\n  map_dfr(bulk_list,\n          `[`,\n          c(\"postcode\", \"longitude\", \"latitude\"))\n\n#as spatial points\nTT_foodbank_location_points10 &lt;- \n  st_as_sf(TT_foodbank_location_points10,\n           coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\nTT_foodbanks11 &lt;- TT_foodbanks[1001:1100, ]\n\n#make postcodes into a list\nlist &lt;- as.list(unique(TT_foodbanks11$postcode))\n\n#foodbank locations\n#lat and long from postcodes\npc_list &lt;- list(postcodes = list)\nbulk_lookup_result &lt;- bulk_postcode_lookup(pc_list)\nstr(bulk_lookup_result[1])\n\nList of 1\n $ :List of 2\n  ..$ query : chr \"NG94HU\"\n  ..$ result:List of 42\n  .. ..$ postcode                       : chr \"NG9 4HU\"\n  .. ..$ quality                        : int 1\n  .. ..$ eastings                       : int 450675\n  .. ..$ northings                      : int 336530\n  .. ..$ country                        : chr \"England\"\n  .. ..$ nhs_ha                         : chr \"East Midlands\"\n  .. ..$ longitude                      : num -1.25\n  .. ..$ latitude                       : num 52.9\n  .. ..$ european_electoral_region      : chr \"East Midlands\"\n  .. ..$ primary_care_trust             : chr \"Nottinghamshire County Teaching\"\n  .. ..$ region                         : chr \"East Midlands\"\n  .. ..$ lsoa                           : chr \"Broxtowe 013E\"\n  .. ..$ msoa                           : chr \"Broxtowe 013\"\n  .. ..$ incode                         : chr \"4HU\"\n  .. ..$ outcode                        : chr \"NG9\"\n  .. ..$ parliamentary_constituency     : chr \"Broxtowe\"\n  .. ..$ parliamentary_constituency_2024: chr \"Broxtowe\"\n  .. ..$ admin_district                 : chr \"Broxtowe\"\n  .. ..$ parish                         : chr \"Broxtowe, unparished area\"\n  .. ..$ admin_county                   : chr \"Nottinghamshire\"\n  .. ..$ date_of_introduction           : chr \"198001\"\n  .. ..$ admin_ward                     : chr \"Chilwell West\"\n  .. ..$ ced                            : chr \"Toton, Chilwell & Attenborough\"\n  .. ..$ ccg                            : chr \"NHS Nottingham and Nottinghamshire\"\n  .. ..$ nuts                           : chr \"Broxtowe\"\n  .. ..$ pfa                            : chr \"Nottinghamshire\"\n  .. ..$ nhs_region                     : chr \"Midlands\"\n  .. ..$ ttwa                           : chr \"Nottingham\"\n  .. ..$ national_park                  : chr \"England (non-National Park)\"\n  .. ..$ bua                            : chr \"Beeston (Broxtowe)\"\n  .. ..$ icb                            : chr \"NHS Nottingham and Nottinghamshire Integrated Care Board\"\n  .. ..$ cancer_alliance                : chr \"East Midlands\"\n  .. ..$ lsoa11                         : chr \"Broxtowe 013E\"\n  .. ..$ msoa11                         : chr \"Broxtowe 013\"\n  .. ..$ lsoa21                         : chr \"Broxtowe 013E\"\n  .. ..$ msoa21                         : chr \"Broxtowe 013\"\n  .. ..$ oa21                           : chr \"E00143172\"\n  .. ..$ ruc11                          : chr \"(England/Wales) Urban minor conurbation\"\n  .. ..$ ruc21                          : chr \"Urban: Nearer to a major town or city\"\n  .. ..$ lep1                           : chr \"Derby, Derbyshire, Nottingham and Nottinghamshire\"\n  .. ..$ lep2                           : NULL\n  .. ..$ codes                          :List of 29\n  .. .. ..$ admin_district                 : chr \"E07000172\"\n  .. .. ..$ admin_county                   : chr \"E10000024\"\n  .. .. ..$ admin_ward                     : chr \"E05010522\"\n  .. .. ..$ parish                         : chr \"E43000289\"\n  .. .. ..$ parliamentary_constituency     : chr \"E14001140\"\n  .. .. ..$ parliamentary_constituency_2024: chr \"E14001140\"\n  .. .. ..$ ccg                            : chr \"E38000243\"\n  .. .. ..$ ccg_id                         : chr \"52R\"\n  .. .. ..$ ced                            : chr \"E58001212\"\n  .. .. ..$ nuts                           : chr \"TLF16\"\n  .. .. ..$ lsoa                           : chr \"E01028101\"\n  .. .. ..$ msoa                           : chr \"E02005862\"\n  .. .. ..$ lau2                           : chr \"E07000172\"\n  .. .. ..$ pfa                            : chr \"E23000019\"\n  .. .. ..$ nhs_region                     : chr \"E40000011\"\n  .. .. ..$ ttwa                           : chr \"E30000249\"\n  .. .. ..$ national_park                  : chr \"E65000001\"\n  .. .. ..$ bua                            : chr \"E63009161\"\n  .. .. ..$ icb                            : chr \"E54000060\"\n  .. .. ..$ cancer_alliance                : chr \"E56000031\"\n  .. .. ..$ lsoa11                         : chr \"E01028101\"\n  .. .. ..$ msoa11                         : chr \"E02005862\"\n  .. .. ..$ lsoa21                         : chr \"E01028101\"\n  .. .. ..$ msoa21                         : chr \"E02005862\"\n  .. .. ..$ oa21                           : chr \"E00143172\"\n  .. .. ..$ ruc11                          : chr \"B1\"\n  .. .. ..$ ruc21                          : chr \"UN1\"\n  .. .. ..$ lep1                           : chr \"E37000045\"\n  .. .. ..$ lep2                           : NULL\n\n#postcode look up\nbulk_list &lt;- lapply(bulk_lookup_result, \"[[\", 2)\n\nTT_foodbank_location_points11 &lt;-\n  map_dfr(bulk_list,\n          `[`,\n          c(\"postcode\", \"longitude\", \"latitude\"))\n\n#as spatial points\nTT_foodbank_location_points11 &lt;- \n  st_as_sf(TT_foodbank_location_points11,\n           coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\nTT_foodbanks12 &lt;- TT_foodbanks[1101:1200, ]\n\n#make postcodes into a list\nlist &lt;- as.list(unique(TT_foodbanks12$postcode))\n\n#foodbank locations\n#lat and long from postcodes\npc_list &lt;- list(postcodes = list)\nbulk_lookup_result &lt;- bulk_postcode_lookup(pc_list)\nstr(bulk_lookup_result[1])\n\nList of 1\n $ :List of 2\n  ..$ query : chr \"SY217LN\"\n  ..$ result:List of 42\n  .. ..$ postcode                       : chr \"SY21 7LN\"\n  .. ..$ quality                        : int 1\n  .. ..$ eastings                       : int 322135\n  .. ..$ northings                      : int 307533\n  .. ..$ country                        : chr \"Wales\"\n  .. ..$ nhs_ha                         : chr \"Powys Teaching Health Board\"\n  .. ..$ longitude                      : num -3.15\n  .. ..$ latitude                       : num 52.7\n  .. ..$ european_electoral_region      : chr \"Wales\"\n  .. ..$ primary_care_trust             : chr \"Powys Teaching Health Board\"\n  .. ..$ region                         : NULL\n  .. ..$ lsoa                           : chr \"Powys 003A\"\n  .. ..$ msoa                           : chr \"Powys 003\"\n  .. ..$ incode                         : chr \"7LN\"\n  .. ..$ outcode                        : chr \"SY21\"\n  .. ..$ parliamentary_constituency     : chr \"Montgomeryshire and Glyndwr\"\n  .. ..$ parliamentary_constituency_2024: chr \"Montgomeryshire and Glyndwr\"\n  .. ..$ admin_district                 : chr \"Powys\"\n  .. ..$ parish                         : chr \"Welshpool\"\n  .. ..$ admin_county                   : NULL\n  .. ..$ date_of_introduction           : chr \"198001\"\n  .. ..$ admin_ward                     : chr \"Welshpool Castle\"\n  .. ..$ ced                            : NULL\n  .. ..$ ccg                            : chr \"Powys Teaching\"\n  .. ..$ nuts                           : chr \"Powys\"\n  .. ..$ pfa                            : chr \"Dyfed-Powys\"\n  .. ..$ nhs_region                     : NULL\n  .. ..$ ttwa                           : chr \"Newtown and Welshpool\"\n  .. ..$ national_park                  : chr \"Wales (non-National Park)\"\n  .. ..$ bua                            : chr \"Welshpool\"\n  .. ..$ icb                            : NULL\n  .. ..$ cancer_alliance                : NULL\n  .. ..$ lsoa11                         : chr \"Powys 003A\"\n  .. ..$ msoa11                         : chr \"Powys 003\"\n  .. ..$ lsoa21                         : chr \"Powys 003A\"\n  .. ..$ msoa21                         : chr \"Powys 003\"\n  .. ..$ oa21                           : chr \"W00002665\"\n  .. ..$ ruc11                          : chr \"(England/Wales) Rural town and fringe in a sparse setting\"\n  .. ..$ ruc21                          : chr \"Larger rural: Further from a major town or city\"\n  .. ..$ lep1                           : NULL\n  .. ..$ lep2                           : NULL\n  .. ..$ codes                          :List of 29\n  .. .. ..$ admin_district                 : chr \"W06000023\"\n  .. .. ..$ admin_county                   : chr \"W99999999\"\n  .. .. ..$ admin_ward                     : chr \"W05001172\"\n  .. .. ..$ parish                         : chr \"W04000352\"\n  .. .. ..$ parliamentary_constituency     : chr \"W07000102\"\n  .. .. ..$ parliamentary_constituency_2024: chr \"W07000102\"\n  .. .. ..$ ccg                            : chr \"W11000024\"\n  .. .. ..$ ccg_id                         : chr \"7A7\"\n  .. .. ..$ ced                            : chr \"W99999999\"\n  .. .. ..$ nuts                           : chr \"TLL41\"\n  .. .. ..$ lsoa                           : chr \"W01000498\"\n  .. .. ..$ msoa                           : chr \"W02000099\"\n  .. .. ..$ lau2                           : chr \"W06000023\"\n  .. .. ..$ pfa                            : chr \"W15000004\"\n  .. .. ..$ nhs_region                     : chr \"W99999999\"\n  .. .. ..$ ttwa                           : chr \"W22000015\"\n  .. .. ..$ national_park                  : chr \"W31000001\"\n  .. .. ..$ bua                            : chr \"W45000836\"\n  .. .. ..$ icb                            : chr \"W99999999\"\n  .. .. ..$ cancer_alliance                : chr \"W99999999\"\n  .. .. ..$ lsoa11                         : chr \"W01000498\"\n  .. .. ..$ msoa11                         : chr \"W02000099\"\n  .. .. ..$ lsoa21                         : chr \"W01000498\"\n  .. .. ..$ msoa21                         : chr \"W02000099\"\n  .. .. ..$ oa21                           : chr \"W00002665\"\n  .. .. ..$ ruc11                          : chr \"D2\"\n  .. .. ..$ ruc21                          : chr \"RLF1\"\n  .. .. ..$ lep1                           : chr \"W99999999\"\n  .. .. ..$ lep2                           : NULL\n\n#postcode look up\nbulk_list &lt;- lapply(bulk_lookup_result, \"[[\", 2)\n\nTT_foodbank_location_points12 &lt;-\n  map_dfr(bulk_list,\n          `[`,\n          c(\"postcode\", \"longitude\", \"latitude\"))\n\n#as spatial points\nTT_foodbank_location_points12 &lt;- \n  st_as_sf(TT_foodbank_location_points12,\n           coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\nTT_foodbanks13 &lt;- TT_foodbanks[1201:1300, ]\n\n#make postcodes into a list\nlist &lt;- as.list(unique(TT_foodbanks13$postcode))\n\n#foodbank locations\n#lat and long from postcodes\npc_list &lt;- list(postcodes = list)\nbulk_lookup_result &lt;- bulk_postcode_lookup(pc_list)\nstr(bulk_lookup_result[1])\n\nList of 1\n $ :List of 2\n  ..$ query : chr \"SN95ES\"\n  ..$ result:List of 42\n  .. ..$ postcode                       : chr \"SN9 5ES\"\n  .. ..$ quality                        : int 1\n  .. ..$ eastings                       : int 416336\n  .. ..$ northings                      : int 160154\n  .. ..$ country                        : chr \"England\"\n  .. ..$ nhs_ha                         : chr \"South West\"\n  .. ..$ longitude                      : num -1.77\n  .. ..$ latitude                       : num 51.3\n  .. ..$ european_electoral_region      : chr \"South West\"\n  .. ..$ primary_care_trust             : chr \"Wiltshire\"\n  .. ..$ region                         : chr \"South West\"\n  .. ..$ lsoa                           : chr \"Wiltshire 024C\"\n  .. ..$ msoa                           : chr \"Wiltshire 024\"\n  .. ..$ incode                         : chr \"5ES\"\n  .. ..$ outcode                        : chr \"SN9\"\n  .. ..$ parliamentary_constituency     : chr \"East Wiltshire\"\n  .. ..$ parliamentary_constituency_2024: chr \"East Wiltshire\"\n  .. ..$ admin_district                 : chr \"Wiltshire\"\n  .. ..$ parish                         : chr \"Pewsey\"\n  .. ..$ admin_county                   : NULL\n  .. ..$ date_of_introduction           : chr \"198001\"\n  .. ..$ admin_ward                     : chr \"Pewsey\"\n  .. ..$ ced                            : NULL\n  .. ..$ ccg                            : chr \"NHS Bath and North East Somerset, Swindon and Wiltshire\"\n  .. ..$ nuts                           : chr \"Wiltshire\"\n  .. ..$ pfa                            : chr \"Wiltshire\"\n  .. ..$ nhs_region                     : chr \"South West\"\n  .. ..$ ttwa                           : chr \"Swindon\"\n  .. ..$ national_park                  : chr \"England (non-National Park)\"\n  .. ..$ bua                            : chr \"Pewsey\"\n  .. ..$ icb                            : chr \"NHS Bath and North East Somerset, Swindon and Wiltshire Integrated Care Board\"\n  .. ..$ cancer_alliance                : chr \"Somerset, Wiltshire, Avon and Gloucestershire\"\n  .. ..$ lsoa11                         : chr \"Wiltshire 024C\"\n  .. ..$ msoa11                         : chr \"Wiltshire 024\"\n  .. ..$ lsoa21                         : chr \"Wiltshire 024C\"\n  .. ..$ msoa21                         : chr \"Wiltshire 024\"\n  .. ..$ oa21                           : chr \"E00162545\"\n  .. ..$ ruc11                          : chr \"(England/Wales) Rural town and fringe\"\n  .. ..$ ruc21                          : chr \"Larger rural: Further from a major town or city\"\n  .. ..$ lep1                           : chr \"Swindon and Wiltshire\"\n  .. ..$ lep2                           : NULL\n  .. ..$ codes                          :List of 29\n  .. .. ..$ admin_district                 : chr \"E06000054\"\n  .. .. ..$ admin_county                   : chr \"E99999999\"\n  .. .. ..$ admin_ward                     : chr \"E05013834\"\n  .. .. ..$ parish                         : chr \"E04013045\"\n  .. .. ..$ parliamentary_constituency     : chr \"E14001217\"\n  .. .. ..$ parliamentary_constituency_2024: chr \"E14001217\"\n  .. .. ..$ ccg                            : chr \"E38000231\"\n  .. .. ..$ ccg_id                         : chr \"92G\"\n  .. .. ..$ ced                            : chr \"E99999999\"\n  .. .. ..$ nuts                           : chr \"TLK72\"\n  .. .. ..$ lsoa                           : chr \"E01031870\"\n  .. .. ..$ msoa                           : chr \"E02006636\"\n  .. .. ..$ lau2                           : chr \"E06000054\"\n  .. .. ..$ pfa                            : chr \"E23000038\"\n  .. .. ..$ nhs_region                     : chr \"E40000006\"\n  .. .. ..$ ttwa                           : chr \"E30000276\"\n  .. .. ..$ national_park                  : chr \"E65000001\"\n  .. .. ..$ bua                            : chr \"E63012478\"\n  .. .. ..$ icb                            : chr \"E54000040\"\n  .. .. ..$ cancer_alliance                : chr \"E56000033\"\n  .. .. ..$ lsoa11                         : chr \"E01031870\"\n  .. .. ..$ msoa11                         : chr \"E02006636\"\n  .. .. ..$ lsoa21                         : chr \"E01031870\"\n  .. .. ..$ msoa21                         : chr \"E02006636\"\n  .. .. ..$ oa21                           : chr \"E00162545\"\n  .. .. ..$ ruc11                          : chr \"D1\"\n  .. .. ..$ ruc21                          : chr \"RLF1\"\n  .. .. ..$ lep1                           : chr \"E37000033\"\n  .. .. ..$ lep2                           : NULL\n\n#postcode look up\nbulk_list &lt;- lapply(bulk_lookup_result, \"[[\", 2)\n\nTT_foodbank_location_points13 &lt;-\n  map_dfr(bulk_list,\n          `[`,\n          c(\"postcode\", \"longitude\", \"latitude\"))\n\n#as spatial points\nTT_foodbank_location_points13 &lt;- \n  st_as_sf(TT_foodbank_location_points13,\n           coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\nTT_foodbanks14 &lt;- TT_foodbanks[1301:1400, ]\n\n#make postcodes into a list\nlist &lt;- as.list(unique(TT_foodbanks14$postcode))\n\n#foodbank locations\n#lat and long from postcodes\npc_list &lt;- list(postcodes = list)\nbulk_lookup_result &lt;- bulk_postcode_lookup(pc_list)\nstr(bulk_lookup_result[1])\n\nList of 1\n $ :List of 2\n  ..$ query : chr \"EX48LZ\"\n  ..$ result:List of 42\n  .. ..$ postcode                       : chr \"EX4 8LZ\"\n  .. ..$ quality                        : int 1\n  .. ..$ eastings                       : int 294494\n  .. ..$ northings                      : int 94335\n  .. ..$ country                        : chr \"England\"\n  .. ..$ nhs_ha                         : chr \"South West\"\n  .. ..$ longitude                      : num -3.5\n  .. ..$ latitude                       : num 50.7\n  .. ..$ european_electoral_region      : chr \"South West\"\n  .. ..$ primary_care_trust             : chr \"Devon\"\n  .. ..$ region                         : chr \"South West\"\n  .. ..$ lsoa                           : chr \"Exeter 002B\"\n  .. ..$ msoa                           : chr \"Exeter 002\"\n  .. ..$ incode                         : chr \"8LZ\"\n  .. ..$ outcode                        : chr \"EX4\"\n  .. ..$ parliamentary_constituency     : chr \"Exeter\"\n  .. ..$ parliamentary_constituency_2024: chr \"Exeter\"\n  .. ..$ admin_district                 : chr \"Exeter\"\n  .. ..$ parish                         : chr \"Exeter, unparished area\"\n  .. ..$ admin_county                   : chr \"Devon\"\n  .. ..$ date_of_introduction           : chr \"198001\"\n  .. ..$ admin_ward                     : chr \"Mincinglake and Whipton\"\n  .. ..$ ced                            : chr \"Pinhoe & Mincinglake\"\n  .. ..$ ccg                            : chr \"NHS Devon\"\n  .. ..$ nuts                           : chr \"Exeter\"\n  .. ..$ pfa                            : chr \"Devon & Cornwall\"\n  .. ..$ nhs_region                     : chr \"South West\"\n  .. ..$ ttwa                           : chr \"Exeter\"\n  .. ..$ national_park                  : chr \"England (non-National Park)\"\n  .. ..$ bua                            : chr \"Exeter\"\n  .. ..$ icb                            : chr \"NHS Devon Integrated Care Board\"\n  .. ..$ cancer_alliance                : chr \"Peninsula\"\n  .. ..$ lsoa11                         : chr \"Exeter 002B\"\n  .. ..$ msoa11                         : chr \"Exeter 002\"\n  .. ..$ lsoa21                         : chr \"Exeter 002B\"\n  .. ..$ msoa21                         : chr \"Exeter 002\"\n  .. ..$ oa21                           : chr \"E00101187\"\n  .. ..$ ruc11                          : chr \"(England/Wales) Urban city and town\"\n  .. ..$ ruc21                          : chr \"Urban: Nearer to a major town or city\"\n  .. ..$ lep1                           : chr \"Heart of the South West\"\n  .. ..$ lep2                           : NULL\n  .. ..$ codes                          :List of 29\n  .. .. ..$ admin_district                 : chr \"E07000041\"\n  .. .. ..$ admin_county                   : chr \"E10000008\"\n  .. .. ..$ admin_ward                     : chr \"E05011015\"\n  .. .. ..$ parish                         : chr \"E43000052\"\n  .. .. ..$ parliamentary_constituency     : chr \"E14001231\"\n  .. .. ..$ parliamentary_constituency_2024: chr \"E14001231\"\n  .. .. ..$ ccg                            : chr \"E38000230\"\n  .. .. ..$ ccg_id                         : chr \"15N\"\n  .. .. ..$ ced                            : chr \"E58000293\"\n  .. .. ..$ nuts                           : chr \"TLK43\"\n  .. .. ..$ lsoa                           : chr \"E01019991\"\n  .. .. ..$ msoa                           : chr \"E02004150\"\n  .. .. ..$ lau2                           : chr \"E07000041\"\n  .. .. ..$ pfa                            : chr \"E23000035\"\n  .. .. ..$ nhs_region                     : chr \"E40000006\"\n  .. .. ..$ ttwa                           : chr \"E30000206\"\n  .. .. ..$ national_park                  : chr \"E65000001\"\n  .. .. ..$ bua                            : chr \"E63013856\"\n  .. .. ..$ icb                            : chr \"E54000037\"\n  .. .. ..$ cancer_alliance                : chr \"E56000014\"\n  .. .. ..$ lsoa11                         : chr \"E01019991\"\n  .. .. ..$ msoa11                         : chr \"E02004150\"\n  .. .. ..$ lsoa21                         : chr \"E01019991\"\n  .. .. ..$ msoa21                         : chr \"E02004150\"\n  .. .. ..$ oa21                           : chr \"E00101187\"\n  .. .. ..$ ruc11                          : chr \"C1\"\n  .. .. ..$ ruc21                          : chr \"UN1\"\n  .. .. ..$ lep1                           : chr \"E37000016\"\n  .. .. ..$ lep2                           : NULL\n\n#postcode look up\nbulk_list &lt;- lapply(bulk_lookup_result, \"[[\", 2)\n\nTT_foodbank_location_points14 &lt;-\n  map_dfr(bulk_list,\n          `[`,\n          c(\"postcode\", \"longitude\", \"latitude\"))\n\n#as spatial points\nTT_foodbank_location_points14 &lt;- \n  st_as_sf(TT_foodbank_location_points14,\n           coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n\nTT_foodbanks15 &lt;- TT_foodbanks[1401:1425, ]\n\n#make postcodes into a list\nlist &lt;- as.list(unique(TT_foodbanks15$postcode))\n\n#foodbank locations\n#lat and long from postcodes\npc_list &lt;- list(postcodes = list)\nbulk_lookup_result &lt;- bulk_postcode_lookup(pc_list)\nstr(bulk_lookup_result[1])\n\nList of 1\n $ :List of 2\n  ..$ query : chr \"L269UH\"\n  ..$ result:List of 42\n  .. ..$ postcode                       : chr \"L26 9UH\"\n  .. ..$ quality                        : int 1\n  .. ..$ eastings                       : int 345152\n  .. ..$ northings                      : int 384999\n  .. ..$ country                        : chr \"England\"\n  .. ..$ nhs_ha                         : chr \"North West\"\n  .. ..$ longitude                      : num -2.83\n  .. ..$ latitude                       : num 53.4\n  .. ..$ european_electoral_region      : chr \"North West\"\n  .. ..$ primary_care_trust             : chr \"Knowsley\"\n  .. ..$ region                         : chr \"North West\"\n  .. ..$ lsoa                           : chr \"Knowsley 020B\"\n  .. ..$ msoa                           : chr \"Knowsley 020\"\n  .. ..$ incode                         : chr \"9UH\"\n  .. ..$ outcode                        : chr \"L26\"\n  .. ..$ parliamentary_constituency     : chr \"Widnes and Halewood\"\n  .. ..$ parliamentary_constituency_2024: chr \"Widnes and Halewood\"\n  .. ..$ admin_district                 : chr \"Knowsley\"\n  .. ..$ parish                         : chr \"Halewood\"\n  .. ..$ admin_county                   : NULL\n  .. ..$ date_of_introduction           : chr \"198001\"\n  .. ..$ admin_ward                     : chr \"Halewood South\"\n  .. ..$ ced                            : NULL\n  .. ..$ ccg                            : chr \"NHS Cheshire and Merseyside\"\n  .. ..$ nuts                           : chr \"Knowsley\"\n  .. ..$ pfa                            : chr \"Merseyside\"\n  .. ..$ nhs_region                     : chr \"North West\"\n  .. ..$ ttwa                           : chr \"Liverpool\"\n  .. ..$ national_park                  : chr \"England (non-National Park)\"\n  .. ..$ bua                            : chr \"Liverpool\"\n  .. ..$ icb                            : chr \"NHS Cheshire and Merseyside Integrated Care Board\"\n  .. ..$ cancer_alliance                : chr \"Cheshire and Merseyside\"\n  .. ..$ lsoa11                         : chr \"Knowsley 020B\"\n  .. ..$ msoa11                         : chr \"Knowsley 020\"\n  .. ..$ lsoa21                         : chr \"Knowsley 020B\"\n  .. ..$ msoa21                         : chr \"Knowsley 020\"\n  .. ..$ oa21                           : chr \"E00032577\"\n  .. ..$ ruc11                          : chr \"(England/Wales) Urban major conurbation\"\n  .. ..$ ruc21                          : chr \"Urban: Nearer to a major town or city\"\n  .. ..$ lep1                           : chr \"Liverpool City Region\"\n  .. ..$ lep2                           : NULL\n  .. ..$ codes                          :List of 29\n  .. .. ..$ admin_district                 : chr \"E08000011\"\n  .. .. ..$ admin_county                   : chr \"E99999999\"\n  .. .. ..$ admin_ward                     : chr \"E05010937\"\n  .. .. ..$ parish                         : chr \"E04000017\"\n  .. .. ..$ parliamentary_constituency     : chr \"E14001584\"\n  .. .. ..$ parliamentary_constituency_2024: chr \"E14001584\"\n  .. .. ..$ ccg                            : chr \"E38000091\"\n  .. .. ..$ ccg_id                         : chr \"01J\"\n  .. .. ..$ ced                            : chr \"E99999999\"\n  .. .. ..$ nuts                           : chr \"TLD71\"\n  .. .. ..$ lsoa                           : chr \"E01006428\"\n  .. .. ..$ msoa                           : chr \"E02001346\"\n  .. .. ..$ lau2                           : chr \"E08000011\"\n  .. .. ..$ pfa                            : chr \"E23000004\"\n  .. .. ..$ nhs_region                     : chr \"E40000014\"\n  .. .. ..$ ttwa                           : chr \"E30000233\"\n  .. .. ..$ national_park                  : chr \"E65000001\"\n  .. .. ..$ bua                            : chr \"E63008477\"\n  .. .. ..$ icb                            : chr \"E54000008\"\n  .. .. ..$ cancer_alliance                : chr \"E56000005\"\n  .. .. ..$ lsoa11                         : chr \"E01006428\"\n  .. .. ..$ msoa11                         : chr \"E02001346\"\n  .. .. ..$ lsoa21                         : chr \"E01006428\"\n  .. .. ..$ msoa21                         : chr \"E02001346\"\n  .. .. ..$ oa21                           : chr \"E00032577\"\n  .. .. ..$ ruc11                          : chr \"A1\"\n  .. .. ..$ ruc21                          : chr \"UN1\"\n  .. .. ..$ lep1                           : chr \"E37000022\"\n  .. .. ..$ lep2                           : NULL\n\n#postcode look up\nbulk_list &lt;- lapply(bulk_lookup_result, \"[[\", 2)\n\nTT_foodbank_location_points15 &lt;-\n  map_dfr(bulk_list,\n          `[`,\n          c(\"postcode\", \"longitude\", \"latitude\"))\n\n#as spatial points\nTT_foodbank_location_points15 &lt;- \n  st_as_sf(TT_foodbank_location_points15,\n           coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\nAfter all the Trussell centres have had their point location derived, we bind them together in one data frame.\n\nTT_foodbank_location_points &lt;- rbind(TT_foodbank_location_points1, TT_foodbank_location_points2,\n                                     TT_foodbank_location_points3, TT_foodbank_location_points4, \n                                     TT_foodbank_location_points5, TT_foodbank_location_points6,\n                                     TT_foodbank_location_points7, TT_foodbank_location_points8,\n                                     TT_foodbank_location_points9, TT_foodbank_location_points10,\n                                     TT_foodbank_location_points11, TT_foodbank_location_points12,\n                                     TT_foodbank_location_points13, TT_foodbank_location_points14,\n                                     TT_foodbank_location_points15)\n\n\nnrow(TT_foodbanks) - nrow(TT_foodbank_location_points)\n\n[1] 42\n\n\nThis process has lost 42 food bank centres as postcode could not be recognised/matched with a longitude and latitude.\nWe need to join the point location to the other information on the food bank centres like the centre’s name and the operating model etc.\n\n#removing white space from postcodes in order to merge with more information\nTT_foodbank_location_points$postcode &lt;- gsub(\"[[:space:]]\", \"\", TT_foodbank_location_points$postcode)\n\n#joining point data with other information on food bank centres e.g. name and operating model\nTT_foodbank_location_points &lt;- TT_foodbank_location_points %&gt;%\n  merge(.,\n        TT_foodbanks, \n        by.x = \"postcode\",\n        by.y = \"postcode\",\n        all.x = TRUE)\n\n#select specific columns\nTT_foodbank_location_points &lt;- TT_foodbank_location_points %&gt;%\n  distinct(foodbank_centre_number, postcode, .keep_all = TRUE) %&gt;%\n  dplyr::select(food_bank_name, distribution_centre_name, foodbank_centre_number, postcode, operating_model, number_of_distribution_centres_final)\n\n\n\n5.1.3 Create a map of Trussell food bank locations in the UK\nNow we have the point location of Trussell centres, we can use the tmap package to visualise where the Trussell food bank centres are across the UK by making a simple map.\n\nTT_foodbank_location_points &lt;- TT_foodbank_location_points %&gt;%\n  st_transform(., 27700) \n\n#spatially subset (select) the point locations to be only within the shape file area - excludes food banks outside the boundary\n# defaults to st_intersects\nTT_foodbank_location_points &lt;- TT_foodbank_location_points[UK_border,] \n\n#plot map\nPlain_map &lt;- \n  tm_shape(UK_border) +\n  tm_fill(col = \"antiquewhite\", alpha = 0.5) +\n  tm_borders(col = \"black\", lwd = 0.1) +\n  tm_shape(TT_foodbank_location_points) +\n  tm_symbols(col = \"forestgreen\", size = 0.03) +\n  tm_scale_bar(position = c(\"left\", \"top\")) + \n  tm_compass(position = c(\"right\", \"top\")) + \n  tm_layout(main.title = \"Trussell food bank centres\", \n            main.title.position = \"centre\",\n            main.title.size = 0.7, \n            legend.outside = TRUE, frame = FALSE)\n\nPlain_map\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuestions\n\nWhat does this tell up about the spatial distribution of food banks?\nWhat could be added to this map to enhance the visualisation?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Food bank accessibility</span>"
    ]
  },
  {
    "objectID": "05_food_bank_accessibility.html#create-a-map-of-trussell-food-bank-locations-in-oxford-and-oxfordshire",
    "href": "05_food_bank_accessibility.html#create-a-map-of-trussell-food-bank-locations-in-oxford-and-oxfordshire",
    "title": "5  Food bank accessibility",
    "section": "5.2 Create a map of Trussell food bank locations in Oxford and Oxfordshire",
    "text": "5.2 Create a map of Trussell food bank locations in Oxford and Oxfordshire\nLet’s zoom in on Oxford and the surrounding area.\n\n#use string detect function to find local authorities with these words in their names\nOxford_LAs &lt;- LA_2022 %&gt;%\n  filter(str_detect(lad22nm, \"Oxford\") | \n           str_detect(lad22nm, \"White Horse\") | \n           str_detect(lad22nm, \"Cherwell\")) \n\n\n#use string detect function to isolate the border of the city of Oxford\nOxford_City_LAs &lt;- LA_2022 %&gt;%\n  filter(str_detect(lad22cd, \"E07000178\")) \n\n\n#find food bank centres from within the border we have created\nOxford_TT_foodbanks &lt;- TT_foodbank_location_points[Oxford_LAs,] \n\nOxford_map &lt;- \n  tm_shape(Oxford_LAs) +\n  tm_fill(col = \"antiquewhite\", alpha = 0.5) +\n  tm_borders(col = \"black\", lwd = 0.1) +\n  tm_shape(Oxford_City_LAs) +\n  tm_borders(col = \"black\", lwd = 1) + #bolder border around the City of Oxford\n  tm_shape(Oxford_TT_foodbanks) +\n  tm_symbols(col = \"deepskyblue\", size = 0.1) +\n  tm_scale_bar(position = c(\"right\", \"top\")) + \n  tm_compass(position = c(\"right\", \"top\")) + \n  tm_layout(main.title = \"Trussell centres in & around Oxford\", \n            main.title.position = \"centre\",\n            main.title.size = 0.9, \n            legend.outside = TRUE, frame = FALSE)\n\nOxford_map\n\n\n\n\n\n\n\n\n\ntmap_mode(\"view\")\n\n#plot map\nOxford_map &lt;- \n  tm_shape(Oxford_LAs) +\n  tm_fill(col = \"antiquewhite\", alpha = 0.5) +\n  tm_borders(col = \"black\", lwd = 0.1) +\n  tm_shape(Oxford_City_LAs) +\n  tm_borders(col = \"black\", lwd = 1) + #bolder border around the City of Oxford\n  tm_shape(Oxford_TT_foodbanks) +\n  tm_symbols(col = \"deepskyblue\", size = 0.1) +\n  tm_scale_bar(position = c(\"right\", \"top\")) + \n  tm_compass(position = c(\"right\", \"top\")) + \n    tm_layout(main.title = \"Trussell centres in/around Oxford\", \n            main.title.position = \"centre\",\n            main.title.size = 0.9, \n            legend.outside = TRUE, frame = FALSE)\n\nOxford_map\n\n\n\n\n\nWe see only seven food bank centres across the region, and no food bank centres in the City of Oxford itself, so we look at other sources of data to see if this is true.\n\n5.2.1 Give Food food bank data\nGive Food is an open access database with information on food banks in the UK. Next, load the food bank data with addresses (from Give Food). However, these are food banks and not distribution centres.\n\n#data from give food\ngive_food_foodbanks &lt;- read_csv(\"prac5_data/foodbanks.csv\") %&gt;%\n  clean_names()\n\n\n#removing white space from postcodes\ngive_food_foodbanks$postcode &lt;- gsub(\"[[:space:]]\", \"\", give_food_foodbanks$postcode)\n\n#filter out only distribution centres\ngive_food_foodbanks &lt;- give_food_foodbanks %&gt;%\n  filter(closed != \"TRUE\") #remove centres that are currently closed\n\n\n#create 2 versions\nOxford_give_food_foodbanks &lt;- give_food_foodbanks %&gt;%\n  filter(str_detect(postcode, \"OX\")) \n\nOxford_give_food_foodbanks2 &lt;- give_food_foodbanks %&gt;%\n  filter(str_detect(postcode, \"OX\")) \n\n\nOxford_give_food_foodbanks2$postcode &lt;- gsub(\"[[:space:]]\", \"\", Oxford_give_food_foodbanks2$postcode)\n\n#create spatial object\nOxford_give_food_foodbanks2 &lt;- Oxford_give_food_foodbanks2 %&gt;%\n  st_transform(., 27700) \n\n#select only specific variables needed\nOxford_give_food_foodbanks &lt;- Oxford_give_food_foodbanks %&gt;%\n  select(name, postcode, district, network)\n\nOxford_give_food_foodbanks &lt;- Oxford_give_food_foodbanks2 %&gt;%\n  merge(.,\n        Oxford_give_food_foodbanks, \n        by.x = \"postcode\",\n        by.y = \"postcode\",\n        all.x = TRUE)\n\n\n#create spatial object\nOxford_give_food_foodbanks &lt;- Oxford_give_food_foodbanks %&gt;%\n  st_transform(., 27700) \n\ntmap_mode(\"plot\")\n\n#plot map\nOxford_map &lt;- \n  tm_shape(Oxford_LAs) +\n  tm_fill(col = \"antiquewhite\", alpha = 0.5) +\n  tm_borders(col = \"black\", lwd = 0.1) +\n  tm_shape(Oxford_City_LAs) +\n  tm_borders(col = \"black\", lwd = 1) + #bolder border around the City of Oxford\n  tm_shape(Oxford_give_food_foodbanks) +\n  tm_symbols(col = \"red\", size = 0.1) +\n  tm_scale_bar(position = c(\"right\", \"top\")) + \n  tm_compass(position = c(\"right\", \"top\")) + \n  tm_layout(main.title = \"Other food banks in & around Oxford\", \n            main.title.position = \"centre\",\n            main.title.size = 0.9, \n            legend.outside = TRUE, frame = FALSE)\n\nOxford_map\n\n\n\n\n\n\n\n\n\ntmap_mode(\"view\")\n\n#plot map\nOxford_map &lt;- \n  tm_shape(Oxford_LAs) +\n  tm_fill(col = \"antiquewhite\", alpha = 0.5) +\n  tm_borders(col = \"black\", lwd = 0.1) +\n  tm_shape(Oxford_City_LAs) +\n  tm_borders(col = \"black\", lwd = 1) + #bolder border around the City of Oxford\n  tm_shape(Oxford_give_food_foodbanks) +\n  tm_symbols(col = \"red\", size = 0.1) +\n  tm_scale_bar(position = c(\"right\", \"top\")) + \n  tm_compass(position = c(\"right\", \"top\")) + \n    tm_layout(main.title = \"Other food banks in & around Oxford\", \n            main.title.position = \"centre\",\n            main.title.size = 0.9, \n            legend.outside = TRUE, frame = FALSE)\n\nOxford_map\n\n\n\n\n\nIf you hover over the food bank centres on the interactive map, information on the centre that is contained in the data frame will pop up in a box.\nNext we map the same data, but spilt the centres by network type.\n\ntrussell_Oxford &lt;- Oxford_give_food_foodbanks %&gt;%\n  filter(network == \"Trussell\")\n\nIFAN_Oxford &lt;- Oxford_give_food_foodbanks %&gt;%\n  filter(network == \"IFAN\")\n\nindependent_Oxford &lt;- Oxford_give_food_foodbanks %&gt;%\n  filter(network == \"Independent\")\n\n#change mode from interactive to static\ntmap_mode(\"plot\")\n\n#plot map\nOxford_map &lt;- \n  tm_shape(Oxford_LAs) +\n  tm_fill(col = \"antiquewhite\", alpha = 0.5) +\n  tm_borders(col = \"black\", lwd = 0.1) +\n  tm_shape(Oxford_City_LAs) +\n  tm_borders(col = \"black\", lwd = 1) + #bolder border around the City of Oxford\n  tm_shape(trussell_Oxford) +\n    tm_symbols(col = \"purple\", size = 0.15) +\n  tm_shape(IFAN_Oxford) +\n    tm_symbols(col = \"orange\", size = 0.15) +\n  tm_shape(independent_Oxford) +\n    tm_symbols(col = \"red\", size = 0.15) +\n  tm_scale_bar(position = c(\"right\", \"top\")) + \n  tm_compass(position = c(\"right\", \"top\")) + \n  tm_layout(legend.outside = TRUE, frame = FALSE, \n            main.title = \"Other food banks in & around Oxford\", \n            main.title.position = \"centre\",\n            main.title.size = 0.9) +\n  tm_add_legend('symbol', \n    col = c(\"purple\", \"orange\", \"red\"),\n    border.col = \"black\",\n    size = 1,\n    labels = c('Trussell',\n               'IFAN',\n               'Independent'))\n\nOxford_map\n\n\n\n\n\n\n\n\nGive Food is an open access database with information on food banks in the UK. Next, load the food bank data with addresses (from Give Food). However, these are food banks (and could under represent the sites) and not distribution centres.\n\n#change mode from interactive to static\ntmap_mode(\"plot\")\n\n#plot map\nOxford_map &lt;- \n  tm_shape(Oxford_LAs) +\n  tm_fill(col = \"antiquewhite\", alpha = 0.5) +\n  tm_borders(col = \"black\", lwd = 0.1) +\n  tm_shape(Oxford_City_LAs) +\n  tm_borders(col = \"black\", lwd = 1) + \n  tm_shape(Oxford_TT_foodbanks) +\n    tm_symbols(col = \"purple\", size = 0.15) +\n  tm_shape(trussell_Oxford) +\n    tm_symbols(col = \"purple\", size = 0.15) +\n  tm_shape(IFAN_Oxford) +\n    tm_symbols(col = \"orange\", size = 0.15) +\n  tm_shape(independent_Oxford) +\n    tm_symbols(col = \"red\", size = 0.15) +\n  tm_scale_bar(position = c(\"right\", \"top\")) + \n  tm_compass(position = c(\"right\", \"top\")) + \n  tm_layout(legend.outside = TRUE, frame = FALSE, \n            main.title = \"Other food banks in & around Oxford\", \n            main.title.position = \"centre\",\n            main.title.size = 0.9) +\n  tm_add_legend('symbol', \n    col = c(\"purple\", \"orange\", \"red\"),\n    border.col = \"black\",\n    size = 1,\n    labels = c('Trussell',\n               'IFAN',\n               'Independent'))\n\nOxford_map",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Food bank accessibility</span>"
    ]
  },
  {
    "objectID": "05_food_bank_accessibility.html#calculate-accessibility-for-walking-driving-and-public-transport-using-r5r-package",
    "href": "05_food_bank_accessibility.html#calculate-accessibility-for-walking-driving-and-public-transport-using-r5r-package",
    "title": "5  Food bank accessibility",
    "section": "5.3 Calculate accessibility for walking, driving, and public transport using r5r package",
    "text": "5.3 Calculate accessibility for walking, driving, and public transport using r5r package\nr5r is an R package for rapid realistic routing on multi-modal transport networks (walk, bike, public transport and car).\nTo use r5r, we will need:\n\nA road network data set from OpenStreetMap in .pbf format (mandatory)\nA public transport feed in GTFS.zip format (optional)\nA raster file of Digital Elevation Model data in .tif format (optional)\n\nLink to R package page: https://ipeagit.github.io/r5r/.\n\n5.3.1 GTFS files\nThe subsections below detail\n\n5.3.1.1 Bus, Trams and Tubes\nThere are a couple of websites to get a UK GTFS feed:\n\nMobility Database\nTraveline National Dataset (TNDS)\n\nAt the moment the TNDS is the best option - as it is divided into regions (and actually works). Note this is just for Bus, Trams and Tubes.\nTo get the data:\n\nRegister for free\nDownload a FTP software, e.g. Core FTP\nFollow the instructions in the TNDS email and download data for the South East.\n\nThis data is in a TransXchange format that we must convert to a GTFS through the UK2GTFS package.\n\nremotes::install_github(\"ITSleeds/UK2GTFS\")\n\nThe UK2GTFS package requires National Public Transport Access Nodes (NapTAN). It should download it automatically, but if it doesn’t download it from gov.uk and place it in a folder called temp_naptan in the root of you project (i.e. in the same places as the .RProj file)\n\nlibrary(UK2GTFS)\n\n# you might not need this\nnaptan_local &lt;- read_csv(\"temp_naptan/Stops.csv\", show_col_types = FALSE) %&gt;%\n  rename(\n    stop_id = ATCOCode,\n    stop_name = CommonName,\n    stop_lat = Latitude,\n    stop_lon = Longitude,\n    stop_type = StopType\n  )\n\n# %&gt;%\n#    filter(!is.na(stop_id),\n#          !is.na(stop_name),\n#          !is.na(stop_lat),\n#          !is.na(stop_lon))\n\n\n\n\npath_in &lt;- \"prac5_data/SE.zip\"\ngtfs &lt;- transxchange2gtfs(path_in = path_in,\n                          # remove this line if it is not local\n                          naptan = naptan_local,\n                          ncores = 5)\n\nOxford_LAs_wgs84 &lt;- Oxford_LAs %&gt;%\nst_transform(., 4326)\n  \ngtfs_oxford &lt;- gtfs_clip(gtfs, Oxford_LAs_wgs84)\n\nSave the GTFS file\n\ngtfs_write(gtfs, \n           folder = \"prac5_data/r5r/\",\n           name = \"gtfs_SE\")\n\nIf errors appear with the GTFS file, either now or later, try comparing to best practice standards or the code below\n\n# merge error\ngtfs &lt;- gtfs_merge(gtfs, force = TRUE)\n# clean the file\ngtfs &lt;- gtfs_clean(gtfs)\n# force it to be valid\ngtfs &lt;- gtfs_force_valid(gtfs)\n\n\n\n5.3.1.2 Trains\nRail data comes from the National Rail Portal in an ATOC CIF that we must convert to GTFS. To access this you must register for a free account. UK2GTFS can download the relevant file, however you must set the following in the .Renviro file:NRDP_username and NRDP_password. You can access this file through the usethis package: usethis::edit_r_environ(). For example:\nNRDP_username=“your username”\nRestart R after making these edits.\n\nnrdp_timetable(\"prac5_data/trains/timetable.zip\")\n\nConvert the ATOC CIF to GTFS\n\npath_in &lt;- \"prac5_data/trains/timetable.zip\"\ngtfs &lt;- atoc2gtfs(path_in = path_in,\n          ncores = 5)\n\nSave the GTFS\n\ngtfs_write(gtfs, \n           folder = \"prac5_data/r5r/\",\n           name = \"gtfs_rail\")\n\n\n\n\n5.3.2 Centroids from LSOAs\nFirst we need to get the point locations for LSOA centroids for Oxford and the surrounding area. You can also download the population weighted centroids for LSOAs, but we are not using those today.\nRepeat the filtering process for LSOAs like we did for LAs.\n\nOxford_LSOAs &lt;- LSOA_2021 %&gt;%\n  filter(str_detect(lsoa21nm, \"Oxford\") | \n           str_detect(lsoa21nm, \"White Horse\") | \n           str_detect(lsoa21nm, \"Cherwell\")) \n\n\n#change mode from interactive to static\ntmap_mode(\"plot\")\n\n#plot map\nOxford_map_LSOAs &lt;- \n  tm_shape(Oxford_LSOAs) +\n  tm_fill(col = \"white\", alpha = 0.5) +\n  tm_borders(col = \"black\", lwd = 0.1) +\n  tm_shape(Oxford_City_LAs) +\n  tm_borders(col = \"black\", lwd = 1) + \n  tm_shape(Oxford_TT_foodbanks) +\n    tm_symbols(col = \"purple\", size = 0.15) +\n  tm_shape(trussell_Oxford) +\n    tm_symbols(col = \"purple\", size = 0.15) +\n  tm_shape(IFAN_Oxford) +\n    tm_symbols(col = \"orange\", size = 0.15) +\n  tm_shape(independent_Oxford) +\n    tm_symbols(col = \"red\", size = 0.15) +\n  tm_scale_bar(position = c(\"right\", \"top\")) + \n  tm_compass(position = c(\"right\", \"top\")) + \n  tm_layout(legend.outside = TRUE, frame = FALSE, \n            main.title = \"Food bank centres in & around Oxford\", \n            main.title.position = \"centre\",\n            main.title.size = 0.9) +\n  tm_add_legend('symbol', \n    col = c(\"purple\", \"orange\", \"red\"),\n    border.col = \"black\",\n    size = 1,\n    labels = c('Trussell',\n               'IFAN',\n               'Independent'))\n\nOxford_map_LSOAs\n\n\n\n\n\n\n\n\nBy mapping food bank centres at the LSOA scale rather than the LA scale, we can assume that they are usually located in more densely populated areas like towns/villages in the area.\n\nsf_use_s2(FALSE)\n\ncentroids &lt;- st_centroid(Oxford_LSOAs) %&gt;%\n  st_transform(.,4326)\n  \ncentroids &lt;- centroids %&gt;%\n  mutate(long = unlist(map(centroids$geometry,1)),\n           lat = unlist(map(centroids$geometry,2)))\n\ncentroids$key &lt;- 1\ncentroids$geometry &lt;- NULL\n\ncentroids &lt;- centroids %&gt;%\n  dplyr::select(lsoa21cd, long, lat, key)\n\n\n# load in origin locations - LSOA centroids\npoints_o &lt;- centroids\npoints_o$lon &lt;- as.numeric(points_o$long)\npoints_o$lat &lt;- as.numeric(points_o$lat)\npoints_o$id &lt;- points_o$lsoa21cd\npoints_o &lt;- points_o[,c(\"id\",\"lon\",\"lat\")]\n\n#look at the data\nhead(points_o,10)\n\n          id       lon      lat\n1  E01028422 -1.300924 52.01779\n2  E01028423 -1.334394 52.00905\n3  E01028426 -1.321150 52.05172\n4  E01028427 -1.334637 52.05552\n5  E01028428 -1.327504 52.04657\n6  E01028429 -1.330818 52.05232\n7  E01028430 -1.344268 52.06073\n8  E01028431 -1.343978 52.05493\n9  E01028432 -1.347226 52.04977\n10 E01028433 -1.336181 52.04593\n\n\nWe now have a choice to make - what to use as a destination point, either:\n\nTrussell Trust data\n\nWe can filter distribution (access) points\nBut we have taken postcodes and assigned a lat/lon to them\nThe lat/lon will be a centroid of the postcode not the actual location of the distribution centre.\n\nGive food data\n\nProvided location, so assumed to be more accurate locations\nHowever, the points could be a mixture of food banks, distribution centres and even offices!\n\n\nHere, I have used give food data.\n\nOxford_give_food_foodbanks &lt;- Oxford_give_food_foodbanks %&gt;%\n  sf::st_transform(crs = 4326)\n\n#unlist geometry column\nOxford_give_food_foodbanks &lt;- Oxford_give_food_foodbanks %&gt;%\n  mutate(longitude = unlist(map(Oxford_give_food_foodbanks$geometry,1)),\n           latitude = unlist(map(Oxford_give_food_foodbanks$geometry,2)))\n\npoints_d &lt;- Oxford_give_food_foodbanks\npoints_d$lon &lt;- as.numeric(points_d$longitude)\npoints_d$lat &lt;- as.numeric(points_d$latitude)\npoints_d$id &lt;- points_d$postcode\npoints_d &lt;- points_d[,c(\"id\",\"lon\",\"lat\")]\n\n\n#look at the data\nhead(points_d,10)\n\nSimple feature collection with 10 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -1.595267 ymin: 51.59898 xmax: -1.124697 ymax: 52.06575\nGeodetic CRS:  WGS 84\n        id       lon      lat                   geometry\n1  OX100EW -1.124697 51.59898 POINT (-1.124697 51.59898)\n2  OX110BS -1.254187 51.60542 POINT (-1.254187 51.60542)\n3  OX141PL -1.276512 51.68172 POINT (-1.276512 51.68172)\n4  OX155JS -1.481743 52.00048 POINT (-1.481743 52.00048)\n5  OX160AH -1.340702 52.06057 POINT (-1.340702 52.06057)\n6  OX160RL -1.358554 52.06575 POINT (-1.358554 52.06575)\n7  OX183AG -1.595267 51.75998 POINT (-1.595267 51.75998)\n8  OX201RS -1.349942 51.82934 POINT (-1.349942 51.82934)\n9  OX201RS -1.349942 51.82934 POINT (-1.349942 51.82934)\n10  OX20ES -1.273756 51.74815 POINT (-1.273756 51.74815)\n\n\nIf you look at the ID column it is now postcode, however might need this to be an LSOA. So we must use a lookup table to convert it back\n\nLookup &lt;- read_csv(\"prac5_data/PCD_OA21_LSOA21_MSOA21_LAD_FEB25_UK_LU.csv\")\n\n# remove all spaces in postcode\nLookup &lt;- Lookup %&gt;%\n  mutate(pcd7 = str_replace_all(pcd7, \" \", \"\"))\n\n#join oxford walking time to food banks \n#select only the postcode and LSOA from lookup\n# join on the postcode\npoints_d &lt;- points_d %&gt;%\n  left_join(Lookup %&gt;% select(pcd7, lsoa21cd), \n            by = c(\"id\" = \"pcd7\")) %&gt;%\n  select(lsoa21cd, lon, lat, id)%&gt;%\n  # must have an id\n    rename(postcode=id)%&gt;%\n  rename(id=lsoa21cd)%&gt;%\n  mutate(numeric_id=row_number())\n\nThe next code chunk downloads the road network for the area. However, for this practical, it is included in the data provided (as is the timetable data for public transport).\n\n# load libraries\n#library(r5r)\n#https://download.geofabrik.de/europe.html\n\n#osm_lines = oe_get(\"Oxfordshire\", stringsAsFactors = FALSE, quiet = TRUE)\n#plot(st_geometry(osm_lines))\n\n#EW_url = oe_match(\"Oxfordshire\")\n#oe_download(\n  #file_url = EW_url$url,\n  #file_size = EW_url$file_size,\n  #download_directory = tempdir())\n\n#EW_url[[\"url\"]]\n\nNext we need to set up the r5r core. This sets up the road network and transit zip files in a folder. Note, once this is run once it will save the output as a network.dat file and load from it in future.\n\n# increase Java memory\noptions(java.parameters = \"-Xmx6G\")\n\n# load libraries\nlibrary(r5r)\n\n#path &lt;- system.file(\"r5r_path\", package = \"r5r\")\n\nr5r_core &lt;- setup_r5(data_path = \"prac5_data/r5r.\")\n\nFor this example, I set the maximum trip duration to 1 hour (60 minutes) and set the departure time at 2pm.\n\n# \n# gtfs_rail &lt;- read_gtfs(\"prac5_data/r5r/gtfs_rail.zip\")\n# \n# range(gtfs_rail$calendar$start_date)\n# range(gtfs_rail$calendar$end_date)\n# \n# # Summarize all service periods\n# gtfs_rail$calendar %&gt;%\n#   select(service_id, start_date, end_date) %&gt;%\n#   arrange(start_date)\n# \n# \n# gtfs_bus &lt;- read_gtfs(\"prac5_data/r5r/gtfs_SE.zip\")\n\n# \n# # Set your desired date\n# dep_date &lt;- ymd(\"2026-01-01\")  # replace with your date\n# \n# # Check calendar periods\n# gtfs_bus$calendar %&gt;%\n#   mutate(start_date = ymd(start_date),\n#          end_date = ymd(end_date)) %&gt;%\n#   filter(dep_date &gt;= start_date & dep_date &lt;= end_date)\n# \n# # Check calendar_dates for exceptions\n# if(\"calendar_dates\" %in% names(gtfs_bus)) {\n#   gtfs_bus$calendar_dates %&gt;%\n#     mutate(date = ymd(date)) %&gt;%\n#     filter(date == dep_date)\n# }\n\n\n\n# routing paramaters\nmode &lt;- c(\"TRANSIT\")\n\nmax_trip_duration &lt;- 60 # minutes\n\n# can change to just a day (no time) and it will return min time of the entire day (to each Food bank within 60 mins)\ndeparture_datetime &lt;- as.POSIXct(\"01-03-2021 14:00:00\", \n                                 format = \"%d-%m-%Y %H:%M:%S\",\n                                 tz = \"Europe/London\")\n\n# estimating the travel time\nTTM_Oxford_PT &lt;- travel_time_matrix(r5r_core,\n                          origins = points_o,\n                          destinations = points_d,\n                          mode = mode,\n                          departure_datetime = departure_datetime,\n                          max_trip_duration = max_trip_duration)\n\n\n# routing paramaters\nmode &lt;- c(\"WALK\")\n\nmax_trip_duration &lt;- 60  # minutes\n\ndeparture_datetime &lt;- as.POSIXct(\"01-03-2021 14:00:00\", \n                                 format = \"%d-%m-%Y %H:%M:%S\",\n                                 tz = \"Europe/London\")\n\n# estimating the travel time\nTTM_Oxford_walk &lt;- travel_time_matrix(r5r_core,\n                          origins = points_o,\n                          destinations = points_d,\n                          mode = mode,\n                          departure_datetime = departure_datetime,\n                          max_trip_duration = max_trip_duration)\n\nNow for Driving\n\n# routing paramaters\nmode &lt;- c(\"CAR\")\n\nmax_trip_duration &lt;- 60  # minutes\n\ndeparture_datetime &lt;- as.POSIXct(\"01-03-2021 14:00:00\", \n                                 format = \"%d-%m-%Y %H:%M:%S\",\n                                 tz = \"Europe/London\")\n\n# estimating the travel time\nTTM_Oxford_drive &lt;- travel_time_matrix(r5r_core,\n                          origins = points_o,\n                          destinations = points_d,\n                          mode = mode,\n                          departure_datetime = departure_datetime,\n                          max_trip_duration = max_trip_duration)\n\nWe now have three modes of transport (measure of access) to food banks in the area - public transport, walking, and driving. As you can see many more LSOAs have access to a food bank centre within a one hour drive, compared to one hour walking or public transport.\n\n\n5.3.3 Getting minimum travel time\n\n5.3.3.1 Walking\nLet’s start with walking. This removes origin-destination pairs that have two or more values for travel time under one hour by taking the minimum value.\n\n#select minimum value between pairs \nTTM_Oxford_walk2 &lt;- TTM_Oxford_walk %&gt;%\n  group_by(to_id, from_id) %&gt;% \n  slice_min(travel_time_p50, n = 1, with_ties = FALSE, na_rm = FALSE) %&gt;%\n  ungroup()\n\nNext we take the long tibble and make it wide. The number of rows has decreased as not all LSOAs will have accessibility to a centre within one hour.\n\n#pivot wider\nTTM_Oxford_walk3 &lt;- TTM_Oxford_walk2 %&gt;%\n  pivot_wider(names_from = to_id, values_from = travel_time_p50)\n\nhead(TTM_Oxford_walk3)\n\n# A tibble: 6 × 194\n  from_id  E01028426 E01028427 E01028428 E01028429 E01028430 E01028431 E01028432\n  &lt;chr&gt;        &lt;int&gt;     &lt;int&gt;     &lt;int&gt;     &lt;int&gt;     &lt;int&gt;     &lt;int&gt;     &lt;int&gt;\n1 E010284…        46        24        35        25        10        12        27\n2 E010284…        NA        49        60        52        24        35        44\n3 E010357…        NA        NA        NA        NA        NA        NA        NA\n4 E010284…        NA        NA        NA        NA        NA        NA        NA\n5 E010284…        NA        NA        NA        NA        NA        NA        NA\n6 E010288…        NA        NA        NA        NA        NA        NA        NA\n# ℹ 186 more variables: E01028433 &lt;int&gt;, E01028434 &lt;int&gt;, E01028435 &lt;int&gt;,\n#   E01028437 &lt;int&gt;, E01028439 &lt;int&gt;, E01028440 &lt;int&gt;, E01028441 &lt;int&gt;,\n#   E01028443 &lt;int&gt;, E01028444 &lt;int&gt;, E01028445 &lt;int&gt;, E01028446 &lt;int&gt;,\n#   E01028447 &lt;int&gt;, E01028448 &lt;int&gt;, E01028449 &lt;int&gt;, E01028450 &lt;int&gt;,\n#   E01028451 &lt;int&gt;, E01028452 &lt;int&gt;, E01028453 &lt;int&gt;, E01028454 &lt;int&gt;,\n#   E01028455 &lt;int&gt;, E01028456 &lt;int&gt;, E01028457 &lt;int&gt;, E01028458 &lt;int&gt;,\n#   E01028459 &lt;int&gt;, E01028460 &lt;int&gt;, E01028461 &lt;int&gt;, E01028462 &lt;int&gt;, …\n\n\n\n\n5.3.3.2 Driving\nLet’s repeat this for driving and public transport.\n\n#select minimum value between pairs \nTTM_Oxford_drive2 &lt;- TTM_Oxford_drive %&gt;%\n  group_by(to_id, from_id) %&gt;% \n  slice(which.min(travel_time_p50))\n\n#check travel times as numeric\nTTM_Oxford_drive2$travel_time_p50 &lt;- as.numeric(TTM_Oxford_drive2$travel_time_p50)\n\n#pivot wider to create a matrix\nTTM_Oxford_drive3 &lt;- TTM_Oxford_drive2 %&gt;%\n  pivot_wider(names_from = to_id, values_from = travel_time_p50)\n\nhead(TTM_Oxford_drive3)\n\n# A tibble: 6 × 15\n# Groups:   from_id [6]\n  from_id  E01028430 E01028452 E01028485 E01028491 E01028549 E01028624 E01028673\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 E010284…         9        15        17        22        32        48        47\n2 E010284…        13        19        16        23        33        49        48\n3 E010284…         6        12        23        29        39        55        54\n4 E010284…         4        10        21        27        37        53        52\n5 E010284…         7        13        21        26        37        52        51\n6 E010284…         5        11        21        28        38        54        53\n# ℹ 7 more variables: E01028677 &lt;dbl&gt;, E01028699 &lt;dbl&gt;, E01028742 &lt;dbl&gt;,\n#   E01028771 &lt;dbl&gt;, E01028818 &lt;dbl&gt;, E01028825 &lt;dbl&gt;, E01035735 &lt;dbl&gt;\n\n\n\n\n5.3.3.3 Public transport\n\n#select minimum value between pairs \nTTM_Oxford_PT2 &lt;- TTM_Oxford_PT %&gt;%\n  group_by(to_id, from_id) %&gt;% \n  slice(which.min(travel_time_p50))\n\n#check travel times as numeric\nTTM_Oxford_PT2$travel_time_p50 &lt;- as.numeric(TTM_Oxford_PT2$travel_time_p50)\n\n#pivot wider to create a matrix\nTTM_Oxford_PT3 &lt;- TTM_Oxford_PT2 %&gt;%\n  pivot_wider(names_from = to_id, values_from = travel_time_p50)\n\nhead(TTM_Oxford_PT3)\n\n# A tibble: 6 × 15\n# Groups:   from_id [6]\n  from_id  E01028430 E01028452 E01028485 E01028491 E01028549 E01028624 E01028673\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 E010284…        46        NA        NA        NA        NA        NA        NA\n2 E010284…        24        49        NA        NA        NA        NA        NA\n3 E010284…        35        NA        NA        NA        NA        NA        NA\n4 E010284…        25        52        NA        NA        NA        NA        NA\n5 E010284…        10        24        NA        NA        NA        NA        NA\n6 E010284…        12        35        NA        NA        NA        NA        NA\n# ℹ 7 more variables: E01028677 &lt;dbl&gt;, E01028699 &lt;dbl&gt;, E01028742 &lt;dbl&gt;,\n#   E01028771 &lt;dbl&gt;, E01028818 &lt;dbl&gt;, E01028825 &lt;dbl&gt;, E01035735 &lt;dbl&gt;\n\n\n\n\n\n5.3.4 Join to LSOAs\nNow we have the accessibility data in a usable form that we can join to our Oxford LSOAs data frame, we can calculate mean and minimum travel times for each LSOA for each mode of transport. R5R might find more than 1 food bank that is reachable for each centroid, so we can identify the minimum (and mean) travel time from the origin centroid.\n\nTTM_Oxford_drive3 &lt;- TTM_Oxford_drive3 %&gt;%\n  # forces operation to happen row by row - find the smallest time in the row (not column)\n  rowwise() %&gt;%\n  mutate(\n    minimum_time_drive = min(c_across(2:14), na.rm = TRUE),\n    mean_time_drive    = mean(c_across(2:14), na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\n\nTTM_Oxford_walk3 &lt;- TTM_Oxford_walk3 %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    minimum_time_walk = min(c_across(3:15), na.rm = TRUE),\n    mean_time_walk  = mean(c_across(3:15), na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\nTTM_Oxford_PT3 &lt;- TTM_Oxford_PT3 %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    minimum_time_PT = min(c_across(2:14), na.rm = TRUE),\n    mean_time_PT    = mean(c_across(2:14), na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\nNow we need to join this to the Oxford LSOAs dataframe and map it.\n\n#rename to from id columns for joining purposes\n#select only the ID column and mean and minimum times\n\nTTM_Oxford_drive3 &lt;- TTM_Oxford_drive3 %&gt;%\n  select(from_id, mean_time_drive, minimum_time_drive) %&gt;%\n  rename(., lsoa21cd = \"from_id\")\n\nTTM_Oxford_walk3 &lt;- TTM_Oxford_walk3 %&gt;%\n  select(from_id, mean_time_walk, minimum_time_walk) %&gt;%\n  rename(., lsoa21cd = \"from_id\")\n\nTTM_Oxford_PT3 &lt;- TTM_Oxford_PT3 %&gt;%\n  select(from_id, mean_time_PT, minimum_time_PT) %&gt;%\n  rename(., lsoa21cd = \"from_id\")\n\nOxford_LSOAs &lt;- Oxford_LSOAs %&gt;%\n  left_join(.,\n            TTM_Oxford_drive3,\n            by = \"lsoa21cd\") %&gt;%\n  left_join(.,\n            TTM_Oxford_walk3,\n            by = \"lsoa21cd\") %&gt;%\n  left_join(.,\n            TTM_Oxford_PT3,\n            by = \"lsoa21cd\")\n\nremove any inf values as these won’t map\n\nOxford_LSOAs &lt;- Oxford_LSOAs %&gt;%\n  mutate(\n    minimum_time_walk  = ifelse(is.infinite(minimum_time_walk), NA, minimum_time_walk),\n    minimum_time_drive = ifelse(is.infinite(minimum_time_drive), NA, minimum_time_drive),\n    minimum_time_PT    = ifelse(is.infinite(minimum_time_PT), NA, minimum_time_PT)\n  )\n\nMap\n\n#map for minimum public transport time\nTravel_time_map &lt;-\n  tm_shape(Oxford_LSOAs) +\n  tm_fill(col = \"minimum_time_PT\", title = \"Minimum time to food bank (mins)\", style = \"cont\", n=10, palette = \"-Spectral\",\n          colorNA = \"white\", # colour of missing data\n          textNA = \"Public transport beyond one hour\") +\n    tm_borders(col = \"grey\", lwd = 0.01) +\n  tm_shape(Oxford_LAs) +\n    tm_borders(col = \"black\", lwd = 1) + \n  tm_shape(Oxford_City_LAs) +\n    tm_borders(col = \"black\", lwd = 1) + \n  tm_scale_bar(position = c(\"right\", \"top\")) + \n  tm_compass(position = c(\"right\", \"top\")) +\n  tm_layout(legend.outside = TRUE, frame = FALSE, \n            main.title = \"PT accessibility in & around Oxford\", \n            main.title.position = \"centre\",\n            main.title.size = 0.9) \n\nTravel_time_map \n\n\n\n\n\n\n\n\n\n#map for minimum driving time\nTravel_time_map &lt;-\n  tm_shape(Oxford_LSOAs) +\n  tm_fill(col = \"minimum_time_drive\", title = \"Minimum time to food bank (mins)\", style = \"cont\", n=10, palette = \"-Spectral\",\n          colorNA = \"white\", # colour of missing data\n          textNA = \"Driving beyond one hour\") +\n    tm_borders(col = \"grey\", lwd = 0.01) +\n  tm_shape(Oxford_LAs) +\n    tm_borders(col = \"black\", lwd = 1) + \n  tm_shape(Oxford_City_LAs) +\n    tm_borders(col = \"black\", lwd = 1) + \n  tm_scale_bar(position = c(\"right\", \"top\")) + \n  tm_compass(position = c(\"right\", \"top\")) +\n  tm_layout(legend.outside = TRUE, frame = FALSE, \n            main.title = \"Driving accessibility in & around Oxford\", \n            main.title.position = \"centre\",\n            main.title.size = 0.9) \n\nTravel_time_map \n\n\n\n\n\n\n\n\n\n#map for minimum walking time\nTravel_time_map &lt;-\n  tm_shape(Oxford_LSOAs) +\n  tm_fill(col = \"minimum_time_walk\", title = \"Minimum time to food bank (mins)\", style = \"cont\", n=10, palette = \"-Spectral\",\n          colorNA = \"white\", # colour of missing data\n          textNA = \"Walking beyond one hour\") +\n    tm_borders(col = \"grey\", lwd = 0.01) +\n  tm_shape(Oxford_LAs) +\n    tm_borders(col = \"black\", lwd = 1) + \n  tm_shape(Oxford_City_LAs) +\n    tm_borders(col = \"black\", lwd = 1) + \n  tm_scale_bar(position = c(\"right\", \"top\")) + \n  tm_compass(position = c(\"right\", \"top\")) +\n  tm_layout(legend.outside = TRUE, frame = FALSE, \n            main.title = \"Walking accessibility in & around Oxford\", \n            main.title.position = \"centre\",\n            main.title.size = 0.9) \n\nTravel_time_map \n\n\n\n\n\n\n\n\nFrom these three maps it is very clear that areas have unequal access to food bank centres, especially in the rural parts of Oxfordshire. However, do people in these areas of low accessibility need greater access to centres.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Food bank accessibility</span>"
    ]
  },
  {
    "objectID": "05_food_bank_accessibility.html#examining-demand",
    "href": "05_food_bank_accessibility.html#examining-demand",
    "title": "5  Food bank accessibility",
    "section": "5.4 Examining demand",
    "text": "5.4 Examining demand\nIn this last section we will examine two determinants of food bank use. Let’s take housing tenure (socially rented homes) and disability. This is census data from 2021 at the LSOA scale.\n\n#Type of tenure of housing\nTenure &lt;- read_csv(\"prac5_data/Tenure.csv\") %&gt;%\n  clean_names() %&gt;%\n  # separate a single column into 2 by using : as the divider\n  separate(lsoa21cd, c('lsoa_code', 'lsoa_name'), sep = ':') %&gt;%\n  #trimws = remove white space before and after\n  mutate(lsoa_name = trimws(lsoa_name)) %&gt;%\n  mutate(lsoa_code = trimws(lsoa_code)) \n\n#select socially rented variable\nTenure &lt;- Tenure %&gt;%\n  select(lsoa_code, lsoa_name, social_rented)\n\n#households that are socially rented\nTenure$social_rented &lt;- as.numeric(Tenure$social_rented)\nfavstats(Tenure$social_rented)\n\n min  Q1 median   Q3 max     mean      sd     n missing\n   0 4.6   11.4 24.9  92 16.90616 15.9975 35672       7\n\n#social rented\nggplot(Tenure,\n       aes(x=social_rented)) + \n  geom_histogram(aes(y=after_stat(density)), color=\"black\", fill=\"white\") +\n  geom_vline(aes(xintercept=mean(social_rented)),\n            color=\"blue\", linetype=\"dashed\", size=1) +\n geom_density(alpha=.2, fill=\"#FF6666\")\n\n\n\n\n\n\n\n\n\n#Disability under the Equality Act\nDisability &lt;- read_csv(\"prac5_data/Disability.csv\") %&gt;%\n  clean_names() %&gt;%\n  separate(lsoa, c('lsoa_code', 'lsoa_name'), sep = ':') %&gt;%\n  mutate(lsoa_name = trimws(lsoa_name)) %&gt;%\n  mutate(lsoa_code = trimws(lsoa_code)) \n\n#select socially rented variable\nDisability &lt;- Disability %&gt;%\n  select(lsoa_code, lsoa_name, disabled_under_the_equality_act)\n\n#households with 1+ disabled person\nDisability$disabled_under_the_equality_act &lt;- as.numeric(Disability$disabled_under_the_equality_act)\nfavstats(Disability$disabled_under_the_equality_act)\n\n min   Q1 median   Q3  max     mean       sd     n missing\n 1.8 14.1   17.2 20.9 44.7 17.71969 4.931695 35672       7\n\n#disability\nggplot(Disability,\n       aes(x=disabled_under_the_equality_act)) + \n  geom_histogram(aes(y=after_stat(density)), color=\"black\", fill=\"white\") +\n  geom_vline(aes(xintercept=mean(disabled_under_the_equality_act)),\n            color=\"blue\", linetype=\"dashed\", size=1) +\n geom_density(alpha=.2, fill=\"#FF6666\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuestion:\nWhat can we say about the distribution of these two demand variables? And how could we customise these histograms to improve the look of them?\n\n\n\n#rename lsoa code variable\nDisability &lt;- Disability %&gt;%\n  rename(., \"lsoa21cd\" = \"lsoa_code\") %&gt;%\n  rename(., \"lsoa21nm\" = \"lsoa_name\")\n\nTenure &lt;- Tenure %&gt;%\n  rename(., \"lsoa21cd\" = \"lsoa_code\") %&gt;%\n  rename(., \"lsoa21nm\" = \"lsoa_name\")\n\n#join to Oxford LSOA data frame\nOxford_LSOAs &lt;- Oxford_LSOAs %&gt;%\n  left_join(.,\n            Disability, \n            by = \"lsoa21cd\") %&gt;%\n  left_join(.,\n            Tenure, \n            by = \"lsoa21cd\")\n\nLet’s make a map of disability (% of households with one or more disabled person living there) and socially rented homes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.4.1 Identifying under-served areas\nFirstly, what are the national averages for disability and social renting across England and Wales?\n\nmean(Tenure$social_rented, na.rm = TRUE)\n\n[1] 16.90616\n\nmean(Disability$disabled_under_the_equality_act, na.rm = TRUE)\n\n[1] 17.71969\n\n\n\nSocially renting - 16.9% of households\nDisability - 17.7% of households\n\nAnd what are the Oxford and surrounding area averages?\n\nmean(Oxford_LSOAs$social_rented, na.rm = TRUE)\n\n[1] 14.51145\n\nmean(Oxford_LSOAs$disabled_under_the_equality_act, na.rm = TRUE)\n\n[1] 14.61028\n\n\n\nSocially renting - 14.5% of households\nDisability - 14.6% of households\n\nThey are lower than the national averages.\nLet’s take a look for the three travel times too.\n\nmean(Oxford_LSOAs$mean_time_drive, na.rm = TRUE)\n\n[1] 31.41028\n\nmean(Oxford_LSOAs$mean_time_walk, na.rm = TRUE)\n\n[1] 37.66783\n\nmean(Oxford_LSOAs$mean_time_PT, na.rm = TRUE)\n\n[1] 31.89674\n\n\nVery simply, if we take the areas that are below the area average for social renting and disability and one of the accessibility measures, which places in and around Oxford are under served to food banks based on high demand and lower accessibility?\n\n#set some parameters to identify which areas are under served\nOxford_LSOAs &lt;- Oxford_LSOAs %&gt;%\n  mutate(Underserved_areas = case_when\n         (social_rented &gt;= 14.5 & disabled_under_the_equality_act &gt;= 14.6 & mean_time_PT &gt;= 32 ~ \"Underserved\", \n        TRUE ~ \"Served\"))\n\n\n#creating a buffer around the LSOAs\nbuffer &lt;- Oxford_LSOAs %&gt;%\n  filter(Underserved_areas == \"Underserved\") %&gt;%\n  st_buffer(., 500) %&gt;%\n  st_union()\n\n Underserved_map &lt;-\n  tm_shape(Oxford_LSOAs) +\n  tm_fill(col = \"Underserved_areas\", style = \"cat\", title = \"Underserved Areas\", palette = \"Blues\") +\n  tm_borders(col = \"grey\", lwd = 0.1) +\n  tm_shape(buffer) +\n  tm_polygons(alpha = 0.1) +\n  tm_shape(Oxford_LAs) +\n    tm_borders(col = \"black\", lwd = 1) + \n  tm_shape(Oxford_City_LAs) +\n    tm_borders(col = \"black\", lwd = 1) + \n  tm_scale_bar(position = c(\"right\", \"top\")) + \n  tm_compass(position = c(\"right\", \"top\")) +\n  tm_layout(main.title = \"Under-served areas to food banks\", main.title.position = \"centre\",\n            legend.outside = TRUE, frame = FALSE, main.title.size = 1) \n\nUnderserved_map",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Food bank accessibility</span>"
    ]
  },
  {
    "objectID": "05_food_bank_accessibility.html#quick-summary",
    "href": "05_food_bank_accessibility.html#quick-summary",
    "title": "5  Food bank accessibility",
    "section": "5.5 Quick summary",
    "text": "5.5 Quick summary\nWe can use accessibility tools and census data to assess where in an area is under-served to particular services.\n\nHow would we improve this analysis?\nWhat are the limitations of the data used?\nHow could we improve the visualisations?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Food bank accessibility</span>"
    ]
  },
  {
    "objectID": "06_datasets.html",
    "href": "06_datasets.html",
    "title": "6  Datasets",
    "section": "",
    "text": "Health data can be hard to find online. Here are some interesting datasets i have come across:\n\nUK Small area mental health index / other mental health data\nUK Understanding society health and wellbeing\nUK Police data (e.g. for looking at violence and mental wellbeing)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Datasets</span>"
    ]
  }
]